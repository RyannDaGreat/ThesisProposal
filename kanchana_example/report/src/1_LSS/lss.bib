@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})
@String(ICML= {Int. Conf. Machine Learning})

@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})
@String(ICML= {ICML})
@String(arxiv= {Arxiv})



@article{luo2022clip4clip,
	title={{CLIP4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval}},
	author={Luo, Huaishao and Ji, Lei and Zhong, Ming and Chen, Yang and Lei, Wen and Duan, Nan and Li, Tianrui},
	journal={Neurocomputing},
	volume={508},
	pages={293--304},
	year={2022},
	publisher={Elsevier}
}

@article{wang2021actionclip,
	title={{ActionCLIP: A New Paradigm for Video Action Recognition}},
	author={Wang, Mengmeng and Xing, Jiazheng and Liu, Yong},
	journal={arXiv preprint arXiv:2109.08472},
	year={2021}
}

@article{bain2022cliphitchhiker,
	title={{A CLIP-Hitchhiker's Guide to Long Video Retrieval}},
	author={Bain, Max and Nagrani, Arsha and Varol, G{\"u}l and Zisserman, Andrew},
	journal={arXiv preprint arXiv:2205.08508},
	year={2022}
}

@article{lin2022evl,
	title={{Frozen CLIP Models are Efficient Video Learners}},
	author={Lin, Ziyi and Geng, Shijie and Zhang, Renrui and Gao, Peng and de Melo, Gerard and Wang, Xiaogang and Dai, Jifeng and Qiao, Yu and Li, Hongsheng},
	journal={arXiv preprint arXiv:2208.03550},
	year={2022}
}

@inproceedings{radford2021clip,
	title={{Learning Transferable Visual Models From Natural Language Supervision}},
	author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
	booktitle={ICML},
	pages={8748--8763},
	year={2021},
	organization={PMLR}
}

@article{qian2022multimodal,
	title={{Multimodal Open-Vocabulary Video Classification via Pre-Trained Vision and Language Models}},
	author={Qian, Rui and Li, Yeqing and Xu, Zheng and Yang, Ming-Hsuan and Belongie, Serge and Cui, Yin},
	journal={arXiv preprint arXiv:2207.07646},
	year={2022}
}

@article{fang2021clip2video,
	title={{CLIP2Video: Mastering Video-Text Retrieval via Image CLIP}},
	author={Fang, Han and Xiong, Pengfei and Xu, Luhui and Chen, Yu},
	journal={arXiv preprint arXiv:2106.11097},
	year={2021}
}

@inproceedings{fan2021mvit,
	title={{Multiscale Vision Transformers}},
	author={Fan, Haoqi and Xiong, Bo and Mangalam, Karttikeya and Li, Yanghao and Yan, Zhicheng and Malik, Jitendra and Feichtenhofer, Christoph},
	booktitle={ICCV},
	pages={6824--6835},
	year={2021}
}

@inproceedings{li2022mvitv2,
	title={{MViTv2: Improved Multiscale Vision Transformers for Classification and Detection}},
	author={Li, Yanghao and Wu, Chao-Yuan and Fan, Haoqi and Mangalam, Karttikeya and Xiong, Bo and Malik, Jitendra and Feichtenhofer, Christoph},
	booktitle={CVPR},
	pages={4804--4814},
	year={2022}
}

@inproceedings{arnab2021vivit,
	title={{ViViT: A Video Vision Transformer}},
	author={Arnab, Anurag and Dehghani, Mostafa and Heigold, Georg and Sun, Chen and Lu{\v{c}}i{\'c}, Mario and Schmid, Cordelia},
	booktitle={ICCV},
	pages={6836--6846},
	year={2021}
}

@article{li2022uniformer,
	title={{UniFormer: Unifying Convolution and Self-attention for Visual Recognition}},
	author={Li, Kunchang and Wang, Yali and Zhang, Junhao and Gao, Peng and Song, Guanglu and Liu, Yu and Li, Hongsheng and Qiao, Yu},
	journal={arXiv preprint arXiv:2201.09450},
	year={2022}
}

@inproceedings{liu2022videoswin,
	title={{Video Swin Transformer}},
	author={Liu, Ze and Ning, Jia and Cao, Yue and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Hu, Han},
	booktitle={CVPR},
	pages={3202--3211},
	year={2022}
}

@inproceedings{bertasius2021timesformer,
	title={{Is Space-Time Attention All You Need for Video Understanding?}},
	author={Bertasius, Gedas and Wang, Heng and Torresani, Lorenzo},
	booktitle={ICML},
	pages={4},
	year={2021}
}

@article{ryoo2021tokenlearner,
	title={{TokenLearner: Adaptive Space-Time Tokenization for Videos}},
	author={Ryoo, Michael and Piergiovanni, AJ and Arnab, Anurag and Dehghani, Mostafa and Angelova, Anelia},
	journal={NeurIPS},
	volume={34},
	pages={12786--12797},
	year={2021}
}

@article{zhang2021cover,
	title={{Co-training Transformer with Videos and Images Improves Action Recognition}},
	author={Zhang, Bowen and Yu, Jiahui and Fifty, Christopher and Han, Wei and Dai, Andrew M and Pang, Ruoming and Sha, Fei},
	journal={arXiv preprint arXiv:2112.07175},
	year={2021}
}

@article{pan2022stadapter,
	title={{ST-Adapter: Parameter-Efficient Image-to-Video Transfer Learning}},
	author={Pan, Junting and Lin, Ziyi and Zhu, Xiatian and Shao, Jing and Li, Hongsheng},
	journal={NeurIPS},
	year={2022}
}

@article{wu2022text4vis,
	title={{Revisiting Classifier: Transferring Vision-Language Models for Video Recognition}},
	author={Wu, Wenhao and Sun, Zhun and Ouyang, Wanli},
	journal={AAAI},
	year={2023}
}

@inproceedings{yan2022mtv,
	title={{Multiview Transformers for Video Recognition}},
	author={Yan, Shen and Xiong, Xuehan and Arnab, Anurag and Lu, Zhichao and Zhang, Mi and Sun, Chen and Schmid, Cordelia},
	booktitle={CVPR},
	pages={3333--3343},
	year={2022}
}

@inproceedings{lin2019tsm,
	title={{TSM: Temporal Shift Module for Efficient Video Understanding}},
	author={Lin, Ji and Gan, Chuang and Han, Song},
	booktitle={ICCV},
	pages={7083--7093},
	year={2019}
}

@article{patrick2021mformer,
	title={{Keeping Your Eye on the Ball: Trajectory Attention in Video Transformers}},
	author={Patrick, Mandela and Campbell, Dylan and Asano, Yuki and Misra, Ishan and Metze, Florian and Feichtenhofer, Christoph and Vedaldi, Andrea and Henriques, Jo{\~a}o F},
	journal={NeurIPS},
	volume={34},
	pages={12493--12506},
	year={2021}
}

@inproceedings{kwon2021selfynet,
	title={{Learning Self-Similarity in Space and Time as Generalized Motion for Video Action Recognition}},
	author={Kwon, Heeseung and Kim, Manjin and Kwak, Suha and Cho, Minsu},
	booktitle={ICCV},
	pages={13065--13075},
	year={2021}
}

@inproceedings{wang2021tdn,
	title={{TDN: Temporal Difference Networks for Efficient Action Recognition}},
	author={Wang, Limin and Tong, Zhan and Ji, Bin and Wu, Gangshan},
	booktitle={CVPR},
	pages={1895--1904},
	year={2021}
}

@inproceedings{kwon2020msnet,
	title={{MotionSqueeze: Neural Motion Feature Learning for Video Understanding}},
	author={Kwon, Heeseung and Kim, Manjin and Kwak, Suha and Cho, Minsu},
	booktitle={ECCV},
	pages={345--362},
	year={2020},
	organization={Springer}
}

@inproceedings{piergiovanni2019evanet,
	title={{Evolving Space-Time Neural Architectures for Videos}},
	author={Piergiovanni, AJ and Angelova, Anelia and Toshev, Alexander and Ryoo, Michael S},
	booktitle={ICCV},
	pages={1793--1802},
	year={2019}
}

@inproceedings{feichtenhofer2019slowfast,
	title={{SlowFast Networks for Video Recognition}},
	author={Feichtenhofer, Christoph and Fan, Haoqi and Malik, Jitendra and He, Kaiming},
	booktitle={ICCV},
	pages={6202--6211},
	year={2019}
}

@inproceedings{feichtenhofer2020x3d,
	title={{X3D: Expanding Architectures for Efficient Video Recognition}},
	author={Feichtenhofer, Christoph},
	booktitle={CVPR},
	pages={203--213},
	year={2020}
}

@inproceedings{wang2018nonlocal,
	title={{Non-local Neural Networks}},
	author={Wang, Xiaolong and Girshick, Ross and Gupta, Abhinav and He, Kaiming},
	booktitle={CVPR},
	pages={7794--7803},
	year={2018}
}

@article{ryoo2019assemblenet,
	title={{AssembleNet: Searching for Multi-Stream Neural Connectivity in Video Architectures}},
	author={Ryoo, Michael S and Piergiovanni, AJ and Tan, Mingxing and Angelova, Anelia},
	journal={ICLR},
	year={2020}
}

@inproceedings{ryoo2020assemblenet++,
	title={{AssembleNet++: Assembling Modality Representations via Attention Connections}},
	author={Ryoo, Michael S and Piergiovanni, AJ and Kangaspunta, Juhana and Angelova, Anelia},
	booktitle={ECCV},
	pages={654--671},
	year={2020},
	organization={Springer}
}

@inproceedings{wu2019lfb,
	title={{Long-Term Feature Banks for Detailed Video Understanding}},
	author={Wu, Chao-Yuan and Feichtenhofer, Christoph and Fan, Haoqi and He, Kaiming and Krahenbuhl, Philipp and Girshick, Ross},
	booktitle={CVPR},
	pages={284--293},
	year={2019}
}

@inproceedings{kondratyuk2021movinets,
	title={{MoViNets: Mobile Video Networks for Efficient Video Recognition}},
	author={Kondratyuk, Dan and Yuan, Liangzhe and Li, Yandong and Zhang, Li and Tan, Mingxing and Brown, Matthew and Gong, Boqing},
	booktitle={CVPR},
	pages={16020--16030},
	year={2021}
}

@inproceedings{xu2016mte,
	title={{Multi-Task Zero-Shot Action Recognition with Prioritised Data Augmentation
	}},
	author={Xu, Xun and Hospedales, Timothy M and Gong, Shaogang},
	booktitle={ECCV},
	pages={343--359},
	year={2016},
	organization={Springer}
}

@inproceedings{wang2017asr,
	title={{Alternative Semantic Representations for Zero-Shot Human Action Recognition}},
	author={Wang, Qian and Chen, Ke},
	booktitle={ECML-PKDD},
	pages={87--102},
	year={2017},
	organization={Springer}
}

@inproceedings{qin2017zsecoc,
	title={{Zero-Shot Action Recognition with Error-Correcting Output Codes}},
	author={Qin, Jie and Liu, Li and Shao, Ling and Shen, Fumin and Ni, Bingbing and Chen, Jiaxin and Wang, Yunhong},
	booktitle={CVPR},
	pages={2833--2842},
	year={2017}
}

@inproceedings{gao2019tsgcn,
	title={{I Know the Relationships: Zero-Shot Action Recognition via Two-Stream Graph Convolutional Networks and Knowledge Graphs}},
	author={Gao, Junyu and Zhang, Tianzhu and Xu, Changsheng},
	booktitle={AAAI},
	pages={8303--8311},
	year={2019}
}

@inproceedings{zhu2018ur,
	title={{Towards Universal Representation for Unseen Action Recognition}},
	author={Zhu, Yi and Long, Yang and Guan, Yu and Newsam, Shawn and Shao, Ling},
	booktitle={CVPR},
	pages={9436--9445},
	year={2018}
}

@inproceedings{brattoli2020e2e,
	title={{Rethinking Zero-shot Video Classification: End-to-end Training for Realistic Applications}},
	author={Brattoli, Biagio and Tighe, Joseph and Zhdanov, Fedor and Perona, Pietro and Chalupka, Krzysztof},
	booktitle={CVPR},
	pages={4613--4623},
	year={2020}
}

@inproceedings{chen2021erzsar,
	title={{Elaborative Rehearsal for Zero-shot Action Recognition}},
	author={Chen, Shizhe and Huang, Dong},
	booktitle={ICCV},
	pages={13638--13647},
	year={2021}
}

@article{kay2017kinetics,
	title={{The Kinetics Human Action Video Dataset}},
	author={Kay, Will and Carreira, Joao and Simonyan, Karen and Zhang, Brian and Hillier, Chloe and Vijayanarasimhan, Sudheendra and Viola, Fabio and Green, Tim and Back, Trevor and Natsev, Paul and others},
	journal={arXiv preprint arXiv:1705.06950},
	year={2017}
}

@inproceedings{sigurdsson2016hollywood,
	title={{Hollywood in Homes: Crowdsourcing Data Collection for Activity Understanding}},
	author={Sigurdsson, Gunnar A and Varol, G{\"u}l and Wang, Xiaolong and Farhadi, Ali and Laptev, Ivan and Gupta, Abhinav},
	booktitle={ECCV},
	pages={510--526},
	year={2016},
	organization={Springer}
}

@article{soomro2012ucf101,
	title={{UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild}},
	author={Soomro, Khurram and Zamir, Amir Roshan and Shah, Mubarak},
	journal={arXiv preprint arXiv:1212.0402},
	year={2012}
}

@inproceedings{goyal2017something,
	title={{The ``something something'' video database for learning and evaluating visual common sense}},
	author={Goyal, Raghav and Ebrahimi Kahou, Samira and Michalski, Vincent and Materzynska, Joanna and Westphal, Susanne and Kim, Heuna and Haenel, Valentin and Fruend, Ingo and Yianilos, Peter and Mueller-Freitag, Moritz and others},
	booktitle={ICCV},
	pages={5842--5850},
	year={2017}
}

@inproceedings{kuehne2011hmdb,
	title={{HMDB: A large video database for human motion recognition}},
	author={Kuehne, Hildegard and Jhuang, Hueihan and Garrote, Est{\'\i}baliz and Poggio, Tomaso and Serre, Thomas},
	booktitle={ICCV},
	pages={2556--2563},
	year={2011},
	organization={IEEE}
}

@inproceedings{carreira2017i3d,
	title={{Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset}},
	author={Carreira, Joao and Zisserman, Andrew},
	booktitle={CVPR},
	pages={6299--6308},
	year={2017}
}

@article{nagrani2021bottleneck,
	title={{Attention Bottlenecks for Multimodal Fusion}},
	author={Nagrani, Arsha and Yang, Shan and Arnab, Anurag and Jansen, Aren and Schmid, Cordelia and Sun, Chen},
	journal={NeurIPS},
	volume={34},
	pages={14200--14213},
	year={2021}
}

@inproceedings{jia2021align,
	title={{Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision}},
	author={Jia, Chao and Yang, Yinfei and Xia, Ye and Chen, Yi-Ting and Parekh, Zarana and Pham, Hieu and Le, Quoc and Sung, Yun-Hsuan and Li, Zhen and Duerig, Tom},
	booktitle={ICML},
	pages={4904--4916},
	year={2021},
	organization={PMLR}
}

@article{zeng2022socratic,
	title={{Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language}},
	author={Zeng, Andy and Wong, Adrian and Welker, Stefan and Choromanski, Krzysztof and Tombari, Federico and Purohit, Aveek and Ryoo, Michael and Sindhwani, Vikas and Lee, Johnny and Vanhoucke, Vincent and others},
	journal={arXiv preprint arXiv:2204.00598},
	year={2022}
}

@inproceedings{zhai2022lit,
	title={{LiT: Zero-Shot Transfer with Locked-image text Tuning}},
	author={Zhai, Xiaohua and Wang, Xiao and Mustafa, Basil and Steiner, Andreas and Keysers, Daniel and Kolesnikov, Alexander and Beyer, Lucas},
	booktitle={CVPR},
	pages={18123--18133},
	year={2022}
}

@article{yang2023vid2seq,
	title={{Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning}},
	author={Yang, Antoine and Nagrani, Arsha and Seo, Paul Hongsuck and Miech, Antoine and Pont-Tuset, Jordi and Laptev, Ivan and Sivic, Josef and Schmid, Cordelia},
	journal={arXiv preprint arXiv:2302.14115},
	year={2023}
}

@article{karamcheti2023language,
	title={{Language-Driven Representation Learning for Robotics}},
	author={Karamcheti, Siddharth and Nair, Suraj and Chen, Annie S and Kollar, Thomas and Finn, Chelsea and Sadigh, Dorsa and Liang, Percy},
	journal={arXiv preprint arXiv:2302.12766},
	year={2023}
}

@article{paiss2023teaching,
	title={{Teaching CLIP to Count to Ten}},
	author={Paiss, Roni and Ephrat, Ariel and Tov, Omer and Zada, Shiran and Mosseri, Inbar and Irani, Michal and Dekel, Tali},
	journal={arXiv preprint arXiv:2302.12066},
	year={2023}
}

@inproceedings{xu2022groupvit,
	title={{GroupViT: Semantic Segmentation Emerges from Text Supervision}},
	author={Xu, Jiarui and De Mello, Shalini and Liu, Sifei and Byeon, Wonmin and Breuel, Thomas and Kautz, Jan and Wang, Xiaolong},
	booktitle={CVPR},
	pages={18134--18144},
	year={2022}
}


@article{recasens2023zorro,
	title={{Zorro: the masked multimodal transformer}},
	author={Recasens, Adri{\`a} and Lin, Jason and Carreira, Jo{\=a}o and Jaegle, Drew and Wang, Luyu and Alayrac, Jean-baptiste and Luc, Pauline and Miech, Antoine and Smaira, Lucas and Hemsley, Ross and others},
	journal={arXiv preprint arXiv:2301.09595},
	year={2023}
}

@article{huang2022mavil,
	title={{MAViL: Masked Audio-Video Learners}},
	author={Huang, Po-Yao and Sharma, Vasu and Xu, Hu and Ryali, Chaitanya and Fan, Haoqi and Li, Yanghao and Li, Shang-Wen and Ghosh, Gargi and Malik, Jitendra and Feichtenhofer, Christoph},
	journal={arXiv preprint arXiv:2212.08071},
	year={2022}
}

@article{tschannen2022image,
	title={{Image-and-Language Understanding from Pixels Only}},
	author={Tschannen, Michael and Mustafa, Basil and Houlsby, Neil},
	journal={arXiv preprint arXiv:2212.08045},
	year={2022}
}

@article{jiang2022vima,
	title={{VIMA: General Robot Manipulation with Multimodal Prompts}},
	author={Jiang, Yunfan and Gupta, Agrim and Zhang, Zichen and Wang, Guanzhi and Dou, Yongqiang and Chen, Yanjun and Fei-Fei, Li and Anandkumar, Anima and Zhu, Yuke and Fan, Linxi},
	journal={arXiv preprint arXiv:2210.03094},
	year={2022}
}

@article{cheng2022vindlu,
	title={{VindLU: A Recipe for Effective Video-and-Language Pretraining}},
	author={Cheng, Feng and Wang, Xizi and Lei, Jie and Crandall, David and Bansal, Mohit and Bertasius, Gedas},
	journal={arXiv preprint arXiv:2212.05051},
	year={2022}
}

@article{yan2022videococa,
	title={{Video-Text Modeling with Zero-Shot Transfer from Contrastive Captioners}},
	author={Yan, Shen and Zhu, Tao and Wang, Zirui and Cao, Yuan and Zhang, Mi and Ghosh, Soham and Wu, Yonghui and Yu, Jiahui},
	journal={arXiv preprint arXiv:2212.04979},
	year={2022}
}

@article{zhao2022learning,
	title={{Learning Video Representations from Large Language Models}},
	author={Zhao, Yue and Misra, Ishan and Kr{\"a}henb{\"u}hl, Philipp and Girdhar, Rohit},
	journal={arXiv preprint arXiv:2212.04501},
	year={2022}
}

@article{nukrai2022text,
	title={{Text-Only Training for Image Captioning using Noise-Injected CLIP}},
	author={Nukrai, David and Mokady, Ron and Globerson, Amir},
	journal={EMNLP},
	year={2022}
}

@article{xue2022clipvip,
	title={{CLIP-ViP: Adapting Pre-trained Image-Text Model to Video-Language Representation Alignment}},
	author={Xue, Hongwei and Sun, Yuchong and Liu, Bei and Fu, Jianlong and Song, Ruihua and Li, Houqiang and Luo, Jiebo},
	journal={arXiv preprint arXiv:2209.06430},
	year={2022}
}

@inproceedings{ju2022prompting,
	title={{Prompting Visual-Language Models for Efficient Video Understanding}},
	author={Ju, Chen and Han, Tengda and Zheng, Kunhao and Zhang, Ya and Xie, Weidi},
	booktitle={ECCV},
	pages={105--124},
	year={2022},
	organization={Springer}
}

@article{lin2022egocentric,
	title={{Egocentric Video-Language Pretraining}},
	author={Lin, Kevin Qinghong and Wang, Alex Jinpeng and Soldan, Mattia and Wray, Michael and Yan, Rui and Xu, Eric Zhongcong and Gao, Difei and Tu, Rongcheng and Zhao, Wenzhe and Kong, Weijie and others},
	journal={NeurIPS},
	year={2022}
}

@article{wang2022language,
	title={{Language Models with Image Descriptors are Strong Few-Shot Video-Language Learners}},
	author={Wang, Zhenhailong and Li, Manling and Xu, Ruochen and Zhou, Luowei and Lei, Jie and Lin, Xudong and Wang, Shuohang and Yang, Ziyi and Zhu, Chenguang and Hoiem, Derek and others},
	journal={arXiv preprint arXiv:2205.10747},
	year={2022}
}

@inproceedings{feichtenhofer2021large,
	title={{A Large-Scale Study on Unsupervised Spatiotemporal Representation Learning}},
	author={Feichtenhofer, Christoph and Fan, Haoqi and Xiong, Bo and Girshick, Ross and He, Kaiming},
	booktitle={CVPR},
	pages={3299--3309},
	year={2021}
}

@article{han2020self,
	title={{Self-supervised Co-training for Video Representation Learning}},
	author={Han, Tengda and Xie, Weidi and Zisserman, Andrew},
	journal={NeurIPS},
	volume={33},
	pages={5679--5690},
	year={2020}
}

@inproceedings{recasens2021brave,
	title={{Broaden Your Views for Self-Supervised Video Learning}},
	author={Recasens, Adria and Luc, Pauline and Alayrac, Jean-Baptiste and Wang, Luyu and Strub, Florian and Tallec, Corentin and Malinowski, Mateusz and P{\u{a}}tr{\u{a}}ucean, Viorica and Altch{\'e}, Florent and Valko, Michal and others},
	booktitle={ICCV},
	pages={1255--1265},
	year={2021}
}

@inproceedings{qian2021spatiotemporal,
	title={{Spatiotemporal Contrastive Video Representation Learning}},
	author={Qian, Rui and Meng, Tianjian and Gong, Boqing and Yang, Ming-Hsuan and Wang, Huisheng and Belongie, Serge and Cui, Yin},
	booktitle={CVPR},
	pages={6964--6974},
	year={2021}
}

@article{guo2022calip,
	title={{CALIP: Zero-Shot Enhancement of CLIP with Parameter-free Attention}},
	author={Guo, Ziyu and Zhang, Renrui and Qiu, Longtian and Ma, Xianzheng and Miao, Xupeng and He, Xuming and Cui, Bin},
	journal={AAAI},
	year={2023}
}

@article{zhou2022learning,
	title={{Learning to Prompt for Vision-Language Models}},
	author={Zhou, Kaiyang and Yang, Jingkang and Loy, Chen Change and Liu, Ziwei},
	journal={IJCV},
	volume={130},
	number={9},
	pages={2337--2348},
	year={2022},
	publisher={Springer}
}

@inproceedings{wu2019long,
	title={{Long-Term Feature Banks for Detailed Video Understanding}},
	author={Wu, Chao-Yuan and Feichtenhofer, Christoph and Fan, Haoqi and He, Kaiming and Krahenbuhl, Philipp and Girshick, Ross},
	booktitle={CVPR},
	pages={284--293},
	year={2019}
}

@article{xie2017s3d,
	title={{Rethinking Spatiotemporal Feature Learning For Video Understanding}},
	author={Xie, Saining and Sun, Chen and Huang, Jonathan and Tu, Zhuowen and Murphy, Kevin},
	journal={arXiv preprint arXiv:1712.04851},
	volume={1},
	number={2},
	pages={5},
	year={2017}
}

@inproceedings{tran2018r2p1d,
	title={{A Closer Look at Spatiotemporal Convolutions for Action Recognition}},
	author={Tran, Du and Wang, Heng and Torresani, Lorenzo and Ray, Jamie and LeCun, Yann and Paluri, Manohar},
	booktitle={CVPR},
	pages={6450--6459},
	year={2018}
}

@inproceedings{wu2022memvit,
	title={{MeMViT: Memory-Augmented Multiscale Vision Transformer for Efficient Long-Term Video Recognition}},
	author={Wu, Chao-Yuan and Li, Yanghao and Mangalam, Karttikeya and Fan, Haoqi and Xiong, Bo and Malik, Jitendra and Feichtenhofer, Christoph},
	booktitle={CVPR},
	pages={13587--13597},
	year={2022}
}

@article{ryoo2022ttm,
	title={{Token Turing Machines}},
	author={Ryoo, Michael S and Gopalakrishnan, Keerthana and Kahatapitiya, Kumara and Xiao, Ted and Rao, Kanishka and Stone, Austin and Lu, Yao and Ibarz, Julian and Arnab, Anurag},
	journal={arXiv preprint arXiv:2211.09119},
	year={2022}
}

@inproceedings{piergiovanni2018superevents,
	title={{Learning Latent Super-Events to Detect Multiple Activities in Videos}},
	author={Piergiovanni, AJ and Ryoo, Michael S},
	booktitle={CVPR},
	pages={5304--5313},
	year={2018}
}

@inproceedings{piergiovanni2019tgm,
	title={{Temporal Gaussian Mixture Layer for Videos}},
	author={Piergiovanni, AJ and Ryoo, Michael},
	booktitle={ICML},
	pages={5152--5161},
	year={2019},
	organization={PMLR}
}

@inproceedings{kahatapitiya2021coarsefine,
	title={{Coarse-Fine Networks for Temporal Activity Detection in Videos}},
	author={Kahatapitiya, Kumara and Ryoo, Michael S},
	booktitle={CVPR},
	pages={8385--8394},
	year={2021}
}

@inproceedings{dai2022ms,
	title={{MS-TCT: Multi-Scale Temporal ConvTransformer for Action Detection}},
	author={Dai, Rui and Das, Srijan and Kahatapitiya, Kumara and Ryoo, Michael S and Br{\'e}mond, Fran{\c{c}}ois},
	booktitle={CVPR},
	pages={20041--20051},
	year={2022}
}

@article{xu2021videoclip,
	title={{VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding}},
	author={Xu, Hu and Ghosh, Gargi and Huang, Po-Yao and Okhonko, Dmytro and Aghajanyan, Armen and Metze, Florian and Zettlemoyer, Luke and Feichtenhofer, Christoph},
	journal={EMNLP},
	year={2021}
}

@article{gu2021vild,
	title={{Open-vocabulary Object Detection via Vision and Language Knowledge Distillation}},
	author={Gu, Xiuye and Lin, Tsung-Yi and Kuo, Weicheng and Cui, Yin},
	journal={ICLR},
	year={2021}
}

@article{minderer2022simple,
	title={{Simple Open-Vocabulary Object Detection with Vision Transformers}},
	author={Minderer, Matthias and Gritsenko, Alexey and Stone, Austin and Neumann, Maxim and Weissenborn, Dirk and Dosovitskiy, Alexey and Mahendran, Aravindh and Arnab, Anurag and Dehghani, Mostafa and Shen, Zhuoran and others},
	journal={ECCV},
	year={2022}
}

@article{alayrac2022flamingo,
	title={{Flamingo: a Visual Language Model for Few-Shot Learning}},
	author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katie and Reynolds, Malcolm and others},
	journal={NeurIPS},
	year={2022}
}

@article{yuan2021florence,
	title={{Florence: A New Foundation Model for Computer Vision}},
	author={Yuan, Lu and Chen, Dongdong and Chen, Yi-Ling and Codella, Noel and Dai, Xiyang and Gao, Jianfeng and Hu, Houdong and Huang, Xuedong and Li, Boxin and Li, Chunyuan and others},
	journal={arXiv preprint arXiv:2111.11432},
	year={2021}
}

@inproceedings{singh2022flava,
	title={{FLAVA: A Foundational Language And Vision Alignment Model}},
	author={Singh, Amanpreet and Hu, Ronghang and Goswami, Vedanuj and Couairon, Guillaume and Galuba, Wojciech and Rohrbach, Marcus and Kiela, Douwe},
	booktitle={CVPR},
	pages={15638--15650},
	year={2022}
}

@article{yao2021filip,
	title={{FILIP: Fine-grained Interactive Language-Image Pre-Training}},
	author={Yao, Lewei and Huang, Runhui and Hou, Lu and Lu, Guansong and Niu, Minzhe and Xu, Hang and Liang, Xiaodan and Li, Zhenguo and Jiang, Xin and Xu, Chunjing},
	journal={arXiv preprint arXiv:2111.07783},
	year={2021}
}

@inproceedings{ji2020action,
	title={{Action Genome: Actions as Composition of Spatio-temporal Scene Graph}},
	author={Ji, Jingwei and Krishna, Ranjay and Fei-Fei, Li and Niebles, Juan Carlos},
	booktitle={CVPR},
	pages={10236--10247},
	year={2020}
}

@article{menon2022visual,
	title={{Visual Classification via Description from Large Language Models}},
	author={Menon, Sachit and Vondrick, Carl},
	journal={arXiv preprint arXiv:2210.07183},
	year={2022}
}

@article{rasheed2022fine,
	title={{Fine-tuned CLIP Models are Efficient Video Learners}},
	author={Rasheed, Hanoona and Khattak, Muhammad Uzair and Maaz, Muhammad and Khan, Salman and Khan, Fahad Shahbaz},
	journal={arXiv preprint arXiv:2212.03640},
	year={2022}
}

@inproceedings{escorcia2016daps,
	title={{DAPs: Deep Action Proposals for Action Understanding}},
	author={Escorcia, Victor and Caba Heilbron, Fabian and Niebles, Juan Carlos and Ghanem, Bernard},
	booktitle={ECCV},
	pages={768--784},
	year={2016},
	organization={Springer}
}

@article{yeung2018every,
	title={{Every Moment Counts: Dense Detailed Labeling of Actions in Complex Videos}},
	author={Yeung, Serena and Russakovsky, Olga and Jin, Ning and Andriluka, Mykhaylo and Mori, Greg and Fei-Fei, Li},
	journal={IJCV},
	volume={126},
	pages={375--389},
	year={2018},
	publisher={Springer}
}

@inproceedings{gu2018ava,
	title={{AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Actions}},
	author={Gu, Chunhui and Sun, Chen and Ross, David A and Vondrick, Carl and Pantofaru, Caroline and Li, Yeqing and Vijayanarasimhan, Sudheendra and Toderici, George and Ricco, Susanna and Sukthankar, Rahul and others},
	booktitle={CVPR},
	pages={6047--6056},
	year={2018}
}

@article{yu2022coca,
	title={{CoCa: Contrastive Captioners are Image-Text Foundation Models}},
	author={Yu, Jiahui and Wang, Zirui and Vasudevan, Vijay and Yeung, Legg and Seyedhosseini, Mojtaba and Wu, Yonghui},
	journal={arXiv preprint arXiv:2205.01917},
	year={2022}
}

@inproceedings{bain2021frozen,
	title={{Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval}},
	author={Bain, Max and Nagrani, Arsha and Varol, G{\"u}l and Zisserman, Andrew},
	booktitle={ICCV},
	pages={1728--1738},
	year={2021}
}

@article{dosovitskiy2020vit,
	title={{An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}},
	author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
	journal={ICLR},
	year={2021}
}

@article{sennrich2015neural,
	title={{Neural Machine Translation of Rare Words with Subword Units}},
	author={Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
	journal={ACL},
	year={2016}
}

@article{oord2018representation,
	title={{Representation Learning with Contrastive Predictive Coding}},
	author={Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
	journal={arXiv preprint arXiv:1807.03748},
	year={2018}
}

@article{loshchilov2017adamw,
	title={{Decoupled Weight Decay Regularization}},
	author={Loshchilov, Ilya and Hutter, Frank},
	journal={ICLR},
	year={2019}
}

@inproceedings{xu2016msrvtt,
	title={{MSR-VTT: A Large Video Description Dataset for Bridging Video and Language}},
	author={Xu, Jun and Mei, Tao and Yao, Ting and Rui, Yong},
	booktitle={CVPR},
	pages={5288--5296},
	year={2016}
}

@article{frome2013devise,
	title={{DeViSE: A Deep Visual-Semantic Embedding Model}},
	author={Frome, Andrea and Corrado, Greg S and Shlens, Jon and Bengio, Samy and Dean, Jeff and Ranzato, Marc'Aurelio and Mikolov, Tomas},
	journal={NeurIPS},
	volume={26},
	year={2013}
}

@article{akata2015ale,
	title={{Label-Embedding for Image Classification}},
	author={Akata, Zeynep and Perronnin, Florent and Harchaoui, Zaid and Schmid, Cordelia},
	journal={PAMI},
	volume={38},
	number={7},
	pages={1425--1438},
	year={2015},
	publisher={IEEE}
}

@inproceedings{akata2015sje,
	title={{Evaluation of Output Embeddings for Fine-Grained Image Classification}},
	author={Akata, Zeynep and Reed, Scott and Walter, Daniel and Lee, Honglak and Schiele, Bernt},
	booktitle={CVPR},
	pages={2927--2936},
	year={2015}
}

@inproceedings{romera2015eszsl,
	title={{An embarrassingly simple approach to zero-shot learning}},
	author={Romera-Paredes, Bernardino and Torr, Philip},
	booktitle={ICML},
	pages={2152--2161},
	year={2015},
	organization={PMLR}
}

@inproceedings{zhang2017dem,
	title={{Learning a Deep Embedding Model for Zero-Shot Learning
	}},
	author={Zhang, Li and Xiang, Tao and Gong, Shaogang},
	booktitle={CVPR},
	pages={2021--2030},
	year={2017}
}

@article{ghosh2020gcn,
	title={{All About Knowledge Graphs for Actions}},
	author={Ghosh, Pallabi and Saini, Nirat and Davis, Larry S and Shrivastava, Abhinav},
	journal={arXiv preprint arXiv:2008.12432},
	year={2020}
}

@inproceedings{chen2021elaborative,
	title={Elaborative rehearsal for zero-shot action recognition},
	author={Chen, Shizhe and Huang, Dong},
	booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
	pages={13638--13647},
	year={2021}
}

@article{Kahatapitiya2023VicTRVT,
	title={VicTR: Video-conditioned Text Representations for Activity Recognition},
	author={Kumara Kahatapitiya and Anurag Arnab and Arsha Nagrani and Michael S. Ryoo},
	journal={ArXiv},
	year={2023},
	volume={abs/2304.02560}
}


@inproceedings{gberta_2021_ICML,
	author  = {Gedas Bertasius and Heng Wang and Lorenzo Torresani},
	title = {Is Space-Time Attention All You Need for Video Understanding?},
	booktitle   = ICML, 
	month = {July},
	year = {2021}
}

@inproceedings{caron2021emerging,
	title={Emerging Properties in Self-Supervised Vision Transformers},
	author={Caron, Mathilde and Touvron, Hugo and Misra, Ishan and J\'egou, Herv\'e  and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
	booktitle=ICCV,
	year={2021}
}

@inproceedings{grill2020bootstrap,
	title={Bootstrap your own latent: A new approach to self-supervised learning},
	author={Grill, Jean-Bastien and Strub, Florian and Altch{\'e}, Florent and Tallec, Corentin and Richemond, Pierre H and Buchatskaya, Elena and Doersch, Carl and Pires, Bernardo Avila and Guo, Zhaohan Daniel and Azar, Mohammad Gheshlaghi and others},
	booktitle=NIPS,
	year={2020}
}


@article{recasens2021broaden,
	title={Broaden Your Views for Self-Supervised Video Learning},
	author={Recasens, Adri{\`a} and Luc, Pauline and Alayrac, Jean-Baptiste and Wang, Luyu and Strub, Florian and Tallec, Corentin and Malinowski, Mateusz and Patraucean, Viorica and Altch{\'e}, Florent and Valko, Michal and others},
	journal=ICCV,
	year={2021}
}

@article{vaswani2017attention,
	title={Attention is all you need},
	author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
	journal=NIPS,
	year={2017}
}

@article{devlin2018bert,
	title={Bert: Pre-training of deep bidirectional transformers for language understanding},
	author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	journal=arxiv,
	year={2018}
}

@inproceedings{carion2020end,
	title={End-to-end object detection with transformers},
	author={Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
	booktitle=ECCV,
	year={2020},
}

@inproceedings{wang2018non,
	title={Non-local neural networks},
	author={Wang, Xiaolong and Girshick, Ross and Gupta, Abhinav and He, Kaiming},
	booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages={7794--7803},
	year={2018}
}

@article{wang2020end,
	title={End-to-End Video Instance Segmentation with Transformers},
	author={Wang, Yuqing and Xu, Zhaoliang and Wang, Xinlong and Shen, Chunhua and Cheng, Baoshan and Shen, Hao and Xia, Huaxia},
	journal=arxiv,
	year={2020}
}

@inproceedings{zhang2020dynamic,
	title={Dynamic graph message passing networks},
	author={Zhang, Li and Xu, Dan and Arnab, Anurag and Torr, Philip HS},
	booktitle=CVPR,
	year={2020}
}

@article{dosovitskiy2020image,
	title={An image is worth 16x16 words: Transformers for image recognition at scale},
	author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
	journal=ICLR,
	year={2021}
}

@InProceedings{pmlr-v139-touvron21a,
	title =     {Training data-efficient image transformers and distillation through attention},
	author =    {Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and Jegou, Herve},
	booktitle = ICML,
	year =      {2021},
}

@article{Steiner2021HowTT,
	title={How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers},
	author={Andreas Steiner and Alexander Kolesnikov and Xiaohua Zhai and Ross Wightman and Jakob Uszkoreit and Lucas Beyer},
	journal=arxiv,
	year={2021},
}

@article{liu2021swin,
	title={Swin transformer: Hierarchical vision transformer using shifted windows},
	author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
	journal=arxiv,
	year={2021}
}

@article{liu2021video,
  title={Video Swin Transformer},
  author={Liu, Ze and Ning, Jia and Cao, Yue and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Hu, Han},
  journal=arxiv,
  year={2021}
}

@article{Ryoo2021TokenLearnerWC,
	title={TokenLearner: What Can 8 Learned Tokens Do for Images and Videos?},
	author={Michael S. Ryoo and A. J. Piergiovanni and Anurag Arnab and Mostafa Dehghani and Anelia Angelova},
	journal=arxiv,
	year={2021},
}

@article{sharir2021image,
	title={An Image is Worth 16x16 Words, What is a Video Worth?},
	author={Sharir, Gilad and Noy, Asaf and Zelnik-Manor, Lihi},
	journal=arxiv,
	year={2021}
}


@article{fan2021multiscale,
	title={Multiscale Vision Transformers},
	author={Fan, Haoqi and Xiong, Bo and Mangalam, Karttikeya and Li, Yanghao and Yan, Zhicheng and Malik, Jitendra and Feichtenhofer, Christoph},
	journal=ICCV,
	year={2021}
}


@inproceedings{komodakis2018unsupervised,
	title={Unsupervised representation learning by predicting image rotations},
	author={Komodakis, Nikos and Gidaris, Spyros},
	booktitle=ICLR,
	year={2018}
}

@inproceedings{pathak2016context,
	title={Context encoders: Feature learning by inpainting},
	author={Pathak, Deepak and Krahenbuhl, Philipp and Donahue, Jeff and Darrell, Trevor and Efros, Alexei A},
	booktitle=CVPR,
	year={2016}
}

@inproceedings{vincent2008extracting,
	title={Extracting and composing robust features with denoising autoencoders},
	author={Vincent, Pascal and Larochelle, Hugo and Bengio, Yoshua and Manzagol, Pierre-Antoine},
	booktitle=ICML,
	year={2008}
}

@inproceedings{noroozi2016unsupervised,
	title={Unsupervised learning of visual representations by solving jigsaw puzzles},
	author={Noroozi, Mehdi and Favaro, Paolo},
	booktitle=ECCV,
	year={2016},
}

@inproceedings{doersch2015unsupervised,
	title={Unsupervised visual representation learning by context prediction},
	author={Doersch, Carl and Gupta, Abhinav and Efros, Alexei A},
	booktitle=ICCV,
	year={2015}
}

@inproceedings{doersch2017multi,
	title={Multi-task self-supervised visual learning},
	author={Doersch, Carl and Zisserman, Andrew},
	booktitle=ICCV,
	year={2017}
}

@inproceedings{zhang2016colorful,
	title={Colorful image colorization},
	author={Zhang, Richard and Isola, Phillip and Efros, Alexei A},
	booktitle=ECCV,
	year={2016},
}

@article{Ramasinghe2021ConditionalGM,
	title={Conditional Generative Modeling via Learning the Latent Space},
	author={Sameera Ramasinghe and Kanchana Ranasinghe and Salman Hameed Khan and Nick Barnes and Stephen Gould},
	journal=ICLR,
	year={2021},
	volume={abs/2010.03132}
}

@inproceedings{chen2020simple,
	title={A simple framework for contrastive learning of visual representations},
	author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
	booktitle=ICML,
	year={2020},
}

@inproceedings{he2020momentum,
	title={Momentum contrast for unsupervised visual representation learning},
	author={He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
	booktitle=CVPR,
	year={2020}
}

@article{chen2020improved,
	title={Improved baselines with momentum contrastive learning},
	author={Chen, Xinlei and Fan, Haoqi and Girshick, Ross and He, Kaiming},
	journal=arxiv,
	year={2020}
}

@article{chen2021empirical,
	title={An empirical study of training self-supervised visual transformers},
	author={Chen, Xinlei and Xie, Saining and He, Kaiming},
	journal=arxiv,
	year={2021}
}

@inproceedings{AMDIM,
	title={Learning Representations by Maximizing Mutual Information Across Views},
	author={Philip Bachman and R Devon Hjelm and William Buchwalter},
	booktitle=NIPS,
	year={2019}
}

@inproceedings{dosovitskiy2014discriminative,
	title={Discriminative unsupervised feature learning with convolutional neural networks},
	author={Dosovitskiy, Alexey and Springenberg, Jost Tobias and Riedmiller, Martin and Brox, Thomas},
	booktitle=NIPS,
	year={2014}
}

@inproceedings{henaff2019data,
	title={Data-efficient image recognition with contrastive predictive coding},
	author={H{\'e}naff, Olivier J and Razavi, Ali and Doersch, Carl and Eslami, SM and Oord, Aaron van den},
	booktitle = ICML,
	year={2020}
}

@article{le2020contrastive,
	title={Contrastive Representation Learning: A Framework and Review},
	author={Le-Khac, Phuc H and Healy, Graham and Smeaton, Alan F},
	journal={IEEE Access},
	year={2020},
	publisher={IEEE}
}

% Varying Frame rates
@inproceedings{zhu2018random,
  title={Random temporal skipping for multirate video analysis},
  author={Zhu, Yi and Newsam, Shawn},
  booktitle=ACCV,
  year={2018},
}

@inproceedings{kahatapitiya2021coarse,
  title={Coarse-Fine Networks for Temporal Activity Detection in Videos},
  author={Kahatapitiya, Kumara and Ryoo, Michael S},
  booktitle=CVPR,
  year={2021}
}

@inproceedings{piergiovanni2020evolving,
      title={Evolving Losses for Unsupervised Video Representation Learning}, 
      author={AJ Piergiovanni and Anelia Angelova and Michael S. Ryoo},
      year={2020},
      booktitle=CVPR,
}

@inproceedings{zhang2018dynamic,
  title={Dynamic temporal pyramid network: A closer look at multi-scale modeling for activity detection},
  author={Zhang, Da and Dai, Xiyang and Wang, Yuan-Fang},
  booktitle=ACCV,
  year={2018},
}

@inproceedings{yang2020temporal,
  title={Temporal pyramid network for action recognition},
  author={Yang, Ceyuan and Xu, Yinghao and Shi, Jianping and Dai, Bo and Zhou, Bolei},
  booktitle=CVPR,
  year={2020}
}

@article{varol2021synthetic,
  title={Synthetic humans for action recognition from unseen viewpoints},
  author={Varol, G{\"u}l and Laptev, Ivan and Schmid, Cordelia and Zisserman, Andrew},
  journal=IJCV,
  volume={129},
  number={7},
  pages={2264--2287},
  year={2021},
  publisher={Springer}
}


@inproceedings{PIRL,
	title={Self-Supervised Learning of Pretext-Invariant Representations},
	author={Ishan Misra and Laurens van der Maaten},
	booktitle=CVPR,
	year={2020},
}

@article{tian2019contrastive,
	title={Contrastive Multiview Coding},
	author={Tian, Yonglong and Krishnan, Dilip and Isola, Phillip},
	journal=ECCV,
	year={2020}
}

@inproceedings{tian2020makes,
	title={What makes for good views for contrastive learning},
	author={Tian, Yonglong and Sun, Chen and Poole, Ben and Krishnan, Dilip and Schmid, Cordelia and Isola, Phillip},
	booktitle=NIPS,
	year={2020}
}

@article{oord2018representation,
	title={Representation learning with contrastive predictive coding},
	author={Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
	journal=NIPS,
	year={2018}
}

@inproceedings{caron2018DeepCF,
	title={Deep Clustering for Unsupervised Learning of Visual Features},
	author={Mathilde Caron and Piotr Bojanowski and Armand Joulin and Matthijs Douze},
	booktitle=ECCV,
	year={2018}
}


@inproceedings{caron2020unsupervised,
	title={Unsupervised learning of visual features by contrasting cluster assignments},
	author={Caron, Mathilde and Misra, Ishan and Mairal, Julien and Goyal, Priya and Bojanowski, Piotr and Joulin, Armand},
	booktitle=NIPS,
	year={2020}
}

@inproceedings{alwassel2019self,
	title={Self-Supervised Learning by Cross-Modal Audio-Video Clustering},
	author={Alwassel, Humam and Mahajan, Dhruv and Torresani, Lorenzo and Ghanem, Bernard and Tran, Du},
	booktitle=NIPS,
	year={2020}
}

@inproceedings{asano2020selflabelling,
	title={Self-labelling via simultaneous clustering and representation learning}, 
	author={Yuki Markus Asano and Christian Rupprecht and Andrea Vedaldi},
	year={2020},
	booktitle=ICLR
}

@inproceedings{bautista2016cliquecnn,
	title={CliqueCNN: Deep Unsupervised Exemplar Learning}, 
	author={Miguel A. Bautista and Artsiom Sanakoyeu and Ekaterina Sutter and Bj√∂rn Ommer},
	year={2016},
	booktitle=NIPS
}

@inproceedings{huang2019unsupervised,
	title={Unsupervised Deep Learning by Neighbourhood Discovery}, 
	author={Jiabo Huang and Qi Dong and Shaogang Gong and Xiatian Zhu},
	year={2019},
	booktitle=ICML
}

@inproceedings{tian2017deepcluster,
	title={Deepcluster: A general clustering framework based on deep learning},
	author={Tian, Kai and Zhou, Shuigeng and Guan, Jihong},
	booktitle={ECML/PKDD},
	year={2017},
}

@inproceedings{xie2016unsupervised,
	title={Unsupervised Deep Embedding for Clustering Analysis}, 
	author={Junyuan Xie and Ross Girshick and Ali Farhadi},
	year={2016},
	booktitle=ICML
}


@InProceedings{mathieu2015deep,
	title={Deep multi-scale video prediction beyond mean square error},
	author={Mathieu, Michael and Couprie, Camille and LeCun, Yann},
	booktitle=ICLR,
	year={2016}
}

@inproceedings{PatrauceanHC16,
	author    = {Viorica P{\u a}tr{\u a}ucean and
	Ankur Handa and
	Roberto Cipolla},
	title     = {Spatio-temporal video autoencoder with differentiable memory},
	booktitle = {ICLR (Workshop)},
	year      = {2016}
}

@InProceedings{pmlr-v37-srivastava15, 
	title = {Unsupervised Learning of Video Representations using LSTMs}, 
	author = {Nitish Srivastava and Elman Mansimov and Ruslan Salakhudinov},
	booktitle=ICML,
	year = {2015}, 
}

@inproceedings{Vondrick16a,
	author = {Vondrick, Carl and Pirsiavash, Hamed and Torralba, Antonio},
	title = {Generating Videos with Scene Dynamics},
	year = {2016},
	booktitle = NIPS,
}


@inproceedings{vondrick2018tracking,
	title={Tracking emerges by colorizing videos},
	author={Vondrick, Carl and Shrivastava, Abhinav and Fathi, Alireza and Guadarrama, Sergio and Murphy, Kevin},
	booktitle=ECCV,
	year={2018}
}

@InProceedings{Agrawal_2015_ICCV,
	author = {Agrawal, Pulkit and Carreira, Joao and Malik, Jitendra},
	title = {Learning to See by Moving},
	booktitle = ICCV,
	year = {2015}
}

@InProceedings{Goroshin_2015_ICCV,
	author = {Goroshin, Ross and Bruna, Joan and Tompson, Jonathan and Eigen, David and LeCun, Yann},
	title = {Unsupervised Learning of Spatiotemporally Coherent Metrics},
	booktitle = ICCV,
	year = {2015}
}


@InProceedings{DBLP:journals/corr/IsolaZKA15,
	author    = {Phillip Isola and Daniel Zoran and Dilip Krishnan and Edward H. Adelson},
	title     = {Learning visual groups from co-occurrences in space and time},
	booktitle = ICLR,
	year      = {2016},
}

  @conference{Misra-2016-5596,
	author = {Ishan Misra and C. Lawrence Zitnick and Martial Hebert},
	title = {Shuffle and Learn: Unsupervised Learning using Temporal Order Verification},
	booktitle = ECCV,
	year = {2016},
}

@InProceedings{7410677,
	author={Wang, Xiaolong and Gupta, Abhinav},
	booktitle=ICCV, 
	title={Unsupervised Learning of Visual Representations Using Videos},
        year={2015},
}

@inproceedings{walker2016uncertain,
	title={An uncertain future: Forecasting from static images using variational autoencoders},
	author={Walker, Jacob and Doersch, Carl and Gupta, Abhinav and Hebert, Martial},
	booktitle=ECCV,
	year={2016},
}

@article{Feichtenhofer_large,
	title={A Large-Scale Study on Unsupervised Spatiotemporal Representation Learning},
	author={Feichtenhofer, Christoph and Fan, Haoqi and Xiong, Bo and Girshick, Ross and He, Kaiming},
	journal=arxiv,
	year={2021}
}

@inproceedings{han2019video,
	title={Video representation learning by dense predictive coding},
	author={Han, Tengda and Xie, Weidi and Zisserman, Andrew},
	booktitle=ICCV,
	year={2019}
}

@article{qian2020spatiotemporal,
	title={Spatiotemporal contrastive video representation learning},
	author={Qian, Rui and Meng, Tianjian and Gong, Boqing and Yang, Ming-Hsuan and Wang, Huisheng and Belongie, Serge and Cui, Yin},
	journal=CVPR,
	year={2021}
}

@article{hjelm2020representation,
	title={Representation Learning with Video Deep InfoMax},
	author={Devon, R and others},
	journal=arxiv,
	year={2020}
}

@InProceedings{kinetics400,
  year		= {2017},
  booktitle	= CVPR,
  title		= {Quo Vadis, Action Recognition? {A} New Model and the {Kinetics} Dataset},
  author	= {Carreira, Joao and Zisserman, Andrew}
}

@article{imagenet,
Author = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},
Title = {ImageNet Large Scale Visual Recognition Challenge},
Year = {2015},
journal   = IJCV,
}

@InProceedings{kingma15adam,
  author	= {Kingma, Diederik P. and Ba, Jimmy},
  title		= {Adam: A Method for Stochastic Optimization},
  booktitle	= ICLR,
  year		= {2015}
}

@Article{chen2021mocov3,
  author  = {Xinlei Chen* and Saining Xie* and Kaiming He},
  title   = {An Empirical Study of Training Self-Supervised Vision Transformers},
  journal = arxiv,
  year    = {2021},
}

@article{soomro2012ucf,
  title         = {{UCF101}: {A} dataset of 101 human actions classes from videos in the wild},
  author        = {Soomro, Khurram and Zamir, Amir Roshan and Shah, Mubarak},
  journal       = arxiv,
  year          = {2012}
}

@article{xiao2021modist,
  title={MoDist: Motion Distillation for Self-supervised Video Representation Learning},
  author={Xiao, Fanyi and Tighe, Joseph and Modolo, Davide},
  journal=arxiv,
  year={2021}
}


@inproceedings{
naseer2021intriguing,
title={Intriguing Properties of Vision Transformers},
author={Muzammal Naseer and Kanchana Ranasinghe and Salman Khan and Munawar Hayat and Fahad Khan and Ming-Hsuan Yang},
booktitle=NIPS,
year={2021},
url={https://openreview.net/forum?id=o2mbl-Hmfgd}
}



@article{khan2021transformers,
  title={Transformers in vision: A survey},
  author={Khan, Salman and Naseer, Muzammal and Hayat, Munawar and Zamir, Syed Waqas and Khan, Fahad Shahbaz and Shah, Mubarak},
  journal=arxiv,
  year={2021}
}


@inproceedings{wang2021removing,
  title={Removing the Background by Adding the Background: Towards Background Robust Self-supervised Video Representation Learning},
  author={Wang, Jinpeng and Gao, Yuting and Li, Ke and Lin, Yiqi and Ma, Andy J and Cheng, Hao and Peng, Pai and Ji, Rongrong and Sun, Xing},
  booktitle=CVPR,
  year={2021}
}

@article{Huang2021SSVR,
  title={Self-supervised Video Representation Learning by Context and Motion Decoupling},
  author={Lianghua Huang and Yu Liu and Bin Wang and Pan Pan and Yinghui Xu and Rong Jin},
  journal=CVPR,
  year={2021},
}

@InProceedings{Diba_2021_ICCV,
    author    = {Diba, Ali and Sharma, Vivek and Safdari, Reza and Lotfi, Dariush and Sarfraz, Saquib and Stiefelhagen, Rainer and Van Gool, Luc},
    title     = {Vi2CLR: Video and Image for Visual Contrastive Learning of Representation},
    booktitle = ICCV,
    year      = {2021},
}

@InProceedings{Lin_2021_ICCV,
    author    = {Lin, Yuanze and Guo, Xun and Lu, Yan},
    title     = {Self-Supervised Video Representation Learning With Meta-Contrastive Network},
    booktitle = ICCV,
    year      = {2021},
}

@InProceedings{Huang_2021_ICCV,
    author    = {Huang, Deng and Wu, Wenhao and Hu, Weiwen and Liu, Xu and He, Dongliang and Wu, Zhihua and Wu, Xiangmiao and Tan, Mingkui and Ding, Errui},
    title     = {ASCNet: Self-Supervised Video Representation Learning With Appearance-Speed Consistency},
    booktitle = ICCV,
    year      = {2021},
}

@InProceedings{Jenni_2021_ICCV,
    author    = {Jenni, Simon and Jin, Hailin},
    title     = {Time-Equivariant Contrastive Video Representation Learning},
    booktitle = ICCV,
    year      = {2021},
}

@inproceedings{pan2021videomoco,
  title={Videomoco: Contrastive video representation learning with temporally adversarial examples},
  author={Pan, Tian and Song, Yibing and Yang, Tianyu and Jiang, Wenhao and Liu, Wei},
  booktitle=CVPR,
  year={2021}
}

@inproceedings{Behrmann2021LongSV,
  title={Long Short View Feature Decomposition via Contrastive Video Representation Learning},
  author={Nadine Behrmann and Mohsen Fayyaz and Juergen Gall and Mehdi Noroozi},
  booktitle=ICCV,
  year={2021}
}

@InProceedings{Hu_2021_ICCV,
    author    = {Hu, Kai and Shao, Jie and Liu, Yuan and Raj, Bhiksha and Savvides, Marios and Shen, Zhiqiang},
    title     = {Contrast and Order Representations for Video Self-Supervised Learning},
    booktitle = ICCV,
    year      = {2021}
}

@inproceedings{chen2021rspnet,
  title={Rspnet: Relative speed perception for unsupervised video representation learning},
  author={Chen, Peihao and Huang, Deng and He, Dongliang and Long, Xiang and Zeng, Runhao and Wen, Shilei and Tan, Mingkui and Gan, Chuang},
  booktitle=AAAI,
  volume={1},
  year={2021}
}

@article{tarvainen2017mean,
  title={Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results},
  author={Tarvainen, Antti and Valpola, Harri},
  journal=arxiv,
  year={2017}
}

@misc{fan2020pyslowfast,
  author =       {Haoqi Fan and Yanghao Li and Bo Xiong and Wan-Yen Lo and
                  Christoph Feichtenhofer},
  title =        {PySlowFast},
  howpublished = {\url{https://github.com/facebookresearch/slowfast}},
  year =         {2020}
}

@misc{rw2019timm,
  author = {Ross Wightman},
  title = {PyTorch Image Models},
  year = {2019},
  publisher = {GitHub},
  journal = {GitHub repository},
  doi = {10.5281/zenodo.4414861},
  howpublished = {\url{https://github.com/rwightman/pytorch-image-models}}
}


@inproceedings{chen2021deep,
  title={Deep analysis of cnn-based spatio-temporal representations for action recognition},
  author={Chen, Chun-Fu Richard and Panda, Rameswar and Ramakrishnan, Kandan and Feris, Rogerio and Cohn, John and Oliva, Aude and Fan, Quanfu},
  booktitle=CVPR,
  year={2021}
}

@inproceedings{xie2018rethinking,
  title={Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification},
  author={Xie, Saining and Sun, Chen and Huang, Jonathan and Tu, Zhuowen and Murphy, Kevin},
  booktitle=ECCV,
  year={2018}
}

@inproceedings{goyal2017something,
      title={The "something something" video database for learning and evaluating visual common sense},
      author={Raghav Goyal and Samira Ebrahimi Kahou and Vincent Michalski and Joanna Materzy≈Ñska and Susanne Westphal and Heuna Kim and Valentin Haenel and Ingo Fruend and Peter Yianilos and Moritz Mueller-Freitag and Florian Hoppe and Christian Thurau and Ingo Bax and Roland Memisevic},
      year={2017},
      booktitle=arXiv,
}


@article{zhu_arxiv2020_comprehensiveVideo,
  title={A comprehensive study of deep video action recognition},
  author={Zhu, Yi and Li, Xinyu and Liu, Chunhui and Zolfaghari, Mohammadreza and Xiong, Yuanjun and Wu, Chongruo and Zhang, Zhi and Tighe, Joseph and Manmatha, R and Li, Mu},
  journal=arxiv,
  year={2020}
}

@inproceedings{youtube8M,
title	= {YouTube-8M: A Large-Scale Video Classification Benchmark},
author	= {Sami Abu-El-Haija and Nisarg Kothari and Joonseok Lee and Apostol (Paul) Natsev and George Toderici and Balakrishnan Varadarajan and Sudheendra Vijayanarasimhan},
year	= {2016},
booktitle	= arxiv
}

@article{Dave2021TCLRTC,
  title={{TCLR}: Temporal Contrastive Learning for Video Representation},
  author={Ishan Rajendra Dave and Rohit Gupta and Mamshad Nayeem Rizve and Mubarak Shah},
  journal=arxiv,
  year={2021},
}

@inproceedings{Zamir2021Restormer,
    title={Restormer: Efficient Transformer for High-Resolution Image Restoration}, 
    author={Syed Waqas Zamir and Aditya Arora and Salman Khan and Munawar Hayat 
            and Fahad Shahbaz Khan and Ming-Hsuan Yang},
    booktitle= CVPR,
    year={2022}
}

@article{Ran2021SVT,
  title={Self-supervised Video Transformer},
  author={Kanchana Ranasinghe and Muzammal Naseer and Salman Hameed Khan and Fahad Shahbaz Khan and Michael S. Ryoo},
  journal={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2021},
  pages={2864-2874}
}

@article{Tong2022VidMAE,
  title={VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training},
  author={Zhan Tong and Yibing Song and Jue Wang and Limin Wang},
  journal={ArXiv},
  year={2022},
  volume={abs/2203.12602}
}

@inproceedings{mean_teacher,
  author =        {Antti Tarvainen and Harri Valpola},
  booktitle =     {{NIPS}},
  pages =         {1195--1204},
  title =         {Mean teachers are better role models: Weight-averaged
                   consistency targets improve semi-supervised deep
                   learning results},
  year =          {2017},
}

@inproceedings{fixmatch,
  author =        {Sohn, Kihyuk and Berthelot, David and Li, Chun-Liang and
                   Zhang, Zizhao and Carlini, Nicholas and Cubuk, Ekin D and
                   Kurakin, Alex and Zhang, Han and Raffel, Colin},
  booktitle =     {NeurIPS},
  title =         {Fixmatch: Simplifying semi-supervised learning with
                   consistency and confidence},
  year =          {2020},
}

@inproceedings{remixmatch,
  author =        {David Berthelot and Nicholas Carlini and
                   Ekin D. Cubuk and Alex Kurakin and Kihyuk Sohn and
                   Han Zhang and Colin Raffel},
  booktitle =     {{ICLR}},
  title =         {ReMixMatch: Semi-Supervised Learning with
                   Distribution Alignment and Augmentation Anchoring},
  year =          {2020},
}

@article{Li2022MaskedUS,
  title={Masked Unsupervised Self-training for Zero-shot Image Classification},
  author={Junnan Li and Silvio Savarese and Steven C. H. Hoi},
  journal={ArXiv},
  year={2022},
  volume={abs/2206.02967}
}

@article{Kahana2022ImprovingZM,
  title={Improving Zero-Shot Models with Label Distribution Priors},
  author={Jonathan Kahana and Niv Cohen and Yedid Hoshen},
  journal={ArXiv},
  year={2022},
  volume={abs/2212.00784}
}

@inproceedings{liu2011recognizing,
  title={Recognizing human actions by attributes},
  author={Liu, Jingen and Kuipers, Benjamin and Savarese, Silvio},
  booktitle={CVPR},
  year={2011},
}

@article{zellers2017zero,
  title={Zero-shot activity recognition with verb attribute induction},
  author={Zellers, Rowan and Choi, Yejin},
  journal={arXiv preprint arXiv:1707.09468},
  year={2017}
}

@inproceedings{jain2015objects2action,
  title={Objects2action: Classifying and localizing actions without any video example},
  author={Jain, Mihir and Van Gemert, Jan C and Mensink, Thomas and Snoek, Cees GM},
  booktitle={ICCV},
  year={2015}
}


@article{xu2017transductive,
  title={Transductive zero-shot action recognition by word-vector embedding},
  author={Xu, Xun and Hospedales, Timothy and Gong, Shaogang},
  journal={IJCV},
  year={2017},
  publisher={Springer}
}


@inproceedings{gao2019know,
  title={I know the relationships: Zero-shot action recognition via two-stream graph convolutional networks and knowledge graphs},
  author={Gao, Junyu and Zhang, Tianzhu and Xu, Changsheng},
  booktitle={AAAI},
  year={2019}
}

@article{gao2020learning,
  title={Learning to model relationships for zero-shot video classification},
  author={Gao, Junyu and Zhang, Tianzhu and Xu, Changsheng},
  journal={TPAMI},
  year={2020},
  publisher={IEEE}
}

@inproceedings{brattoli2020rethinking,
  title={Rethinking zero-shot video classification: End-to-end training for realistic applications},
  author={Brattoli, Biagio and Tighe, Joseph and Zhdanov, Fedor and Perona, Pietro and Chalupka, Krzysztof},
  booktitle={CVPR},
  year={2020}
}

@article{Balestriero2023ACO,
  title={A Cookbook of Self-Supervised Learning},
  author={Randall Balestriero and Mark Ibrahim and Vlad Sobal and Ari S. Morcos and Shashank Shekhar and Tom Goldstein and Florian Bordes and Adrien Bardes and Gr{\'e}goire Mialon and Yuandong Tian and Avi Schwarzschild and Andrew Gordon Wilson and Jonas Geiping and Quentin Garrido and Pierre Fernandez and Amir Bar and Hamed Pirsiavash and Yann LeCun and Micah Goldblum},
  journal={ArXiv},
  year={2023},
  volume={abs/2304.12210}
}


@misc{brown2020language,
  author = {Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
  title = {Language Models are Few-Shot Learners},
  year = {2020},
  eprint = {2005.14165},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL}
}


@article{Bardes2021VICRegVR,
  title={VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning},
  author={Adrien Bardes and Jean Ponce and Yann LeCun},
  journal={ArXiv},
  year={2021},
  volume={abs/2105.04906}
}

@inproceedings{miller1995wordnet,
  title={WordNet: a lexical database for English},
  author={Miller, George A},
  booktitle={Communications of the ACM},
  pages={39--41},
  year={1995},
  publisher={ACM}
}

@incollection{fellbaum2005wordnet,
  author = {Fellbaum, Christiane},
  title = {WordNet and WordNets},
  booktitle = {Encyclopedia of Language \& Linguistics},
  edition = {2nd},
  editor = {Brown, Keith and others},
  publisher = {Elsevier},
  year = {2005},
  pages = {665--670},
}

@article{Chantry_SSLV,
  title={Self-Supervised Learning for Videos: A Survey},
  author={Madeline Chantry Schiappa and Yogesh Singh Rawat and Mubarak Shah},
  journal={ACM Computing Surveys},
  year={2022}
}

@inproceedings{ma2022xclip,
  title={Expanding Language-Image Pretrained Models for General Video Recognition},
  author={Bolin Ni and Houwen Peng and Minghao Chen and Songyang Zhang and Gaofeng Meng and Jianlong Fu and Shiming Xiang and Haibin Ling},
  booktitle={European Conference on Computer Vision},
  year={2022}
}

@inproceedings{other_xclip,
	title={{X-CLIP: End-to-End Multi-grained Contrastive Learning for Video-Text Retrieval}},
	author={Ma, Yiwei and Xu, Guohai and Sun, Xiaoshuai and Yan, Ming and Zhang, Ji and Ji, Rongrong},
	booktitle={ACMMM},
	pages={638--647},
	year={2022}
}


@article{Zeng2022LearningTS,
  title={Learning Transferable Spatiotemporal Representations from Natural Script Knowledge},
  author={Ziyun Zeng and Yuying Ge and Xihui Liu and Bin Chen and Ping Luo and Shutao Xia and Yixiao Ge},
  journal={ArXiv},
  year={2022},
  volume={abs/2209.15280}
}

@article{Zellers2021MERLOTMN,
  title={MERLOT: Multimodal Neural Script Knowledge Models},
  author={Rowan Zellers and Ximing Lu and Jack Hessel and Youngjae Yu and Jae Sung Park and Jize Cao and Ali Farhadi and Yejin Choi},
  journal={ArXiv},
  year={2021},
  volume={abs/2106.02636}
}

@article{Zhao2022LearningVR,
  title={Learning Video Representations from Large Language Models},
  author={Yue Zhao and Ishan Misra and Philipp Krahenbuhl and Rohit Girdhar},
  journal={ArXiv},
  year={2022},
  volume={abs/2212.04501}
}

@article{Akbari2021VATTTF,
  title={VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text},
  author={Hassan Akbari and Linagzhe Yuan and Rui Qian and Wei-Hong Chuang and Shih-Fu Chang and Yin Cui and Boqing Gong},
  journal={ArXiv},
  year={2021},
  volume={abs/2104.11178}
}

@article{ranasinghe2022perceptual,
  title={Perceptual Grouping in Contrastive Vision-Language Models},
  author={Kanchana Ranasinghe and Brandon McKinzie and Sachin Ravi and Yinfei Yang and Alexander Toshev and Jonathon Shlens},
  journal=ICCV,
  year={2023}
}

@article{Xu2023LearningOS,
  title={Learning Open-vocabulary Semantic Segmentation Models From Natural Language Supervision},
  author={Jilan Xu and Junlin Hou and Yuejie Zhang and Rui Feng and Yi Wang and Yu Qiao and Weidi Xie},
  journal={ArXiv},
  year={2023}
}

@misc{liu2023llava,
      title={Visual Instruction Tuning}, 
      author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
      publisher={arXiv:2304.08485},
      year={2023},
}

@article{Ryoo2006RecognitionOC,
  title={Recognition of Composite Human Activities through Context-Free Grammar Based Representation},
  author={Michael S. Ryoo and Jake K. Aggarwal},
  journal={2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)},
  year={2006},
  volume={2},
  pages={1709-1718},
  url={https://api.semanticscholar.org/CorpusID:14039104}
}

@article{Aggarwal2011HumanAA,
  title={Human activity analysis},
  author={Jake K. Aggarwal and Michael S. Ryoo},
  journal={ACM Computing Surveys (CSUR)},
  year={2011},
  volume={43},
  pages={1 - 43},
  url={https://api.semanticscholar.org/CorpusID:5388357}
}

@article{Zhao2023ASO,
  title={A Survey of Large Language Models},
  author={Wayne Xin Zhao and Kun Zhou and Junyi Li and Tianyi Tang and Xiaolei Wang and Yupeng Hou and Yingqian Min and Beichen Zhang and Junjie Zhang and Zican Dong and Yifan Du and Chen Yang and Yushuo Chen and Z. Chen and Jinhao Jiang and Ruiyang Ren and Yifan Li and Xinyu Tang and Zikang Liu and Peiyu Liu and Jianyun Nie and Ji-rong Wen},
  journal={ArXiv},
  year={2023},
  volume={abs/2303.18223},
  url={https://api.semanticscholar.org/CorpusID:257900969}
}

@article{Naveed2023ACO,
  title={A Comprehensive Overview of Large Language Models},
  author={Humza Naveed and Asad Ullah Khan and Shi Qiu and Muhammad Saqib and Saeed Anwar and Muhammad Usman and Nick Barnes and Ajmal S. Mian},
  journal={ArXiv},
  year={2023},
  volume={abs/2307.06435},
  url={https://api.semanticscholar.org/CorpusID:259847443}
}

@article{Lin2023MAtchEA,
  title={MAtch, eXpand and Improve: Unsupervised Finetuning for Zero-Shot Action Recognition with Language Knowledge},
  author={Wei Lin and Leonid Karlinsky and Nina Shvetsova and Horst Possegger and Mateusz Kozi≈Ñski and Rameswar Panda and Rog{\'e}rio Schmidt Feris and Hilde Kuehne and Horst Bischof},
  journal={ArXiv},
  year={2023},
  volume={abs/2303.08914},
  url={https://api.semanticscholar.org/CorpusID:257557275}
}

@article{Hanu2023LanguageAT,
  title={Language as the Medium: Multimodal Video Classification through text only},
  author={Laura Hanu and Anita Lilla Vero and James Thewlis},
  journal={ArXiv},
  year={2023},
  volume={abs/2309.10783},
  url={https://api.semanticscholar.org/CorpusID:262054213}
}

@article{Rasheed2022FinetunedCM,
  title={Fine-tuned CLIP Models are Efficient Video Learners},
  author={Hanoona Rasheed and Muhammad Uzair Khattak and Muhammad Maaz and Salman Khan and Fahad Shahbaz Khan},
  journal={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2022},
  pages={6545-6554},
  url={https://api.semanticscholar.org/CorpusID:254366626}
}