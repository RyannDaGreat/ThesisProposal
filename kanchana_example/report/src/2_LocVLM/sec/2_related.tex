\section{Spatial Reasoning in Visual-LLMs}
\label{locvlm:related}

\noindent
\textbf{Localization in Contrastive Vision Language Models}:
Foundation vision language models (VLMs) such as CLIP \cite{radford2021learning} resulted in extensive exploration into language-tied localization in images both under dense (pixel / bounding-box) supervision \cite{gu2021vild, kamath2021mdetr, Li2022AdaptingCF, Li2021GroundedLP, Zeng2021MultiGrainedVL,Dou2022CoarsetoFineVP,ghiasi2022open, li2022language, Zhang2022GLIPv2UL,Ding2021DecouplingZS} and weak supervision \cite{xu2022groupvit, Xu2023LearningOS, Zhou2021ExtractFD, yao2022filip,cui2022democratizing, Zhang2023AssociatingSG,Luo2022SegCLIPPA,Ranasinghe2022PerceptualGI,Mukhoti2022OpenVS}. Recovering explicit localization information within model representations has enabled more robust operation for certain tasks \cite{Ranasinghe2022PerceptualGI}. While our work differs from this contrastive setting given our use of LLM based generative predictions, we similarly explore how explicit location information within the language modality can improve V-LLMs.  

\vspace{0.5em}

\noindent
\textbf{Visual Large Language Models (V-LLMs)}:
The advent of powerful large language models (LLMs) such as GPT-3 \cite{brown2020language}, Chat-GPT \cite{gpt4}, and PaLM \cite{chowdhery2022palm}, as well as their open-source counterparts BLOOM~\citep{scao2022bloom}, Vicuna \cite{vicuna2023}, and LLaMA~\citep{touvron2023llama,touvron2023llama2}, has resulted in direct use of these LLMs for computer vision tasks \cite{Gupta2022VisualPC, Suris2023ViperGPTVI}. Alternate lines of work explore how LLMs can be connected to existing visual foundation models \cite{alayrac2022flamingo,anas_awadalla_2023_7733589,Ranasinghe2023LanguagebasedAC,li2023blip,liu2023visual,Maaz2023VideoChatGPT}, in particular to CLIP visual backbones \cite{radford2021learning}. While earlier models explored large-scale (millions to billions of samples) image-text training \cite{alayrac2022flamingo,anas_awadalla_2023_7733589}, later models \cite{li2023blip,liu2023visual,Maaz2023VideoChatGPT} scale down on data dependency. LLaVA \cite{liu2023visual} in particular scales down on pre-training data to under 1 million image-text pairs, and use instruction fine-tuning \cite{Wei2021FinetunedLM} to enable human-style conversation with visual awareness. This is extended to video domain in \cite{Maaz2023VideoChatGPT,ranasinghe2024understanding}. A shortcoming of these models is their lack of spatial awareness or location understanding in image space \cite{chen2023shikra,Gokhale2022BenchmarkingSR,Cho2022DALLEVALPT}.
% - in our work we attempt to address this limitation. 
Spatial reasoning limitations in generative VLMs are studied in \cite{Gokhale2022BenchmarkingSR,Cho2022DALLEVALPT}. Similar failures in captioning (and VQA) models are explored in \cite{Kamath2023WhatsW}. A solution in \cite{Hsu2023WhatsLC} proposes code-generation based reasoning. Our work tackles these same limitations but follows an alternate direction of spatial-aware instruction fine-tuning.
Another line of recent works \cite{zhang2023gpt4roi,zhao2023bubogpt,zang2023contextual,peng2023kosmos,You2023FerretRA,wang2023visionllm,lai2023lisa} tackle this by introducing architectural modifications to explicitly extract region level features that are injected to the LLM as special tokens. While introducing extra tokens and layers, this also separates the localization task from language. 
In contrast, we use a generic architectures with purely textual location information (i.e. image space coordinates as text).  
% We postulate that generic architectures with purely language driven location information (i.e. reasoning in image coordinate space) would inject LLMs with stronger spatial awareness more similar to human visual understanding and reasoning. 
Concurrent work in \cite{chen2023shikra} explores this same idea, but we differ in 3 ways with, a) focus on optimal coordinate representation forms, b) data-efficient pseudo-labelling strategies, and c) video domain operation (see also \cref{tbl:related}). 

\input{src/2_LocVLM/figures/related}

\vspace{0.5em}
\noindent
\textbf{Location Representations}:
Selecting regions within an image has a rich history in computer vision \cite{malik2001visual,uijlings2013selective} with greater focus on location outputs since the popularity of object detection \cite{girshick2015fastrcnn, redmon2016yolo, tian2019fcos, carion2020detr, chen2022diffusiondet, chen2021pix2seq, wang2023visionllm}. Early anchor-based methods regress locations from anchor centers \cite{girshick2015fastrcnn, redmon2016yolo}, followed by direct location regression from object-level features \cite{tian2019fcos, carion2020detr}. Recent works explore generative location predictions with diffusion processes \cite{chen2022diffusiondet} and sequence-generation \cite{chen2021pix2seq, wang2023visionllm}. Ours resembles the latter given our use of an LLM, next token prediction objective, and sequential generation of textual location representations. However, \cite{chen2021pix2seq, wang2023visionllm} utilize 1000 specialized location tokens (introduced to the LLM vocabulary) corresponding to 1000 bins uniformly spread across image space. While we explore similar binning strategies, in contrast we introduce no additional tokens, focus on purely textual representation of locations, and explore multiple textual location representation forms.  

