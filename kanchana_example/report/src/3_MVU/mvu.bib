@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})

@inproceedings{haldar2023watch,
  title        = {Watch and match: Supercharging imitation with regularized optimal transport},
  author       = {Haldar, Siddhant and Mathur, Vaibhav and Yarats, Denis and Pinto, Lerrel},
  booktitle    = {Conference on Robot Learning},
  pages        = {32--43},
  year         = {2023},
  organization = {PMLR}
}
@article{saytap2023,
  author = {Yujin Tang and Wenhao Yu and Jie Tan and Heiga Zen and Aleksandra Faust and
            Tatsuya Harada},
  title  = {SayTap: Language to Quadrupedal Locomotion},
  eprint = {arXiv:2306.07580},
  url    = {https://saytap.github.io},
  note   = {https://saytap.github.io},
  year   = {2023}
}

@misc{matsushima2023weblab,
  title  = {Weblab xArm Dataset},
  author = {Tatsuya Matsushima and Hiroki Furuta and Yusuke Iwasawa and Yutaka Matsuo},
  year   = {2023}
}
@article{salhotra2022dmfd,
  author  = {Salhotra, Gautam and Liu, I-Chun Arthur and Dominguez-Kuhne, Marcus and Sukhatme, Gaurav S.},
  journal = {IEEE Robotics and Automation Letters},
  title   = {Learning Deformable Object Manipulation From Expert Demonstrations},
  year    = {2022},
  volume  = {7},
  number  = {4},
  pages   = {8775-8782},
  doi     = {10.1109/LRA.2022.3187843}
}
@article{Osa22,
  author  = {Takayuki Osa},
  journal = {The International Journal of Robotics Research},
  title   = {Motion Planning by Learning the Solution Manifold in Trajectory Optimization},
  year    = {2022},
  number  = {3},
  pages   = {291--311},
  volume  = {41}
}
@misc{oh2023pr2utokyodatasets,
  author = {Jihoon Oh and Naoaki Kanazawa and Kento Kawaharazuka},
  title  = {X-Embodiment U-Tokyo PR2 Datasets},
  year   = {2023},
  url    = {https://github.com/ojh6404/rlds_dataset_builder}
}
@inproceedings{mendonca2023structured,
  title     = {Structured World Models from Human Videos},
  author    = {Mendonca, Russell  and Bahl, Shikhar and Pathak, Deepak},
  booktitle = {RSS},
  year      = {2023}
}
@inproceedings{bahl2023affordances,
  title     = {Affordances from Human Videos as a Versatile Representation for Robotics},
  author    = {Bahl, Shikhar and Mendonca, Russell and Chen, Lili and Jain, Unnat and Pathak, Deepak},
  booktitle = {CVPR},
  year      = {2023}
}

@inproceedings{zhou2023modularity,
  title        = {Modularity through Attention: Efficient Training and Transfer of Language-Conditioned Policies for Robot Manipulation},
  author       = {Zhou, Yifan and Sonawani, Shubham and Phielipp, Mariano and Stepputtis, Simon and Amor, Heni},
  booktitle    = {Conference on Robot Learning},
  pages        = {1684--1695},
  year         = {2023},
  organization = {PMLR}
}
@article{zhou2023learning,
  title     = {Learning modular language-conditioned robot policies through attention},
  author    = {Zhou, Yifan and Sonawani, Shubham and Phielipp, Mariano and Ben Amor, Heni and Stepputtis, Simon},
  journal   = {Autonomous Robots},
  pages     = {1--21},
  year      = {2023},
  publisher = {Springer}
}
@misc{oh2023pr2utokyodatasets,
  author = {Jihoon Oh and Naoaki Kanazawa and Kento Kawaharazuka},
  title  = {X-Embodiment U-Tokyo PR2 Datasets},
  year   = {2023},
  url    = {https://github.com/ojh6404/rlds_dataset_builder}
}
@misc{matsushima2023weblab,
  title  = {Weblab xArm Dataset},
  author = {Tatsuya Matsushima and Hiroki Furuta and Yusuke Iwasawa and Yutaka Matsuo},
  year   = {2023}
}
@misc{ucsd_kitchens,
  author = {Yan, Ge and Wu, Kris and Wang, Xiaolong},
  title  = {ucsd kitchens Dataset},
  year   = {2023},
  month  = {August}
}

@inproceedings{kahn2018self,
  title        = {Self-supervised deep reinforcement learning with generalized computation graphs for robot navigation},
  author       = {Kahn, Gregory and Villaflor, Adam and Ding, Bosen and Abbeel, Pieter and Levine, Sergey},
  booktitle    = {2018 IEEE international conference on robotics and automation (ICRA)},
  pages        = {5129--5136},
  year         = {2018},
  organization = {IEEE}
}
@article{zhu2022bottom,
  title     = {Bottom-Up Skill Discovery From Unsegmented Demonstrations for Long-Horizon Robot Manipulation},
  author    = {Zhu, Yifeng and Stone, Peter and Zhu, Yuke},
  journal   = {IEEE Robotics and Automation Letters},
  volume    = {7},
  number    = {2},
  pages     = {4126--4133},
  year      = {2022},
  publisher = {IEEE}
}
@article{padalkar2023guided,
  title   = {A guided reinforcement learning approach using shared control templates for learning manipulation skills in the real world},
  author  = {Padalkar, Abhishek and Quere, Gabriel and Raffin, Antonin and Silv{\'e}rio, Jo{\~a}o and Stulp, Freek},
  journal = {Research square preprint rs-3289569/v1},
  year    = {2023}
}
@inproceedings{chi2023diffusionpolicy,
  title     = {Diffusion Policy: Visuomotor Policy Learning via Action Diffusion},
  author    = {Chi, Cheng and Feng, Siyuan and Du, Yilun and Xu, Zhenjia and Cousineau, Eric and Burchfiel, Benjamin and Song, Shuran},
  booktitle = {Proceedings of Robotics: Science and Systems (RSS)},
  year      = {2023}
}
@inproceedings{padalkar2023guiding,
  title        = {Guiding Reinforcement Learning with Shared Control Templates},
  author       = {Padalkar, Abhishek and Quere, Gabriel and Steinmetz, Franz and Raffin, Antonin and Nieuwenhuisen, Matthias and Silv{\'e}rio, Jo{\~a}o and Stulp, Freek},
  booktitle    = {40th IEEE International Conference on Robotics and Automation, ICRA 2023},
  year         = {2023},
  organization = {IEEE}
}
@inproceedings{vogel_edan_2020,
  title     = {EDAN - an EMG-Controlled Daily Assistant to Help People with Physical Disabilities},
  language  = {en},
  booktitle = {2020 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
  author    = {Vogel, Jörn and Hagengruber, Annette and Iskandar, Maged and Quere, Gabriel and Leipscher, Ulrike and Bustamante, Samuel and Dietrich, Alexander and Hoeppner, Hannes and Leidner, Daniel and Albu-Schäffer, Alin},
  year      = {2020}
}
@inproceedings{quere_shared_2020,
  address   = {Paris, France},
  title     = {Shared {Control} {Templates} for {Assistive} {Robotics}},
  language  = {en},
  booktitle = {2020 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
  author    = {Quere, Gabriel and Hagengruber, Annette and Iskandar, Maged and Bustamante, Samuel and Leidner, Daniel and Stulp, Freek and Vogel, Joern},
  year      = {2020},
  pages     = {7}
}
@preprint{Feng2023Finetuning,
  title  = {Finetuning Offline World Models in the Real World},
  author = {Yunhai Feng, Nicklas Hansen, Ziyan Xiong, Chandramouli Rajagopalan, Xiaolong Wang},
  year   = {2023}
}
@article{luo2023multistage,
  author  = {Jianlan Luo and Charles Xu and Xinyang Geng and Gilbert Feng and Kuan Fang and Liam Tan and Stefan Schaal and Sergey Levine},
  title   = {Multi-Stage Cable Routing through Hierarchical Imitation Learning},
  journal = {arXiv pre-print},
  year    = {2023},
  url     = {https://arxiv.org/abs/2307.08927}
}
@article{cui2022play,
  title   = {From Play to Policy: Conditional Behavior Generation from Uncurated Robot Data},
  author  = {Cui, Zichen Jeff and Wang, Yibin and Shafiullah, Nur Muhammad Mahi and Pinto, Lerrel},
  journal = {arXiv preprint arXiv:2210.10047},
  year    = {2022}
}
@inproceedings{liu2022robot,
  title     = {Robot Learning on the Job: Human-in-the-Loop Autonomy and Learning During Deployment},
  author    = {Huihan Liu and Soroush Nasiriany and Lance Zhang and Zhiyao Bao and Yuke Zhu},
  booktitle = {Robotics: Science and Systems (RSS)},
  year      = {2023}
}
@inproceedings{chen2023playfusion,
  title     = {PlayFusion: Skill Acquisition via Diffusion from Language-Annotated Play},
  author    = {Chen, Lili and Bahl, Shikhar and Pathak, Deepak},
  booktitle = {CoRL},
  year      = {2023}
}
@article{hirose2023sacson,
  title   = {SACSoN: Scalable Autonomous Data Collection for Social Navigation},
  author  = {Hirose, Noriaki and Shah, Dhruv and Sridhar, Ajay and Levine, Sergey},
  journal = {arXiv preprint arXiv:2306.01874},
  year    = {2023}
}
@misc{pari2021surprising,
  title         = {The Surprising Effectiveness of Representation Learning for Visual Imitation},
  author        = {Jyothish Pari and Nur Muhammad Shafiullah and Sridhar Pandian Arunachalam and Lerrel Pinto},
  year          = {2021},
  eprint        = {2112.01511},
  archiveprefix = {arXiv},
  primaryclass  = {cs.RO}
}
@article{fanuc_manipulation2023,
  title  = {Fanuc Manipulation: A Dataset for Learning-based Manipulation with FANUC Mate 200iD Robot},
  author = {Zhu, Xinghao and Tian, Ran and Xu, Chenfeng and Ding, Mingyu and Zhan, Wei and Tomizuka, Masayoshi},
  year   = {2023}
}

@misc{dass2023jacoplay,
  author  = {Dass, Shivin and Yapeter, Jullian and Zhang, Jesse and Zhang, Jiahui
             and Pertsch, Karl and Nikolaidis, Stefanos and Lim, Joseph J.},
  title   = {CLVR Jaco Play Dataset},
  url     = {https://github.com/clvrai/clvr_jaco_play_dataset},
  version = {1.0.0},
  year    = {2023}
}

@article{zhu2022viola,
  title   = {VIOLA: Imitation Learning for Vision-Based Manipulation with Object Proposal Priors},
  author  = {Zhu, Yifeng and Joshi, Abhishek and Stone, Peter and Zhu, Yuke},
  journal = {6th Annual Conference on Robot Learning (CoRL)},
  year    = {2022}
}
@article{kimpre,
  title        = {Pre-and post-contact policy decomposition for non-prehensile manipulation with zero-shot sim-to-real transfer},
  author       = {Kim, Minchan and Han, Junhyek and Kim, Jaehyung and Kim, Beomjoon},
  booktitle    = {2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  year         = {2023},
  organization = {IEEE}
}
@inproceedings{Radosavovic2022,
  title     = {Real-World Robot Learning with Masked Visual Pre-training},
  author    = {Ilija Radosavovic and Tete Xiao and Stephen James and Pieter Abbeel and Jitendra Malik and Trevor Darrell},
  booktitle = {CoRL},
  year      = {2022}
}
@article{wang2023d3field,
  title   = {D^3Field: Dynamic 3D Descriptor Fields for Generalizable Robotic Manipulation},
  author  = {Wang, Yixuan and Li, Zhuoran and Zhang, Mingtong and Driggs-Campbell, Katherine and Wu, Jiajun and Fei-Fei, Li and Li, Yunzhu},
  journal = {arXiv preprint arXiv:},
  year    = {2023}
}
@inproceedings{schiavi2023learning,
  title        = {Learning agent-aware affordances for closed-loop interaction with articulated objects},
  author       = {Schiavi, Giulio and Wulkop, Paula and Rizzi, Giuseppe and Ott, Lionel and Siegwart, Roland and Chung, Jen Jen},
  booktitle    = {2023 IEEE International Conference on Robotics and Automation (ICRA)},
  pages        = {5916--5922},
  year         = {2023},
  organization = {IEEE}
}
@inproceedings{shah2021rapid,
  title     = {{Rapid Exploration for Open-World Navigation with Latent Goal Models}},
  author    = {Dhruv Shah and Benjamin Eysenbach and Nicholas Rhinehart and Sergey Levine},
  booktitle = {5th Annual Conference on Robot Learning },
  year      = {2021},
  url       = {https://openreview.net/forum?id=d_SWJhyKfVw}
}
@inproceedings{nasiriany2022sailor,
  title     = {Learning and Retrieval from Prior Data for Skill-based Imitation Learning},
  author    = {Soroush Nasiriany and Tian Gao and Ajay Mandlekar and Yuke Zhu},
  booktitle = {Conference on Robot Learning (CoRL)},
  year      = {2022}
}
@inproceedings{shah2023mutex,
  title     = {{MUTEX}: Learning Unified Policies from Multimodal Task Specifications},
  author    = {Rutav Shah and Roberto Mart{\'\i}n-Mart{\'\i}n and Yuke Zhu},
  booktitle = {7th Annual Conference on Robot Learning},
  year      = {2023},
  url       = {https://openreview.net/forum?id=PwqiqaaEzJ}
}
@inproceedings{lee2019icra,
  title     = {Making sense of vision and touch: Self-supervised learning of multimodal representations for contact-rich tasks},
  author    = {Lee, Michelle A and Zhu, Yuke and Srinivasan, Krishnan and Shah, Parth and Savarese, Silvio and Fei-Fei, Li and  Garg, Animesh and Bohg, Jeannette},
  booktitle = {2019 IEEE International Conference on Robotics and Automation (ICRA)},
  year      = {2019},
  url       = {https://arxiv.org/abs/1810.10191}
}
@article{Radosavovic2023,
  title   = {Robot Learning with Sensorimotor Pre-training},
  author  = {Ilija Radosavovic and Baifeng Shi and Letian Fu and Ken Goldberg and Trevor Darrell and Jitendra Malik},
  year    = {2023},
  journal = {arXiv:2306.10007}
}

@inproceedings{mandlekar2019scaling,
  title        = {Scaling robot supervision to hundreds of hours with roboturk: Robotic manipulation dataset through human reasoning and dexterity},
  author       = {Mandlekar, Ajay and Booher, Jonathan and Spero, Max and Tung, Albert and Gupta, Anchit and Zhu, Yuke and Garg, Animesh and Savarese, Silvio and Fei-Fei, Li},
  booktitle    = {2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages        = {1048--1055},
  year         = {2019},
  organization = {IEEE}
}
@inproceedings{rosete2022tacorl,
  author    = {Erick Rosete-Beas and Oier Mees and Gabriel Kalweit and Joschka Boedecker and Wolfram Burgard},
  title     = {Latent Plans for Task Agnostic Offline Reinforcement Learning},
  booktitle = {Proceedings of the 6th Conference on Robot Learning (CoRL)},
  year      = {2022}
}
@inproceedings{mees23hulc2,
  title     = {Grounding  Language  with  Visual  Affordances  over  Unstructured  Data},
  author    = {Oier Mees and Jessica Borja-Diaz and Wolfram Burgard},
  booktitle = {Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)},
  year      = {2023},
  address   = {London, UK}
}
@inproceedings{saxena2023multiresolution,
  title     = {Multi-Resolution Sensing for Real-Time Control with Vision-Language Models},
  author    = {Saumya Saxena and Mohit Sharma and Oliver Kroemer},
  booktitle = {7th Annual Conference on Robot Learning},
  year      = {2023},
  url       = {https://openreview.net/forum?id=WuBv9-IGDUA}
}
@article{belkhale2023hydra,
  title   = {HYDRA: Hybrid Robot Actions for Imitation Learning},
  author  = {Belkhale, Suneel and Cui, Yuchen and Sadigh, Dorsa},
  journal = {arxiv},
  year    = {2023}
}
@inproceedings{gupta2022maskvit,
  title     = {MaskViT: Masked Visual Pre-Training for Video Prediction},
  author    = {Agrim Gupta and Stephen Tian and Yunzhi Zhang and Jiajun Wu and Roberto Martín-Martín and Li Fei-Fei},
  booktitle = {International Conference on Learning Representations},
  year      = {2022}
}
@misc{BerkeleyUR5Website,
  title        = {Berkeley {UR5} Demonstration Dataset},
  author       = {Lawrence Yunliang Chen and Simeon Adebola and Ken Goldberg},
  howpublished = {https://sites.google.com/view/berkeley-ur5/home}
}
@inproceedings{jang2021bc,
  title     = {{BC}-Z: Zero-Shot Task Generalization with Robotic Imitation Learning},
  author    = {Eric Jang and Alex Irpan and Mohi Khansari and Daniel Kappler and Frederik Ebert and Corey Lynch and Sergey Levine and Chelsea Finn},
  booktitle = {5th Annual Conference on Robot Learning},
  year      = {2021},
  url       = {https://openreview.net/forum?id=8kbp23tSGYv}
}
@inproceedings{heo2023furniturebench,
  title     = {FurnitureBench: Reproducible Real-World Benchmark for Long-Horizon Complex Manipulation},
  author    = {Minho Heo and Youngwoon Lee and Doohyun Lee and Joseph J. Lim},
  booktitle = {Robotics: Science and Systems},
  year      = {2023}
}
@article{brohan2022rt,
  title   = {Rt-1: Robotics transformer for real-world control at scale},
  author  = {Brohan, Anthony and Brown, Noah and Carbajal, Justice and Chebotar, Yevgen and Dabis, Joseph and Finn, Chelsea and Gopalakrishnan, Keerthana and Hausman, Karol and Herzog, Alex and Hsu, Jasmine and others},
  journal = {arXiv preprint arXiv:2212.06817},
  year    = {2022}
}
@article{shi2023robocook,
  title   = {RoboCook: Long-Horizon Elasto-Plastic Object Manipulation with Diverse Tools},
  author  = {Shi, Haochen and Xu, Huazhe and Clarke, Samuel and Li, Yunzhu and Wu, Jiajun},
  journal = {arXiv preprint arXiv:2306.14447},
  year    = {2023}
}
@inproceedings{zhou2023train,
  author    = {Zhou, Gaoyue and Dean, Victoria and Srirama, Mohan Kumar and Rajeswaran, Aravind and Pari, Jyothish and Hatch, Kyle and Jain, Aryan and Yu, Tianhe and Abbeel, Pieter and Pinto, Lerrel and Finn, Chelsea and Gupta, Abhinav},
  booktitle = {2023 IEEE International Conference on Robotics and Automation (ICRA)},
  title     = {Train Offline, Test Online: A Real Robot Learning Benchmark},
  year      = {2023}
}
@inproceedings{gu2023maniskill2,
  title     = {ManiSkill2: A Unified Benchmark for Generalizable Manipulation Skills},
  author    = {Gu, Jiayuan and Xiang, Fanbo and Li, Xuanlin and Ling, Zhan and Liu, Xiqiang and Mu, Tongzhou and Tang, Yihe and Tao, Stone and Wei, Xinyue and Yao, Yunchao and Yuan, Xiaodi and Xie, Pengwei and Huang, Zhiao and Chen, Rui and Su, Hao},
  booktitle = {International Conference on Learning Representations},
  year      = {2023}
}
@inproceedings{walke2023bridgedata,
  title     = {BridgeData V2: A Dataset for Robot Learning at Scale},
  author    = {Walke, Homer and Black, Kevin and Lee, Abraham and Kim, Moo Jin and Du, Max and Zheng, Chongyi and Zhao, Tony and Hansen-Estruch, Philippe and Vuong, Quan and He, Andre and Myers, Vivek and Fang, Kuan and Finn, Chelsea and Levine, Sergey},
  booktitle = {Conference on Robot Learning (CoRL)},
  year      = {2023}
}
@article{lynch2023interactive,
  title     = {Interactive language: Talking to robots in real time},
  author    = {Lynch, Corey and Wahid, Ayzaan and Tompson, Jonathan and Ding, Tianli and Betker, James and Baruch, Robert and Armstrong, Travis and Florence, Pete},
  journal   = {IEEE Robotics and Automation Letters},
  year      = {2023},
  publisher = {IEEE}
}
@inproceedings{sawhney2021playing,
  title        = {Playing with food: Learning food item representations through interactive exploration},
  author       = {Sawhney, Amrita and Lee, Steven and Zhang, Kevin and Veloso, Manuela and Kroemer, Oliver},
  booktitle    = {Experimental Robotics: The 17th International Symposium},
  pages        = {309--322},
  year         = {2021},
  organization = {Springer}
}
@article{kalashnikov2018qt,
  title   = {Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation},
  author  = {Kalashnikov, Dmitry and Irpan, Alex and Pastor, Peter and Ibarz, Julian and Herzog, Alexander and Jang, Eric and Quillen, Deirdre and Holly, Ethan and Kalakrishnan, Mrinal and Vanhoucke, Vincent and others},
  journal = {arXiv preprint arXiv:1806.10293},
  year    = {2018}
}
@inproceedings{dasari2019robonet,
  title         = {RoboNet: Large-Scale Multi-Robot Learning},
  author        = {Sudeep Dasari and Frederik Ebert and Stephen Tian and Suraj Nair and Bernadette Bucher and Karl Schmeckpeper and Siddharth Singh and Sergey Levine and Chelsea Finn},
  year          = {2019},
  eprint        = {1910.11215},
  archiveprefix = {arXiv},
  primaryclass  = {cs.RO},
  booktitle     = {CoRL 2019: Volume 100 Proceedings of Machine Learning Research}
}
@misc{rana2023dgrasp,
  author = {Krishan Rana, Ben Burgess-Limerick, Jad Abou-Chakra, Niko S{\u}nderhauf},
  title  = {DGrasp: A Large Scale Dataset for Dynamic Grasping of Moving Objects.},
  year   = {2023}
} 

@misc{ceola2023lhmanip,
  author = {Federico Ceola, Krishan Rana, Niko S{\u}nderhauf},
  title  = {LHManip: A Dataset for Long Horizon Manipulation Tasks.},
  year   = {2023}
} 
@article{guist2023robust,
  title   = {A Robust Open-source Tendon-driven Robot Arm for Learning Control of Dynamic Motions},
  author  = {Guist, Simon and Schneider, Jan and Ma, Hao and Berenz, Vincent and Martus, Julian and Gr{\u}ninger, Felix and M{\u}hlebach, Michael and Fiene, Jonathan and Sch{\o}lkopf, Bernhard and B{\u}chler, Dieter},
  journal = {arXiv preprint arXiv:2307.02654},
  year    = {2023}
}

@article{Zhao2023LearningFB,
  title   = {Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware},
  author  = {Tony Zhao and Vikash Kumar and Sergey Levine and Chelsea Finn},
  journal = {RSS},
  year    = {2023},
  volume  = {abs/2304.13705},
  url     = {https://arxiv.org/abs/2304.13705}
}


@article{Mangalam2023EgoSchemaAD,
  title   = {EgoSchema: A Diagnostic Benchmark for Very Long-form Video Language Understanding},
  author  = {Karttikeya Mangalam and Raiymbek Akshulakov and Jitendra Malik},
  journal = {ArXiv},
  year    = {2023},
  volume  = {abs/2308.09126},
  url     = {https://api.semanticscholar.org/CorpusID:261031047}
}

@inproceedings{dataset_xiao2021nextqa,
  author    = {Xiao, Junbin and Shang, Xindi and Yao, Angela and Chua, Tat-Seng},
  title     = {{NExT-QA}: Next Phase of Question-Answering to Explaining Temporal Actions},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2021}
}


@article{Minderer2022SimpleOO,
  title   = {Simple Open-Vocabulary Object Detection with Vision Transformers},
  author  = {Matthias Minderer and Alexey A. Gritsenko and Austin Stone and Maxim Neumann and Dirk Weissenborn and Alexey Dosovitskiy and Aravindh Mahendran and Anurag Arnab and Mostafa Dehghani and Zhuoran Shen and Xiao Wang and Xiaohua Zhai and Thomas Kipf and Neil Houlsby},
  journal = {ArXiv},
  year    = {2022},
  volume  = {abs/2205.06230},
  url     = {https://api.semanticscholar.org/CorpusID:248721818}
}

@article{Wang2018FastOO,
  title   = {Fast Online Object Tracking and Segmentation: A Unifying Approach},
  author  = {Qiang Wang and Li Zhang and Luca Bertinetto and Weiming Hu and Philip H. S. Torr},
  journal = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year    = {2018},
  pages   = {1328-1338},
  url     = {https://api.semanticscholar.org/CorpusID:54475412}
}


@article{Goyal2017TheS,
  title   = {The “Something Something” Video Database for Learning and Evaluating Visual Common Sense},
  author  = {Raghav Goyal and Samira Ebrahimi Kahou and Vincent Michalski and Joanna Materzynska and Susanne Westphal and Heuna Kim and Valentin Haenel and Ingo Fr{\"u}nd and Peter N. Yianilos and Moritz Mueller-Freitag and Florian Hoppe and Christian Thurau and Ingo Bax and Roland Memisevic},
  journal = {2017 IEEE International Conference on Computer Vision (ICCV)},
  year    = {2017},
  pages   = {5843-5851},
  url     = {https://api.semanticscholar.org/CorpusID:834612}
}


@article{zhang2023llovi,
  title   = {A simple llm framework for long-range video question-answering},
  author  = {Zhang, Ce and Lu, Taixi and Islam, Md Mohaiminul and Wang, Ziyang and Yu, Shoubin and Bansal, Mohit and Bertasius, Gedas},
  journal = {arXiv preprint arXiv:2312.17235},
  year    = {2023}
}

@article{Buch2022RevisitingT,
  title   = {Revisiting the “Video” in Video-Language Understanding},
  author  = {S. Buch and Cristobal Eyzaguirre and Adrien Gaidon and Jiajun Wu and Li Fei-Fei and Juan Carlos Niebles},
  journal = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year    = {2022},
  pages   = {2907-2917},
  url     = {https://api.semanticscholar.org/CorpusID:249375461}
}

@article{Yu2023SelfChainedIM,
  title   = {Self-Chained Image-Language Model for Video Localization and Question Answering},
  author  = {Shoubin Yu and Jaemin Cho and Prateek Yadav and Mohit Bansal},
  journal = {ArXiv},
  year    = {2023},
  volume  = {abs/2305.06988},
  url     = {https://api.semanticscholar.org/CorpusID:258615748}
}

@article{Balavzevic2024MemoryCE,
  title   = {Memory Consolidation Enables Long-Context Video Understanding},
  author  = {Ivana Balavzevi'c and Yuge Shi and Pinelopi Papalampidi and Rahma Chaabouni and Skanda Koppula and Olivier J. H'enaff},
  journal = {ArXiv},
  year    = {2024},
  volume  = {abs/2402.05861},
  url     = {https://api.semanticscholar.org/CorpusID:267547785}
}

@inproceedings{pmlr-v162-yun22a,
  title     = {Time Is {M}att{E}r: Temporal Self-supervision for Video Transformers},
  author    = {Yun, Sukmin and Kim, Jaehyung and Han, Dongyoon and Song, Hwanjun and Ha, Jung-Woo and Shin, Jinwoo},
  booktitle = {Proceedings of the 39th International Conference on Machine Learning},
  pages     = {25804--25816},
  year      = {2022}
}

@inproceedings{fan-iclr2022,
  title     = {Can an Image Classifier Suffice for Action Recognition?},
  author    = {Quanfu Fan, Richard Chen, Rameswar Panda},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2022}
}

@inproceedings{davis1997mei,
  title     = {The Representation and Recognition of Action Using Temporal Templates},
  author    = {Davis, James and Bobick, Aaron},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
  pages     = {2736--2744},
  year      = {1997}
}

@inproceedings{bilen2016dynamic,
  title     = {Dynamic image networks for action recognition},
  author    = {Bilen, Hakan and Fernando, Basura and Gavves, Efstratios and Vedaldi, Andrea and Gould, Stephen},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages     = {3034--3042},
  year      = {2016}
}

@inproceedings{8658386,
  author    = {Safaei, Marjaneh and Foroosh, Hassan},
  booktitle = {2019 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  title     = {Still Image Action Recognition by Predicting Spatial-Temporal Pixel Evolution},
  year      = {2019}
}

@inproceedings{8451193,
  author    = {Safaei, Marjaneh and Balouchian, Pooyan and Foroosh, Hassan},
  booktitle = {2018 25th IEEE International Conference on Image Processing (ICIP)},
  title     = {TICNN: A Hierarchical Deep Learning Framework for Still Image Action Recognition Using Temporal Image Prediction},
  year      = {2018}
}

@inproceedings{Zhao_2017_ICCV,
  author    = {Zhao, Zhichen and Ma, Huimin and You, Shaodi},
  title     = {Single Image Action Recognition Using Semantic Body Part Actions},
  booktitle = {The IEEE International Conference on Computer Vision (ICCV)},
  month     = {Oct},
  year      = {2017}
}


@article{Agrawal2015VQAVQ,
  title   = {VQA: Visual Question Answering},
  author  = {Aishwarya Agrawal and Jiasen Lu and Stanislaw Antol and Margaret Mitchell and C. Lawrence Zitnick and Devi Parikh and Dhruv Batra},
  journal = {International Journal of Computer Vision},
  year    = {2015},
  volume  = {123},
  pages   = {4 - 31},
  url     = {https://api.semanticscholar.org/CorpusID:3180429}
}

@article{Yu2019ActivityNetQAAD,
  title   = {ActivityNet-QA: A Dataset for Understanding Complex Web Videos via Question Answering},
  author  = {Zhou Yu and D. Xu and Jun Yu and Ting Yu and Zhou Zhao and Yueting Zhuang and Dacheng Tao},
  journal = {ArXiv},
  year    = {2019},
  volume  = {abs/1906.02467},
  url     = {https://api.semanticscholar.org/CorpusID:69645185}
}

@article{videoqaaaai2017,
  title   = {Leveraging Video Descriptions to Learn Video Question Answering},
  volume  = {31},
  url     = {https://ojs.aaai.org/index.php/AAAI/article/view/11238},
  doi     = {10.1609/aaai.v31i1.11238},
  number  = {1},
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  author  = {Zeng, Kuo-Hao and Chen, Tseng-Hung and Chuang, Ching-Yao and Liao, Yuan-Hong and Niebles, Juan Carlos and Sun, Min},
  year    = {2017},
  month   = {Feb.}
}

@inproceedings{xu2017msvdqavideo,
  title     = {Video Question Answering via Gradually Refined Attention over Appearance and Motion},
  author    = {Xu, Dejing and Zhao, Zhou and Xiao, Jun and Wu, Fei and Zhang, Hanwang and He, Xiangnan and Zhuang, Yueting},
  booktitle = {ACM Multimedia},
  year      = {2017}
}

@inproceedings{dataset_lei2018tvqa,
  title     = {{TVQA}: Localized, Compositional Video Question Answering},
  author    = {Lei, Jie  and
               Yu, Licheng  and
               Bansal, Mohit  and
               Berg, Tamara},
  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year      = {2018}
}

@inproceedings{dataset_lei-etal-2020-vlep,
  title     = {What is More Likely to Happen Next? Video-and-Language Future Event Prediction},
  author    = {Lei, Jie  and
               Yu, Licheng  and
               Berg, Tamara  and
               Bansal, Mohit},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year      = {2020}
}

@article{Ranasinghe2021SelfsupervisedVT,
  title   = {Self-supervised Video Transformer},
  author  = {Kanchana Ranasinghe and Muzammal Naseer and Salman Hameed Khan and Fahad Shahbaz Khan and Michael S. Ryoo},
  journal = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year    = {2021},
  pages   = {2864-2874},
  url     = {https://api.semanticscholar.org/CorpusID:244800737}
}

@article{Ramasinghe2018CombinedSA,
  title   = {Combined Static and Motion Features for Deep-Networks-Based Activity Recognition in Videos},
  author  = {Sameera Ramasinghe and Jathushan Rajasegaran and Vinoj Jayasundara and Kanchana Ranasinghe and Ranga Rodrigo and Ajith A. Pasqual},
  journal = {IEEE Transactions on Circuits and Systems for Video Technology},
  year    = {2018},
  volume  = {29},
  pages   = {2693-2707},
  url     = {https://api.semanticscholar.org/CorpusID:53116615}
}

@inproceedings{lei-etal-2020-tvqaplus,
  title     = {{TVQA}+: Spatio-Temporal Grounding for Video Question Answering},
  author    = {Lei, Jie  and
               Yu, Licheng  and
               Berg, Tamara  and
               Bansal, Mohit},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  month     = jul,
  year      = {2020},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2020.acl-main.730},
  doi       = {10.18653/v1/2020.acl-main.730},
  pages     = {8211--8225}
}


@inproceedings{lmappce1_hosseini-etal-2022-knowledge,
  title     = {Knowledge-Augmented Language Models for Cause-Effect Relation Classification},
  author    = {Hosseini, Pedram  and
               Broniatowski, David A.  and
               Diab, Mona},
  booktitle = {Proceedings of the First Workshop on Commonsense Representation and Reasoning (CSRR 2022)},
  month     = may,
  year      = {2022},
  address   = {Dublin, Ireland},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2022.csrr-1.6},
  doi       = {10.18653/v1/2022.csrr-1.6},
  pages     = {43--48}
}

@inproceedings{model_xiao2021hgqa,
  title     = {Video as Conditional Graph Hierarchy for Multi-Granular Question Answering},
  author    = {Junbin Xiao and Angela Yao and Zhiyuan Liu and Yicong Li and Wei Ji and Tat-Seng Chua},
  booktitle = {Proceedings of the 36th AAAI Conference on Artificial Intelligence (AAAI)},
  year      = {2022},
  pages     = {2804-2812}
}

@inproceedings{model_xiao2022vgt,
  title        = {Video Graph Transformer for Video Question Answering},
  author       = {Xiao, Junbin and Zhou, Pan and Chua, Tat-Seng and Yan, Shuicheng},
  booktitle    = {European Conference on Computer Vision},
  pages        = {39--58},
  year         = {2022},
  organization = {Springer}
}

@article{wang2022internvideo,
  title   = {Internvideo: General video foundation models via generative and discriminative learning},
  author  = {Wang, Yi and Li, Kunchang and Li, Yizhuo and He, Yinan and Huang, Bingkun and Zhao, Zhiyu and Zhang, Hongjie and Xu, Jilan and Liu, Yi and Wang, Zun and others},
  journal = {arXiv preprint arXiv:2212.03191},
  year    = {2022}
}

@article{papalampidi2023simple,
  title   = {A Simple Recipe for Contrastively Pre-training Video-First Encoders Beyond 16 Frames},
  author  = {Papalampidi, Pinelopi and Koppula, Skanda and Pathak, Shreya and Chiu, Justin and Heyward, Joe and Patraucean, Viorica and Shen, Jiajun and Miech, Antoine and Zisserman, Andrew and Nematzdeh, Aida},
  journal = {arXiv preprint arXiv:2312.07395},
  year    = {2023}
}

@article{yu2024sevila,
  title   = {Self-chained image-language model for video localization and question answering},
  author  = {Yu, Shoubin and Cho, Jaemin and Yadav, Prateek and Bansal, Mohit},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {36},
  year    = {2024}
}

@article{yang2022frozenblim,
  title   = {Zero-shot video question answering via frozen bidirectional language models},
  author  = {Yang, Antoine and Miech, Antoine and Sivic, Josef and Laptev, Ivan and Schmid, Cordelia},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {35},
  pages   = {124--141},
  year    = {2022}
}


@article{wang2023vamos,
  title   = {Vamos: Versatile Action Models for Video Understanding},
  author  = {Wang, Shijie and Zhao, Qi and Do, Minh Quan and Agarwal, Nakul and Lee, Kwonjoon and Sun, Chen},
  journal = {arXiv preprint arXiv:2311.13627},
  year    = {2023}
}

@article{Weston2023System2A,
  title   = {System 2 Attention (is something you might need too)},
  author  = {Jason Weston and Sainbayar Sukhbaatar},
  journal = {ArXiv},
  year    = {2023},
  volume  = {abs/2311.11829},
  url     = {https://api.semanticscholar.org/CorpusID:265295357}
}

@article{Jiang2023Mistral7,
  title   = {Mistral 7B},
  author  = {Albert Qiaochu Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de Las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and L'elio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timoth{\'e}e Lacroix and William El Sayed},
  journal = {ArXiv},
  year    = {2023},
  volume  = {abs/2310.06825},
  url     = {https://api.semanticscholar.org/CorpusID:263830494}
}


@misc{open_x_embodiment_rt_x_2023,
  title        = {Open {X-E}mbodiment: Robotic Learning Datasets and {RT-X} Models},
  author       = {Open-X-Embodiment-Collaboration and Abhishek Padalkar and Acorn Pooley and Ajinkya Jain and Alex Bewley and Alex Herzog and Alex Irpan and Alexander Khazatsky and Anant Rai and Anikait Singh and Anthony Brohan and Antonin Raffin and Ayzaan Wahid and Ben Burgess-Limerick and others},
  howpublished = {\url{https://arxiv.org/abs/2310.08864}},
  year         = {2023}
}

@article{Anil2023GeminiAF,
  title   = {Gemini: a family of highly capable multimodal models},
  author  = {Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal = {arXiv preprint arXiv:2312.11805},
  year    = {2023}
}

@article{Wang2023GeminiIR,
  title   = {Gemini in Reasoning: Unveiling Commonsense in Multimodal Large Language Models},
  author  = {Yuqing Wang and Yun Zhao},
  journal = {ArXiv},
  year    = {2023},
  volume  = {abs/2312.17661},
  url     = {https://api.semanticscholar.org/CorpusID:266690844}
}

@misc{zhao2023large,
  title  = {Large Language Models as Commonsense Knowledge for Large-Scale Task Planning},
  author = {Zirui Zhao and Wee Sun Lee and David Hsu},
  year   = {2023}
}

@misc{alkhamissi2024investigating,
  title  = {Investigating Cultural Alignment of Large Language Models},
  author = {Badr AlKhamissi and Muhammad ElNokrashy and Mai AlKhamissi and Mona Diab},
  year   = {2024}
}

@misc{yu2023kola,
  title  = {KoLA: Carefully Benchmarking World Knowledge of Large Language Models},
  author = {Jifan Yu and Xiaozhi Wang and Shangqing Tu and Shulin Cao and Daniel Zhang-Li and Xin Lv and Hao Peng and Zijun Yao and Xiaohan Zhang and Hanming Li and Chunyang Li and Zheyuan Zhang and Yushi Bai and Yantao Liu and Amy Xin and Nianyi Lin and Kaifeng Yun and Linlu Gong and Jianhui Chen and Zhili Wu and Yunjia Qi and Weikai Li and Yong Guan and Kaisheng Zeng and Ji Qi and Hailong Jin and Jinxin Liu and Yu Gu and Yuan Yao and Ning Ding and Lei Hou and Zhiyuan Liu and Bin Xu and Jie Tang and Juanzi Li},
  year   = {2023}
}

@misc{li2023language,
  title  = {Can Language Models Understand Physical Concepts?},
  author = {Lei Li and Jingjing Xu and Qingxiu Dong and Ce Zheng and Qi Liu and Lingpeng Kong and Xu Sun},
  year   = {2023}
}

@misc{xu2024penetrative,
  title  = {Penetrative AI: Making LLMs Comprehend the Physical World},
  author = {Huatao Xu and Liying Han and Qirui Yang and Mo Li and Mani Srivastava},
  year   = {2024}
}

@article{Kcman2023CausalRA,
  title   = {Causal Reasoning and Large Language Models: Opening a New Frontier for Causality},
  author  = {Emre Kıcıman and Robert Osazuwa Ness and Amit Sharma and Chenhao Tan},
  journal = {ArXiv},
  year    = {2023},
  volume  = {abs/2305.00050},
  url     = {https://api.semanticscholar.org/CorpusID:258426662}
}

@article{Creswell2022FaithfulRU,
  title   = {Faithful Reasoning Using Large Language Models},
  author  = {Antonia Creswell and Murray Shanahan},
  journal = {ArXiv},
  year    = {2022},
  volume  = {abs/2208.14271},
  url     = {https://api.semanticscholar.org/CorpusID:251929296}
}

@inproceedings{Liu2023TheMO,
  title     = {The Magic of IF: Investigating Causal Reasoning Abilities in Large Language Models of Code},
  author    = {Xiao Liu and Da Yin and Chen Zhang and Yansong Feng and Dongyan Zhao},
  booktitle = {Annual Meeting of the Association for Computational Linguistics},
  year      = {2023},
  url       = {https://api.semanticscholar.org/CorpusID:258968140}
}



@article{Hanu2022VTCIV,
  title   = {VTC: Improving Video-Text Retrieval with User Comments},
  author  = {Laura Hanu and James Thewlis and Yuki M. Asano and C. Rupprecht},
  journal = {ArXiv},
  year    = {2022}
}

@misc{lin2023match,
  title  = {MAtch, eXpand and Improve: Unsupervised Finetuning for Zero-Shot Action Recognition with Language Knowledge},
  author = {Wei Lin and Leonid Karlinsky and Nina Shvetsova and Horst Possegger and Mateusz Kozinski and Rameswar Panda and Rogerio Feris and Hilde Kuehne and Horst Bischof},
  year   = {2023}
}

@inproceedings{Wang2017AlternativeSR,
  title     = {Alternative Semantic Representations for Zero-Shot Human Action Recognition},
  author    = {Qian Wang and Ke Chen},
  booktitle = {ECML/PKDD},
  year      = {2017}
}

@article{violet,
  title   = {An empirical study of end-to-end video-language transformers with masked visual modeling},
  author  = {Fu, Tsu-Jui and Li, Linjie and Gan, Zhe and Lin, Kevin and Wang, William Yang and Wang, Lijuan and Liu, Zicheng},
  journal = {arXiv preprint arXiv:2209.01540},
  year    = {2022}
}


@article{Su2023LanguageMA,
  title   = {Language Models are Causal Knowledge Extractors for Zero-shot Video Question Answering},
  author  = {Hung-Ting Su and Yulei Niu and Xudong Lin and Winston H. Hsu and Shih-Fu Chang},
  journal = {2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
  year    = {2023},
  pages   = {4951-4960},
  url     = {https://api.semanticscholar.org/CorpusID:258041332}
}

@inproceedings{momeni2023vfc,
  title     = {Verbs in action: Improving verb understanding in video-language models},
  author    = {Momeni, Liliane and Caron, Mathilde and Nagrani, Arsha and Zisserman, Andrew and Schmid, Cordelia},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages     = {15579--15591},
  year      = {2023}
}

@article{suris2023vipergpt,
  title   = {Vipergpt: Visual inference via python execution for reasoning},
  author  = {Sur{\'\i}s, D{\'\i}dac and Menon, Sachit and Vondrick, Carl},
  journal = {arXiv preprint arXiv:2303.08128},
  year    = {2023}
}

@article{ko2023llama-vqa,
  title   = {Large language models are temporal and causal reasoners for video question answering},
  author  = {Ko, Dohwan and Lee, Ji Soo and Kang, Wooyoung and Roh, Byungseok and Kim, Hyunwoo J},
  journal = {arXiv preprint arXiv:2310.15747},
  year    = {2023}
}

@article{li2023blip2,
  title   = {Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
  author  = {Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  journal = {arXiv preprint arXiv:2301.12597},
  year    = {2023}
}

@article{xiao2023covgt,
  title   = {Contrastive Video Question Answering via Video Graph Transformer},
  author  = {Xiao, Junbin and Zhou, Pan and Yao, Angela and Li, Yicong and Hong, Richang and Yan, Shuicheng and Chua, Tat-Seng},
  journal = {arXiv preprint arXiv:2302.13668},
  year    = {2023}
}

@article{kim2023sevit,
  title   = {Semi-parametric video-grounded text generation},
  author  = {Kim, Sungdong and Kim, Jin-Hwa and Lee, Jiyoung and Seo, Minjoon},
  journal = {arXiv preprint arXiv:2301.11507},
  year    = {2023}
}

@inproceedings{ye2023hitea,
  title     = {Hitea: Hierarchical temporal-aware video-language pre-training},
  author    = {Ye, Qinghao and Xu, Guohai and Yan, Ming and Xu, Haiyang and Qian, Qi and Zhang, Ji and Huang, Fei},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages     = {15405--15416},
  year      = {2023}
}

@misc{kowal2024understanding,
  title  = {Understanding Video Transformers via Universal Concept Discovery},
  author = {Matthew Kowal and Achal Dave and Rares Ambrus and Adrien Gaidon and Konstantinos G. Derpanis and Pavel Tokmakov},
  year   = {2024}
}

@misc{Radford2018ImprovingLU,
  title  = {Improving Language Understanding by Generative Pre-Training},
  author = {Alec Radford and Karthik Narasimhan},
  year   = {2018},
  url    = {https://api.semanticscholar.org/CorpusID:49313245}
}

@misc{Bishop2006PatternRA,
  title  = {Pattern Recognition and Machine Learning (Information Science and Statistics)},
  author = {Christopher M. Bishop},
  year   = {2006},
  url    = {https://api.semanticscholar.org/CorpusID:268095720}
}

@inproceedings{RanLearningtoLoc23,
  title     = {Learning to Localize Objects Improves Spatial Reasoning in Visual-LLMs},
  booktitle = {CVPR},
  author    = {Kanchana Ranasinghe and Satya Narayan Shukla and Omid Poursaeed and Michael S Ryoo and Tsung-Yu Lin},
  year      = {2024}
}

@article{Sennrich2015NeuralMT,
  title   = {Neural Machine Translation of Rare Words with Subword Units},
  author  = {Rico Sennrich and Barry Haddow and Alexandra Birch},
  journal = {ArXiv},
  year    = {2015},
  volume  = {abs/1508.07909},
  url     = {https://api.semanticscholar.org/CorpusID:1114678}
}

@article{Kahatapitiya2024,
  title   = {Language Repository for Long Video Understanding},
  author  = {Kumara Kahatapitiya and Kanchana Ranasinghe and Jongwoo Park and Michael S Ryoo},
  journal = {ArXiv},
  year    = {2024}
}


@article{Robinson2022LeveragingLL,
  title   = {Leveraging Large Language Models for Multiple Choice Question Answering},
  author  = {Joshua Robinson and Christopher Rytting and David Wingate},
  journal = ICLR,
  year    = {2023}
}

@inproceedings{Maaz2023VideoChatGPTTD,
  title     = {Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models},
  author    = {Muhammad Maaz and Hanoona Abdul Rasheed and Salman H. Khan and Fahad Shahbaz Khan},
  booktitle = {Annual Meeting of the Association for Computational Linguistics},
  year      = {2023}
}

@article{Lin2023VideoLLaVALU,
  title   = {Video-LLaVA: Learning United Visual Representation by Alignment Before Projection},
  author  = {Bin Lin and Bin Zhu and Yang Ye and Munan Ning and Peng Jin and Li Yuan},
  journal = {ArXiv},
  year    = {2023},
  volume  = {abs/2311.10122},
  url     = {https://api.semanticscholar.org/CorpusID:265281544}
}

@article{Ma2023VistaLLaMARV,
  title   = {Vista-LLaMA: Reliable Video Narrator via Equal Distance to Visual Tokens},
  author  = {Fan Ma and Xiaojie Jin and Heng Wang and Yuchen Xian and Jiashi Feng and Yi Yang},
  journal = {ArXiv},
  year    = {2023},
  volume  = {abs/2312.08870},
  url     = {https://api.semanticscholar.org/CorpusID:266209773}
}

@article{Li2023MVBenchAC,
  title   = {MVBench: A Comprehensive Multi-modal Video Understanding Benchmark},
  author  = {Kunchang Li and Yali Wang and Yinan He and Yizhuo Li and Yi Wang and Yi Liu and Zun Wang and Jilan Xu and Guo Chen and Ping Luo and Limin Wang and Yu Qiao},
  journal = {2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year    = {2023},
  pages   = {22195-22206},
  url     = {https://api.semanticscholar.org/CorpusID:265466214}
}

@article{Li2023LLaMAVIDAI,
  title   = {LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models},
  author  = {Yanwei Li and Chengyao Wang and Jiaya Jia},
  journal = {ArXiv},
  year    = {2023},
  volume  = {abs/2311.17043},
  url     = {https://api.semanticscholar.org/CorpusID:265466723}
}

@article{li2024llara,
  title   = {LLaRA: Supercharging Robot Learning Data for Vision-Language Policy},
  author  = {Li, Xiang and Mata, Cristina and Park, Jongwoo and Kahatapitiya, Kumara and Jang, Yoo Sung and Shang, Jinghuan and Ranasinghe, Kanchana and Burgert, Ryan and Cai, Mu and Lee, Yong Jae and Ryoo, Michael S.},
  journal = {arXiv preprint arXiv:2406.20095},
  year    = {2024}
}

@article{Xiao2023CanIT,
  title   = {Can I Trust Your Answer? Visually Grounded Video Question Answering},
  author  = {Junbin Xiao and Angela Yao and Yicong Li and Tat-Seng Chua},
  journal = CVPR,
  year    = {2024}
}

@inproceedings{min2024morevqa,
  title     = {MoReVQA: Exploring Modular Reasoning Models for Video Question Answering},
  author    = {Min, Juhong and Buch, Shyamal and Nagrani, Arsha and Cho, Minsu and Schmid, Cordelia},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages     = {13235--13245},
  year      = {2024}
}

@inproceedings{wang2025videoagent,
  title        = {Videoagent: Long-form video understanding with large language model as agent},
  author       = {Wang, Xiaohan and Zhang, Yuhui and Zohar, Orr and Yeung-Levy, Serena},
  booktitle    = {European Conference on Computer Vision},
  pages        = {58--76},
  year         = {2025},
  organization = {Springer}
}

@article{wang2024videotree,
  title   = {VideoTree: Adaptive Tree-based Video Representation for LLM Reasoning on Long Videos},
  author  = {Wang, Ziyang and Yu, Shoubin and Stengel-Eskin, Elias and Yoon, Jaehong and Cheng, Feng and Bertasius, Gedas and Bansal, Mohit},
  journal = {arXiv preprint arXiv:2405.19209},
  year    = {2024}
}

@inproceedings{Wang2023LifelongMemoryLL,
  title     = {LifelongMemory: Leveraging LLMs for Answering Queries in Long-form Egocentric Videos},
  author    = {Ying Wang and Yanlai Yang and Mengye Ren},
  year      = {2023},
  booktitle = {arxiv}
}

@article{wang2024tarsier,
  title   = {Tarsier: Recipes for training and evaluating large video description models},
  author  = {Wang, Jiawei and Yuan, Liping and Zhang, Yuchen and Sun, Haomiao},
  journal = {arXiv preprint arXiv:2407.00634},
  year    = {2024}
}

@article{wang2024internvideo2,
  title   = {Internvideo2: Scaling video foundation models for multimodal video understanding},
  author  = {Wang, Yi and Li, Kunchang and Li, Xinhao and Yu, Jiashuo and He, Yinan and Chen, Guo and Pei, Baoqi and Zheng, Rongkun and Xu, Jilan and Wang, Zun and others},
  journal = {arXiv e-prints},
  pages   = {arXiv--2403},
  year    = {2024}
}

@article{Park2024TooMF,
  title   = {Too Many Frames, not all Useful: Efficient Strategies for Long-Form Video QA},
  author  = {Jong Sung Park and Kanchana Ranasinghe and Kumara Kahatapitiya and Wonjeong Ryoo and Donghyun Kim and Michael S. Ryoo},
  journal = {ArXiv},
  year    = {2024},
  volume  = {abs/2406.09396},
  url     = {https://api.semanticscholar.org/CorpusID:270440923}
}

@misc{wu2024longvideobench,
  title         = {LongVideoBench: A Benchmark for Long-context Interleaved Video-Language Understanding},
  author        = {Haoning Wu and Dongxu Li and Bei Chen and Junnan Li},
  year          = {2024},
  eprint        = {2407.15754},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2407.15754}
}


@article{Abdin2024Phi3TR,
  title   = {Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone},
  author  = {Marah Abdin and others},
  journal = {ArXiv},
  year    = {2024},
  volume  = {abs/2404.14219},
  url     = {https://api.semanticscholar.org/CorpusID:269293048}
}