\chapter{Understanding Long Videos with Multimodal Language Models}
\label{chapter:mvu}

\section{Introduction}
\label{mvu_sec:intro}

\setlength{\epigraphwidth}{0.9\linewidth}
\epigraph{What can we learn from videos, \\beyond scene context understood from a single natural image?}{}
\vspace{-0.5em}

\noindent
Recent success of large language models (LLMs) and their visual extensions, vision-language models (VLMs), has led to incredible performance on complex language-tied video understanding benchmarks \citep{zhang2023llovi}, particularly on long-video question answering: a task that requires awareness over longer temporal windows \citep{Mangalam2023EgoSchemaAD} as well as causal and temporal action reasoning \citep{dataset_xiao2021nextqa}.
However, the LLMs underlying these approaches contain extensive world knowledge (e.g. understanding of physics, culture, human common sense) and reasoning abilities \citep{yu2023kola,Wang2023GeminiIR}, raising the question of whether they excel at video tasks due to actual \textit{video modality} awareness or simply utilizing world knowledge and contextual information. Such understanding of model reasoning is important for robust deployments avoiding spurious correlation based predictions as well as for better model interpretability \citep{pmlr-v162-yun22a,Xiao2023CanIT}.

\input{src/3_MVU/figures/teaser}

In this work, we systematically study this question in the context of video question-answering (QnA) benchmarks, building two modality-constrained baselines to highlight our findings. These two frameworks are tagged \textit{\llmbaseline} and \textit{\vlmbaseline}. 
The first is constrained to access only the task textual query (i.e. no task-specific visual information). 
The latter is given access to task context with an additional single center-frame from the video as input. 
We discover how these models perform significantly better than random prediction on multiple long-video understanding benchmarks (see \Cref{mvu_tbl:baseline}, similar findings in \cite{min2024morevqa}). 
In fact, the latter, utilizing purely world knowledge and contextual information, even outperforms multiple recent state-of-the-art video understanding works (see \Cref{mvu_tbl:ego_schema}), challenging the notion of how much \textit{video information} is actually utilized by existing approaches to solve these complex video QnA tasks. 

We next focus on efficient inference to allow rapid experimentation with our LLM based frameworks. Therein, we explore suitable prompting and templating to adapt likelihood selection techniques from prior work \citep{Robinson2022LeveragingLL} to video QnA tasks. 
Our resulting framework achieves more efficient inference with improved performance in comparison to prior work that commonly use auto-regressive generation to tackle long-video QnA benchmarks \citep{zhang2023llovi,Balavzevic2024MemoryCE,wang2025videoagent}. 

Motivated by our initial findings on modality-constrained performance, we study how to inject additional video-specific information into our framework using natural language in a concise and interpretable manner to further improve video understanding. We explore three forms of \textit{object-centric} information modalities, develop pipelines requiring zero video-level training to extract such information using off-the-shelf vision tools, and utilize natural language to fuse this multi-modal information using templating operations. Our resulting approach, termed Multi-Modal Video Understanding (MVU) framework, while achieving state-of-the-art zero-shot performance across long-video understanding benchmarks, also exhibits better interpretability (e.g. exposing video-specific information utilized) through its language-based operation.  Moreover, our proposed MVU exhibits generality with its strong performance even on robotics domain tasks. 

\noindent In summary, our key contributions are as follows:
\begin{enumerate}
[leftmargin=3.0em,noitemsep,topsep=-0.4em,itemsep=-1.0ex,partopsep=0ex,parsep=1ex]
    \item Uncover surprisingly strong performance on complex video-language tasks by modality-constrained baselines with limited access to video-specific information. 
    \item Adapting Likelihood Selection strategies to video QnA benchmarks for efficient evaluation.
    \item Novel VLM-based video QnA framework that extracts concise video specific object-centric information followed by natural language based fusion.
\end{enumerate} 
\vspace{0.5em}

We integrate our MVU framework over multiple different baselines and obtain performance improvements across 20 different datasets establishing both its effectiveness and generality. Our evaluations are performed zero-shot with no video-level training on these datasets which cover video QnA tasks (short, medium, and long videos) as well robotics domain tasks. 


\section{Naive Baselines \& Likelihood Selection}
\label{mvu_sec:baselines}

In this section, we first establish our problem setting, then discuss adapting likelihood selection for video QnA tasks, and finally introduce two naive LLM based frameworks for video question answering tasks, tagged \textit{\llmbaseline} and \textit{\vlmbaseline} (see \Cref{mvu_fig:teaser}). 

\subsection{Problem Formulation}
We focus on two categories of video understanding tasks: 
\begin{enumerate}
	[leftmargin=2.5em,noitemsep,topsep=0.3em,itemsep=-1.0ex,partopsep=0ex,parsep=1ex]
	\item Long Video Question Answering (Multiple-Choice-based Selection)
	\item Open Ended Video Question Answering (Text Generation)
\end{enumerate} 
% \vspace{0.5em}
For the first task, we construct a unified problem formulation accounting their choice based selection aspect. For the latter, we resort to standard LLM based answer generation.  

Consider a video $x_v \in \mathbb{R}^{L\times H\times W\times C}$, a textual question $x_t$, a set of textual candidate answers $Y = \left\{ y_i, i=1, ..., M  \right\}$, and a model $V( \cdot )$ selecting one answer from the given set of answers (noted as $\hat{y}:= V(x_v, x_t, Y)$). Selected $\hat{y}$ should ideally be identical to groundtruth $y_g$.
Here $L, H, W, C$ are the number of frames of the video, frame height, width, and number of channels respectively. $M$ is the number of candidate answers.
% 
For multiple choice based selection tasks, $x_v$, $x_t$, and $Y$ are directly present in dataset.
For N-Way Classification tasks, we set $x_t$ as a generic question (details in \Cref{mvu_app:template}) and formulate $Y$ by applying a fixed template to the labels of all N classes of the dataset.
This formulation is used for the remainder of the paper unless a specific exception is noted. 

In the case of open-ended video question answering, we follow standard settings of LLM based text generation for video tasks following \cite{Maaz2023VideoChatGPTTD}. 


\subsection{Likelihood Selection}
\label{mvu_subsec:likelihood}
The common technique for LLM based  approaches tackling question answering (QnA) tasks is likelihood based choice selection (also referred as Cloze Prompting, see \cite{Robinson2022LeveragingLL}).  Adopting such likelihood based selection for different tasks (or to VLMs) is however not straightforward \citep{Robinson2022LeveragingLL}, leading to most existing long video QnA approaches resorting to LLM based answer generation. In fact, most existing long-video QnA approaches using LLMs / VLMs for choice selection \citep{papalampidi2023simple,wang2022internvideo,Balavzevic2024MemoryCE} resort to full answer generation followed by embedding or template based matching to ground-truth choices, incurring significant inference costs for evaluation. 

\input{src/3_MVU/figures/selection}

In light of this, we explore prompting and likelihood calculation techniques optimal for applying \textit{Likelihood Selection} on long video QnA tasks with either LLMs or even VLMs. 
Adapting this technique unlocks autoregressive LLMs / VLMs ability to solve multiple selection problems with only \textit{one} forward pass as illustrated in \Cref{mvu_fig:selection}. This is in contrast to next token sampling requiring iterative generations dependent on previous outputs for each answer token. 
This process uses a likelihood measure based on the LLM latent space allowing better semantic awareness compared to exact or template matching. In addition to the candidate answer batching, we follow prior work to include all candidates in the prompt as well. We direct the reader to \Cref{mvu_app:ls_prompts} for complete details on semantic awareness, candidates in prompts, and video QnA specific implementation.

In addition to the considerable inference speed-up from likelihood selection, we also obtain the additional advantages of avoiding LLM hallucinations and deviations from expected output formats over iterative generation strategies applied to similar visual tasks (see \cite{Hanu2023LanguageAT}). We empirically validate the improved performance from such behavior in our ablations (see \Cref{mvu_ablate:ls}).


\subsection{Modality Constrained Variants}
\label{mvu_subsec:modal_intro}
We next introduce the two modality-constrained variants of our framework tagged \textit{\llmbaseline} and \textit{\vlmbaseline} (illustrated in \Cref{mvu_fig:teaser}). 
% 
The former utilizes only the task question injected as language ($x_t$) with no other task-specific information. Note how this naive variant does not access any information extracted from the video for each task instance. 
The latter utilizes an additional center visual frame ($x_v^{c}$), extracted from the center of the video ($x_v$) timeline. This variant accesses no \textit{video-specific} data (e.g. temporal or motion information). The center frame usage ensures no temporal information leakage in frame selection for this variant.

We hypothesize that \llmbaseline with no access to task-specific knowledge is constrained to generate predictions utilizing its internal world knowledge (e.g. physics, culture, human common sense). We refer to this as \textit{world modality}. For a given question regarding a natural video and a set of candidate answers, there is a possibility that one choice is more probable given how our world operates. In cases that this choice turns out to be correct, the internal world knowledge of the LLM allows it to easily select that choice resulting in above random performance. This variant of our framework highlights such cases in long video QnA tasks. 
A similar baseline is used in \cite{min2024morevqa}.

In the case of \vlmbaseline, it is provided with task information but is limited to a single frame, which could possibly provide important scene context. Therein, we refer to this variant as operating with world and \textit{contextual} information modalities. For example, consider a video with a man walking a dog. The scene context of the dog and man combined with the LLM world knowledge and reasoning skills may be sufficient to correctly answer the question with no temporal or motion information. Performance of this variant highlights the prevalence of similar cases in long video QnA tasks when using LLM based approaches.  

We evaluate these two modality-constrained variants and summarize our findings in \Cref{mvu_tbl:baseline}. We uncover surprisingly strong performance of both variants on two long-video understanding benchmarks. 
In the case of \llmbaseline variant, we achieve performance significantly higher than random selection (+25.8\% on ES-S / +20.1\% on NextQA-T) using zero visual information. This indicates the large portion of questions in existing video-QnA benchmarks that can be answered correctly purely using world knowledge. 
We also highlight our \vlmbaseline performing on par with state-of-the-art LLM based approach from \cite{zhang2023llovi}. In particular, for ES-S we outperform \cite{zhang2023llovi} which uses information extracted from 180 frames per video incurring an inference cost over 100 times higher than ours. In light of these findings, we argue that long video understanding approaches in particular must focus on learning information beyond what a single frame baseline can achieve, possibly in an interpretable manner. 

\input{src/3_MVU/figures/tbl_baseline}

Therein, we introduce \textit{Multimodal Video Understanding} (MVU), a simple framework that aggregates multimodal video-relevant information in an interpretable manner using natural language and achieves significant improvements over baselines across multiple datasets. 


\section{Multimodal Video Understanding Framework}
\label{mvu_sec:mvu}
In this section, we introduce in detail our Multimodal Video Understanding (MVU) framework that integrates several information modalities extracted from video using \textit{natural language} as a medium for information fusion. Our approach adapts off-the-shelf vision tools to construct a powerful long video understanding agent that requires no additional training on videos.
We first utilize vision tools to extract information relevant to three object-centric modalities from uniformly sampled video frames. Next, we leverage suitable prompt templates to aggregate these as natural language. This video level information is injected into our \vlmbaseline variant providing it with video specific awareness. We illustrate an overview of our framework in \Cref{mvu_fig:arch}.


\subsection{Vision Tools for Video Analysis}

Image trained VLMs contain information valuable for video tasks and have been widely used in prior work \citep{zhang2023llovi}. In our proposed framework, we take a step further, exploring more off-the-shelf vision tools trained only on images, in particular object detection and object tracking approaches, in addition to a VLM re-purposed as an image captioner. 

We use an image captioner to identify all unique objects present within a video. For this purpose, we prompt a generative vision language model to list all objects within a given video frame (image) in an open-ended manner. We note how a VLM trained only on images is sufficient for this. In our case, we use a VLM identical to the one in \cite{zhang2023llovi} but applied on significantly less video frames, making our comparisons fair in terms of model size.   

For the case of object detection, we use an open-vocabulary object detector from \cite{Minderer2022SimpleOO} that is trained only on images, and apply it with object category names from captioner to obtain their location information, i.e. image-space coordinates for each unique object. Given the lightweight nature of this detector in comparison to the image captioner, we note how it can be applied more densely (i.e. on more frames) than the captioner without increasing compute demand significantly. Furthermore, the detector acts as a secondary check, grounding the object category names to individual frames, and therein countering any object hallucinations by the captioner. 

Our final tool is an object tracker from \cite{Wang2018FastOO} used to convert our per-frame object detections into motion trajectories spread across the entire video. We feed the tracking algorithm with the locations of each object alongside per-object features extracted from our detector in order to construct motion trajectories for each unique object. 

% We next focus on how these vision tools are utilized to extract object-centric information from a given video. 

\input{src/3_MVU/figures/arch}

\subsection{Object-Centric Information Modalities}
\label{mvu_subsec:obj_modal}

Given off-the-shelf tools suitable for extracting information from videos, we next focus on the exact forms of information, i.e. three object-centric information modalities. We consider all object categories across the video, spatial locations of individual object instances, and their movement across time. We define these as follows:

\begin{enumerate}
    \item \textbf{Global Object Information ($x_{\mathrm{GOI}}$):}
    In this stage, we introduce global information that spans beyond a single video frame. For a given video, we first uniformly sample 8 frames. For each of the 8 selected frames, we utilize our image captioner to generate object lists and obtain a set of distinct object categories contained within each frame across the video. 
    % 
    \item \textbf{Object Spatial Location ($x_{\mathrm{OSL}}$):}
    Given objects present per video, we utilize our open-vocabulary object detector to localize each object category (from previous stage) on to frame coordinates. Categories not localized by the detector are dropped. Additionally, we utilize similarity of feature vectors for same class objects to track object instances across frames using our tracker. Following prior work \citep{RanLearningtoLoc23}, we calculate average center coordinates and scale value for each object instance across all frames. 
    This results in a set of distinct objects $O$ across the video, $O = \{(o_1, q_1), (o_2, q_2), ... \}$. \
    Here, $o_k$ describes the object category in natural language while $q_k$ contains the x, y coordinates of object center and the scale term (area of minimal object bounding box as a ratio to image size, i.e. box area $\div$ image size). 
    % 
    \item \textbf{Object Motion Trajectory ($x_{\mathrm{OMT}}$):}
    Next, we leverage the calculated cross-frame object tracks and compute motion trajectories for each object. This modifies our set of distinct objects, pairing each object $o_k$ with its trajectory ($o_k^1 \rightarrow o_k^2 \rightarrow ... $) across the video frames. We construct an updated set $Z = \{(o_1, q_1^1 \rightarrow q_1^2 \rightarrow ...), (o_2, q_2^1 \rightarrow q_2^2 \rightarrow ...), ... \}$. Intuitively, this information should explicitly capture object motion information. 
\end{enumerate}
We provide further details including examples of each information modality for selected samples (video question pairs) in \Cref{mvu_app:template}. 

This pipeline for extracting per-frame information using an image-trained VLM closely resembles prior work such as \citep{zhang2023llovi}. While motivated by such work, we explore the direction of how more fine-grained information could be extracted from videos to solve these tasks more efficiently. 
% 
Given the role of object interactions in defining the various actions and events in videos, we hypothesize that extracting object-centric information (as opposed to generic frame-level descriptions) followed by modeling of their temporal dependencies would provide more concise representations better suited to efficiently solve these tasks. 


% We also highlight how the use of vision tools to extract specific forms of information provides an additional level of interpretability to the functions of our overall pipeline. We next discuss how we fuse each of these information modalities using natural language as a medium. 


\subsection{Language based Fusion}
\label{mvu_subsec:oc-modalities}
Inspired by \cite{zeng2022socratic}, we construct our overall framework by injecting these three forms of object-centric information into our setup using natural language. 
We represent each modality in a fixed template-based fusion. 
Global object information is represented as a list of category labels, e.g., $x_{\mathrm{GOI}} = $ \{\textit{person, oven, dishwasher, ..., sink}\}. 
Object spatial location modifies this list to include center coordinates ($x, y$) and scale ($s$) where scale is the area percentage occupied by the best-fitting object bounding box. 
For e.g., $x_{\text{OSL}} = $ \{\textit{person located at} (0.2, 0.3, 0.07), ... , \textit{oven located at} (0.8, 0.6, 0.04)\}. 
Finally, object motion trajectories update the list to contain frame-level trajectories, 
e.g., $x_{\text{OMT}} = $ \{\textit{person moving as} [0.2, 0.3, 0.07] $\rightarrow$ [0.2, 0.4, 0.06]  $\rightarrow$ [0.2, 0.6, 0.08], \textit{oven moving as} ... \}. Similar to the examples, information from each object-centric modality is represented in textual form to allow their direct fusion and integration into our framework (as additional language inputs). 
Therein, we describe the resulting setup, our overall framework MVU as follows, 
\begin{align}
    \hat{y} = \mathcal{F}_{\text{MVU}}(x_t, {x}_v^c, x_{\text{GOI}}, x_{\text{OSL}}, x_{\text{OMT}})
\end{align}
where ${x}_v^c$ is the center frame extracted from the video $x_v$ (more details in \Cref{mvu_app:template}).
% 
In comparison to prior work such as \cite{zhang2023llovi}, we note that our fused information is more concise allowing better utilization of the fixed context length in an LLM (see \Cref{mvu_app:llm_context} for more details). 


\input{src/3_MVU/figures/tbl_ego}

\section{Experiments}
\label{mvu_sec:exp}

In this section, we first discuss our experimental setup and datasets. Next, we evaluate MVU on multiple video question-answering and robotics task benchmarks followed by ablative studies.  

\vspace{0.5em}
\noindent \textbf{Experimental Setup:}
Our proposed MVU framework and its variants use off-the-shelf models trained on images, thus requiring no re-training of these models. 
For our evaluations, we directly use these models, utilizing two NVIDIA RTX A5000 24GB GPUs for inference.  
We evaluate on two video question answering datasets focused on long-form videos: EgoSchema \citep{Mangalam2023EgoSchemaAD} and NExT-QA \citep{dataset_xiao2021nextqa}.
We also evaluate using a series of robotics datasets from the Open X-Embodiment robotics dataset \citep{open_x_embodiment_rt_x_2023} to test our model generality (more details in \Cref{mvu_subsec:robotics}). 
We discuss further details of pretrained models and datasets in \Cref{mvu_app:models_data}. 
Also, note that none of the pretrained components of our framework undergo any form of video-level training.


\subsection{Long Video Question Answering}
\label{mvu_subsec:eval_longvid}

Long video question answering benchmarks aim to measure causal and temporal reasoning abilities of models over long temporal windows \citep{dataset_xiao2021nextqa,Mangalam2023EgoSchemaAD}. In this section, we evaluate our framework on two benchmark datasets and present our results in \Cref{mvu_tbl:ego_schema} and \Cref{mvu_tbl:next}. 

On EgoSchema dataset, results reported in \Cref{mvu_tbl:ego_schema} demonstrate the state-of-the-art performance of our framework. 
We integrate MVU over SF-VLM and LVNet \citep{Park2024TooMF} baselines for fair comparison to work operating under different settings. We reiterate how our approach is both zero-shot and requires no video-level training (and our selected baselines are similar). 
% 
In comparison to prior work utilizing open models, our \texttt{SF-VLM+MVU} achieves clear performance improvements, even out-performing works using video-caption supervision for training \citep{papalampidi2023simple,Balavzevic2024MemoryCE}. Compared to methods utilizing proprietary closed language models extending to trillion parameter scale \citep{zhang2023llovi,wang2023vamos,min2024morevqa,wang2025videoagent}, our \texttt{LVNet+MVU} variant using similar scale achieves improved performance. 
We also implement several such large-scale approaches under scaled-down common settings as our smaller variant (details in \Cref{mvu_app:baselines}), where we again achieve clear performance gains. 


\input{src/3_MVU/figures/tbl_next}

Next, we evaluate our framework on the NextQA benchmark and report these results in \Cref{mvu_tbl:next}. 
We similarly integrate MVU with two baselines.
Our MVU achieves state-of-the-art results under zero-shot settings. While \cite{yu2024sevila} outperforms our approach, we note how they require video-caption localization pretraining and appears to overfit to this dataset considering their relatively lower performance on other datasets (see \Cref{mvu_tbl:ego_schema}). 

We also evaluate MVU on the LongVideoBench dataset which contains even longer videos and present these results in \Cref{mvu_app:longvideobench}.
While these three datasets focus on MCQ style QnA, we also explore the generality of our MVU framework on open-ended style QnA tasks in \Cref{mvu_app:open_qa}.


\subsection{Robotics Domain Action Recognition}
\label{mvu_subsec:robotics}
We investigate generalization capabilities of our proposed MVU by evaluating across datasets from robotics domain Open X-Embodiment \citep{open_x_embodiment_rt_x_2023}, following a QnA style formulation of the dataset (details in \Cref{mvu_app:openx}). We highlight visual differences of this data in \Cref{mvu_fig:dt_example}.
% 
We present evaluations in \Cref{mvu_tbl:robotics}, which indicate clear performance improvements for MVU over the baseline from \cite{zhang2023llovi}. The purpose of this experiment is to evaluate the generality of our approach to video domains different from everyday natural videos. We take these promising results to indicate the strong generality of our framework. 
% 
Furthermore, we note how our modality constrained variants do not perform significantly better than random on these robotics domain tasks (details in \Cref{mvu_app:modality}). We attribute this to the significant domain shift in terms of the world of operation in this domain (i.e. robotics tasks tend to involve controlled environments very different to what humans face on an everyday basis).   


\input{src/3_MVU/figures/tbl_robot}
\input{src/3_MVU/figures/merged_examples}

% \input{src/3_MVU/figures/tbl_baseline}

\subsection{Ablations}
\label{mvu_subsec:ablate}
In this section, we systematically dissect our overall MVU framework to establish the usefulness of each of its individual component %
(see \Cref{mvu_app:ablate_more} for more ablations). We first ablate our three different information modalities and report these results in \Cref{mvu_ablate:modal}. Our evaluations indicate clear performance improvements from each of our object-centric information modalities. 

\begin{table}[t]
\centering
\small
\begin{minipage}{0.48\textwidth}
\caption[MVU Ablation]{\textbf{MVU Ablation:} 
We report accuracy (\%) on public subset of EgoSchema (ES-S). In table header, VI stand for visual inputs and GOI, OSL, OMT refer to our object-centric information modalities (see \Cref{mvu_subsec:obj_modal}). 
Each information modality results in clear performance improvements with our full MVU achieving best performance.}
\label{mvu_ablate:modal}
\end{minipage}
\hspace{0.01\textwidth}
\begin{minipage}{0.48\textwidth}
\vspace{-1.0em}
\def\arraystretch{1.0}  % height
\setlength\tabcolsep{0.6em}  % width
\scalebox{0.88}{
\begin{tabular}{lcccccc}
\toprule
Method         & VI  & GOI & OSL & OMT & ES-S       \\ \midrule
\llmbaseline (ours)    & \xmark & \xmark & \xmark & \xmark & 45.8 \\
SF-VLM (ours)          & \cmark & \xmark & \xmark & \xmark & 55.8 \\
MVU (ours)      & \cmark & \cmark & \xmark & \xmark & 56.4 \\
MVU (ours)      & \cmark & \cmark & \cmark & \xmark & 58.6 \\ \rowcolor{Gray}
MVU (ours-full) & \cmark & \cmark & \cmark & \cmark & \textbf{60.3} \\ \bottomrule
\end{tabular}}
\end{minipage}
\vspace{-1.5em}
\end{table} 

We next perform ablations on our adaptation of likelihood selection strategy for video QnA tasks using Ego-Schema subset (ES-S) and Next-QA test-set (NQA-T) . These results reported in \Cref{mvu_ablate:ls} indicate clear performance boosts due to our adaptation of likelihood selection (LS). When removing LS, standard generation (i.e. generate an answer and match against ground-truth selection following \cite{zhang2023llovi}) is utilized with our MVU framework. We also report naive adaptation of LS following \cite{Robinson2022LeveragingLL} where the choice options are directly used, highlighting the importance of our prompting techniques.  
We also note the accuracy gains obtained through LS, and attribute these to reduced LLM output hallucination and deviations from expected output formats, that are commonly observed with iterative generation \citep{Hanu2023LanguageAT}.

We next ablate our overall framework against the existing work, \cite{zhang2023llovi}, by replacing our MVU object-centric information pipeline with the frame description approach in \cite{zhang2023llovi}. We construct a setup identical to our framework except for the inputs to our final stage VLM replaced with frame level descriptions. These results reported in \Cref{mvu_ablate:baseline} indicate the clear significance and improvement of our proposed object-centric information pipeline over simple frame descriptions. The 8 frame variant is the same speed comparison as MVU uses captioner only on 8 frames. Our MVU outperforms both that and the slower 16 frame baseline. We also note the performance drop in the baseline when increasing the number of frames from 16 to 180. While consistent with observations in prior works for long-video tasks \citep{Mangalam2023EgoSchemaAD}, we attribute this drop to decreased signal-to-noise ratio with the introduction of additional frame descriptions. This further highlights the importance of selecting useful information from video frames, and we reiterate how the object-centric information in our MVU framework serves this purpose. 

\begin{table}[t]
\begin{minipage}{0.48\textwidth}
    \centering
    \small
    \caption[Likelihood Selection Ablation]{
    \textbf{Likelihood Selection (LS) Ablation:} Results indicate clear improvements in both accuracy (\%) and inference time (s) with our adaptation of likelihood selection for video tasks.     
    }
    \label{mvu_ablate:ls}
    \vspace{-0.5em}
    \def\arraystretch{1.1}  % height
    \setlength\tabcolsep{0.9em}  % width
    \scalebox{0.85}{
    \begin{tabular}{lcccc}
    \toprule
    Method            & LS & ES-S & NQA-T & Time \\ \midrule
    Generation        & \xmark & 56.4 & 55.3 & 12.7 \\ 
    LS-Naive          & \xmark & 58.2 & 35.8 & 2.42 \\ \rowcolor{Gray}
    LS-MVU (ours)     & \cmark & \textbf{60.3} & \textbf{55.4} & \textbf{2.42} \\ \bottomrule
    \end{tabular}
    }
\end{minipage}
% 
\hspace{0.01\textwidth}
% 
\begin{minipage}{0.48\textwidth}
    \centering
    \small
    \caption[Baseline Ablation]{\textbf{Baseline Ablation:} 
    We replace information input to final stage VLM with frame descriptions following \cite{zhang2023llovi}. Accuracy (\%) on public subset of EgoSchema (ES-S). Time in seconds (s).
    }
    \label{mvu_ablate:baseline}
    \vspace{-1.0em}
    \def\arraystretch{1.0}  % height
    \setlength\tabcolsep{1.2em}  % width
    \scalebox{0.85}{
    \begin{tabular}{lcccc}
    \toprule
    Method            & Frames & ES-S & Time \\ \midrule
    Baseline          &   180  & 55.4 & 207 \\ 
    Baseline          &    8   & 55.8 & 2.38 \\  
    Baseline          &   16   & 56.2 & 4.72 \\  \rowcolor{Gray}
    MVU (ours) &   16   & \textbf{60.3} & 2.42 \\ \bottomrule
    \end{tabular}
    }
\end{minipage}
\vspace{-0.5em}
\end{table}



\section{Conclusion}
\label{mvu_sec:conc}

% modality constrained variants - findings
% LS - adopt to video tasks, use with MLMs for the first time
% MVU contribution

In this work, we present a multimodal video understanding framework, termed MVU, that achieves state-of-the-art performance on complex video understanding tasks. In particular, evaluations on long-video question answering and robotics domain question answering demonstrate the strong performance of our MVU framework as well as its generality. We also adapt likelihood selection for efficient LLM-based answer choice selection, separate video-specific information into three object-centric modalities, demonstrate automated extraction of such information using off-the-shelf vision tools, and propose language-based fusion of this multimodal information. 

We also presented two modality-constrained baselines that uncover surprising insights relevant to LLM based video QnA which serves as a basis for our subsequent MVU framework. Furthermore, these results highlight the need for careful evaluation of LLM-based video QnA approaches. 
Revisiting our original motivation on \textit{``what we can learn from videos, beyond scene context understood from a single natural image''}, in this work our two modality-constrained variants uncover surprising insights relevant to this question. We first achieve strong results on long-video understanding benchmarks using no video-specific data, and build over that baseline to showcase the additional performance gains achievable through injecting video-specific information. 


\input{src/3_MVU/app}