\chapter{Literature Review}
\label{chapter:literature}

Controlling what diffusion models generate has emerged as a central challenge following rapid progress in image and video synthesis. Early diffusion probabilistic models established the theoretical foundations of iterative denoising, and subsequent work on score-based generative modeling made these methods practical for high-fidelity image synthesis. The introduction of latent diffusion brought computational efficiency by operating in a compressed latent space, enabling large-scale text-to-image systems such as GLIDE, DALL-E, Imagen, and Stable Diffusion XL. A parallel line of work explored steering these models beyond text prompts: score distillation sampling showed that frozen diffusion models could supervise optimization in arbitrary parameter spaces, while adapter-based conditioning mechanisms such as ControlNet demonstrated that spatial signals like edges, depth, and poses could guide generation without full retraining. As image diffusion matured, attention shifted to the temporal domain, with video diffusion models extending these architectures to generate coherent motion---though controlling that motion remained largely unsolved.

Building on this progression, a growing body of work has sought finer-grained control over both the spatial and temporal structure of diffusion model outputs. Training-free approaches revealed that frozen models encode rich semantic structure that can be extracted through inference-time optimization alone, enabling tasks from zero-shot segmentation to multi-view illusion generation without any model retraining. On the data and conditioning side, the scarcity of large-scale alpha matting datasets has limited the ability to train models with precise transparency boundaries, and generating such datasets synthetically presents its own challenges around background selection and automated quality control. For video, the difficulty of specifying motion through text alone has motivated noise-space manipulation approaches that inject temporal structure directly into the sampling process, as well as trajectory-based editing methods that modify object and camera motion in existing footage. Despite these advances, most control methods address either spatial or temporal domains but not both, and the field lacks a unified perspective on how different control mechanisms relate to one another.

Motivated by these converging directions, this thesis investigates controlling diffusion models across a range of tasks and modalities. We develop methods that extract spatial localization from frozen diffusion models for zero-shot segmentation (Peekaboo), compose score distillation estimates under multiple geometric transformations to generate optical illusions (Diffusion Illusions), construct a large-scale alpha matte dataset through automated chroma keying of generated images (MAGICK), inject motion structure into video generation through noise warping along optical flow fields (Go-with-the-Flow), and enable true video-to-video motion editing via training on synthetic motion counterfactual pairs (MotionV2V). Each section that follows presents the detailed related work specific to its contribution.

\input{src/2_DiffIllusions/related_work_thesis}

\input{src/1_Peekaboo/related_work_thesis}

\input{src/3_MAGICK/sec/2_relatedwork_thesis}

\input{src/4_GWTF/sec/2_related_work_thesis}

\input{src/5_MotionV2V/sec/relatedworks_thesis}
