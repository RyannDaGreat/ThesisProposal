\section{Zero-Shot Segmentation with Diffusion Models}
\label{peekaboo_sec:related}
\textbf{Vision-Language Models:} Vision-language models have advanced rapidly, enabling zero-shot image recognition \cite{frome2013devise,socher2013zero} and language generation from visual inputs \cite{karpathy2015deep,vinyals2015show,kiros2014unifying,mao2014explain}. Recent contrastive language-image pre-training models \cite{clip,jia2021scaling} showcase open-vocabulary and zero-shot capabilities, with extensions for a wide range of tasks \cite{pham2021combined,yu2022coca, desai2021virtex, yao2022filip,cui2022democratizing,Zeng2022SocraticMC, yuan2021florence, kamath2021mdetr}. Grounding language to images \cite{Gu2022OpenvocabularyOD, Ghiasi2021OpenVocabularyIS} and unsupervised segmentation \cite{Xu2022GroupViTSS, Ranasinghe2022PerceptualGI} have also been explored. Our proposed \modelname leverages an off-the-shelf diffusion model without segmentation-specific re-training and handles sophisticated compound phrases.

\textbf{Diffusion Models:} Diffusion Probabilistic Models \cite{pmlr-v37-sohl-dickstein15} have been adopted for language-vision generative tasks \cite{Nichol2022GLIDETP,dalle,dalle2,imagen,palette,parti,sr3} and extended to various applications \cite{wavegrad,videodiffusion,diffwave, Poole2022DreamFusionTU}. Recent works \cite{Poole2022DreamFusionTU, graikos2022diffusion} sample diffusion models through optimization. Our \modelname utilizes efficient latent diffusion models and is the first zero-shot method for cross-modal discriminative tasks such as segmentation.

\textbf{Score Distillation Loss:} SDL was introduced in DreamFusion \cite{Poole2022DreamFusionTU} and applied to NeRF \cite{mildenhall2020nerf} to create 3D models. Our work applies it to alpha masks for image segmentation, yielding better results than previous CLIP-based techniques \cite{crowson2022vqganclip,jain2021dreamfields,khalid2022clipmesh}. Like Peekaboo, SDL is also used with Stable Diffusion in \cite{burgert2023diffusion_illusions,lin2022magic3d,wordasimage}.

\textbf{Unsupervised Segmentation:} Unsupervised segmentation \cite{malik2001visual} has evolved from early spatially-local affinity methods \cite{comaniciu1997robust,shi2000normalized,ren2003learning} to deep learning-based self-supervised approaches \cite{caron2021emerging, hamilton2022unsupervised, Cho2021PiCIEUS, VanGansbeke2021UnsupervisedSS, Ji2019InvariantIC}. LSeg \cite{li2022language} is a semi-supervised segmentation algorithm because it's trained with ground truth segmentation masks, but attempts to generalize the dataset labels to language using CLIP embeddings. Our \modelname also enables grouping aligned to natural language, is open-vocabulary.

\textbf{Referring Segmentation:} Referring segmentation \cite{hu2016segmentation,yu2018mattnet,ye2019cross} involves vision and language modalities. Early approaches \cite{hu2016segmentation,liu2017recurrent,li2018referring,margffoy2018dynamic} fuse features, while recent works use attention mechanisms \cite{chen2019referring,shi2018key,ye2019cross,huang2020referring,huilinguistic} and cross-modal pre-training \cite{Wang2022CRISCR}.
Large supervised segmentation models \cite{SAM,SEEM} have demonstrated high performance, but they require annotated segmentation datasets. Concurrently, an unsupervised referring segmentation method \cite{ODISE} has been developed using the same Stable Diffusion model as us, but it requires significant computational resources, including 5.3 days of training with 32 NVIDIA V100 GPUs. 
In contrast, \modelname is the first to perform unsupervised referring segmentation without necessitating any model training, effectively reducing the training time to 0 days on 0 GPUs.
