\begin{figure*}[t]
	\centering
	\begin{minipage}{0.65\linewidth}
		\includegraphics[width=\linewidth]{figures__implicit.png}
	\end{minipage}
\hspace{0.02\linewidth}
	\begin{minipage}{0.3\linewidth}
		\caption{\textbf{Implicit neural function:} We illustrate the architecture of the multi-layer perceptron (MLP) network used to represent the learnable alpha masks in parametric form. The number of layers in the network, $L=4$. Masks are converted to Fourier domain and encoded as vectors $\bu$ and $\bv$ which are in turn represented by the weights of the MLP network ($\phi$). We highlight that our loss functions are used only for updating the parameters of this neural network used to represent the alpha masks.}
		\label{fig:implicit}
	\end{minipage}
	% \vspace{-0.5em}
%	\vspace{-1.0em}
\end{figure*}