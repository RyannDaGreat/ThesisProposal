\section{Related Work}
\label{sec:relatedwork}

\paragraph{Synthetic segmentation data generation:} 
Many methods have recently been proposed to synthetically generate segmentation data. Early methods utilize GANs for data synthesis.
DatasetGAN~\cite{zhang2021datasetgan} proposes to decode GAN latent codes to generate segmentation data, primarily of specifc object parts or of limited scenes like bedrooms.
BigDatasetGAN~\cite{zhang2022bigdatasetgan} produces masks for single primary objects by training a GAN on Imagenet~\cite{ILSVRC15}.

Diffusion-based model typically take a text prompt as input, then simultaneously synthesize an image along with a mask. The mask may be of a single primary object~\cite{wu2023diffumask,li2023open} or a semantic segmentation within the domain of an existing hand-labeled dataset~\cite{wu2023datasetdm,nguyen2023dataset}. Variations of this theme include generating multiple images and object masks at once~\cite{xie2023mosaicfusion} or generating the masks first and then the image~\cite{ye2023seggen}. Peekaboo~\cite{burgert2023peekaboo} generates single objects, the most closely related method to our own, but the results lack details like hair and fur. While arguments can be made against training with generated data~\cite{alemohammad2023selfconsuming,shumailov2023curse}, these works show that training with synthetic data, either alone or in conjunction with real data, yields results that are on par with or surpass the state-of-the-art set by methods trained with real data. 

A drawback of these methods is they all focus on binary masks and lack transparencies and fine details such as hair or fur. Our method specifically targets generating images with alpha mattes that exhibit fine details.



\vspace{-4mm}
\paragraph{Alpha matting datasets:}  
While our method is the first to generate images with alpha masks, many alpha matting datasets already exist. Generating alpha mattes is a difficult task, requiring complicated image capture methods and/or excessive user interaction, resulting in the existing matting datasets being small in size. The early matting dataset from alphamatting.com~\cite{rhemann2009perceptually} was generated using triangulation matting, a tedious process of photographing the same object against multiple backgrounds, resulting in a dataset of 27 training images and 8 test images. This was extended to video to produce a small number of frames using stop-motion photography~\cite{erofeev2015perceptually}. 

Manual extraction of the alpha matte from photographs using existing matting methods has been used to create several datasets~\cite{xu2017deep,shen2016deep,wang2021crgnn,sun2021semantic}. However, this approach is very time consuming and prone to error, generally resulting in only a few hundred objects.

Video matting datasets often use chroma keying~\cite{smith1996blue} to generate alpha data~\cite{zhang2021attention,lin2021real}. However, high quality chroma keying requires both careful setup of the green (blue) screen and lighting as well as manual post-processing to tweak parameters and manually correct or mask out mistakes.

While these methods can produce high-quality alpha mattes, they are difficult and tedious to collect. This contrasts our approach that can produce large numbers of accurate alpha mattes with the corresponding images with minimal user interaction.

\vspace{-4mm}
\paragraph{Segmentation and Matting:}  

Arguably, an alternative method of producing a dataset like ours would be to simply extract objects from generated images with standard segmentation/matting methods without bothering to produce them on green screens. Such generated images would contain background details and potentially other foreground objects that would need to be separated from the object.

While many segmentation datasets exist~\cite{lin2014microsoft,zhou2017scene,OpenImages,Cordts2016Cityscapes}, as do many methods for segmenting salient objects~\cite{li2014secrets,li2017instance,qin2022highly} or multiple objects~\cite{kirillov2019panoptic,he2018mask,Qi_2023_ICCV} from images, these methods would not yield the accurate alpha mattes that our method produces.  Matting methods~\cite{xu2017deep,soumyadip2020background,cai2019disentangled,lu2019indices,tang2019learning,dai2022boosting}, despite significant progress recently, are still imperfect and would include errors or artifacts in the alpha mattes. Indeed, we hope our dataset will be used to improve matting methods in future works.


\begin{comment}

\bp{Please cite segementatin methods such as dichotomous image segmentation for foreground selection and the photoshop subject selection if it exists. Also bring up the fact that there are many large sregmentation datasets, but these do not handle matting and are naturally imprecise even for segmentation (coco for example has fairly polygonal boundaries a lot of the time, and doesn't focus on single-subjects like ours does). (barring perhaps the https://xuebinqin.github.io/dis/index.html dichotomous image segmentation dataset, which contains under 6000 samples, all of which had to be manually annotated in gimp and *still* don't contain any matting information. }

\bp{
Important work to talk about because reviewers might try to poke a hole in this dataset because its synthetic:
%
Please also cite "The Curse of Recursion: Training on Generated Data Makes Models Forget" https://arxiv.org/abs/2305.17493 which talks about training on synthetic data. It cautions against it. Brifly mention that paper as it only focuses on LLM's. 
%
Another paper discussed this as well but in the context of image generation:
https://arxiv.org/pdf/2307.01850.pdf
%
They also mention that human-in-the-loop selection can mitigate the degredation of the models, albeit at the cost of sample diversity. However, as we're trying to make isolated subjects, this is arguable desirable. Empiricaly we show its usefulness, and they also mention that adding even a small amount of fresh data is enough to hold back this problem. 
%
I would also mention that this degradation takes more than 2 generations to become a large problem as indicated by the graphs in their paper, and we could always mix our dataset with some other datasets such as dichotomous image segmentation to boost it. 
%
From their paper: """Sampling bias plays a key rôle in autophagous loops. Users of generative models tend to manually curate (“cherry-pick”) their synthetic data, preferring high-quality samples and rejecting low-quality ones. Moreover, state-of-the-art generative models typically feature controllable parameters that can increase synthetic quality at the expense of diversity [42, 58]. We demonstrate that the sampling biases induced through such quality-diversity (precision-recall) trade-offs have a major impact on the behavior of an autophagous training loop. Specifically, we show that, without sampling bias, autophagy can lead to a rapid decline of both quality and diversity, whereas, with sampling bias, quality can be maintained but diversity degrades even more rapidly."""
%
}

\bp{Please also cite SDEDIT - I'm referencing that later in the paper, and show that colloquially its called im2im by both huggingface and the community (you can see that here - in a link in the comment of the code next to this text) 
% https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/img2img 
so I'm calling it img2img as well}

\bp{Not in the related works, but we also have to cite: MSSSIM (multiscale structural image similarity) which is used in the similarity score metric for automatic selection, ControlNet. Canny edges controlnet is straightforward to explain but we also need to briefly explain what "scribbles" controlnet is (see the url in the comment below
%See https://huggingface.co/lllyasviel/sd-controlnet-scribble  --- they explain how they got their dataset. TLDR - it's based on HED edge detection
It's based on HED edge detection (https://arxiv.org/abs/1504.06375 - "Holistically-Nested Edge Detection"
}

\end{comment}