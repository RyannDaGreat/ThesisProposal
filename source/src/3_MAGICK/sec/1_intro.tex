\section{Introduction}
\label{sec:intro}

Recent breakthroughs in diffusion models have led to an abundance of new research in text-to-image generation~\cite{ramesh2022hierarchical,rombach2022high,podell2023sdxl,saharia2022photorealistic}. Given a short text prompt, an entire image can quickly be generated. These images can contain complex foreground objects against complex backgrounds. However, they do not handle use cases in which a user may want an isolated object with an accurate alpha channel.

For example, a user may want to guide the generation of an image with not only a text prompt but also with an accurate alpha mask. While segmentation maps have been used to guide diffusion models~\cite{zhang2023adding}, these maps are rough and do not contain precise details like human hair or transparencies like in a wine glass. 
% Our attempts to use alpha to constrain ControlNet v1.1 
Our attempts to use alpha to condition the off-the-shelf ControlNet v1.1 
yielded results that did not follow the alpha channel 
% (Fig.~\ref{fig:priorwork} top).
(Fig.~\ref{fig:priorwork}).
Other applications could include generating objects to insert into images and generating training data for matting datasets.

%Another application would be inserting generated objects into image. Object insertions could be performed by regenerating the region of the image with an object prompt, but this may result in undesired changes in other objects or the background (Fig.~\ref{fig:priorwork} bottom). Accurate alpha data would need to prevent unwanted changes outside of the inserted object.

The inability of current methods to address generation involving alpha may be due to the lack of training data. While many large-scale segmentation datasets exist~\cite{lin2014microsoft,zhou2017scene,OpenImages,Cordts2016Cityscapes,Qi_2023_ICCV}, they do not contain accurate soft boundaries, usually because they were segmented manually using boundary-tracing tools. Matting datasets contain high-quality alpha ground-truth but are too small for training generation methods due to the difficulty in obtaining ground-truth alpha. For example, the alphamatting.com dataset~\cite{rhemann2009perceptually} only contains 27 images, the Adobe Deep Image Matting (DIM) dataset~\cite{xu2017deep} contains 431 objects, and the Semantic Matting dataset~\cite{sun2021semantic} expands DIM to 726 objects. Without a suitable large-scale alpha dataset, training models with accurate boundaries will remain difficult. 
%Creating large-scale ground-truth matting datasets is a hard task, explaining the small dataset sizes. For training generation datasets, models trained on matting dataset could be used to extract objects from real or generated images, but given the difficulty of natural-image matting such objects would often contain significant errors and artifacts.

\begin{figure}[!t]
  \centering
  %\includegraphics[width=\hsize]{figs/fig1.jpg}
  \includegraphics[width=\hsize]{figs/alpha2rgb.png}
  \vspace{-20pt}
  %\caption{Top: The alpha (left) is used to guide ControlNet v1.1, yielding results that look nice but do not respect the alpha matte (czenter and right). Bottom: Given an image (left), a woman is inserted using Adobe Firefly's generative fill (right). This results undesired changes in the foreground (magenta) and background (yellow).}
  \caption{An alpha matte (inverted for visibility) is used to generate an rgb image using ControlNet trained by our dataset (center) verses the original (right). Note our version trained with our data closely follows the alpha matte.}
  \label{fig:priorwork}
  \vspace{-15pt}
\end{figure}

To address this lack of data, we proposed \methodname, a large-scale dataset of generated objects with high-quality alpha mattes for use in training future generation models. \methodname\ contains 150,000 generated objects across a wide range of object types and includes masks with significant soft edges or transparencies. To create this dataset,  we generate objects on green screen (or other solid colored) backgrounds and extract the objects using chroma keying. Several significant challenges complicate this approach: 1)  an appropriate background color must be selected (generating a green leaf on a green background will yield a poor result), 2) current diffusion-based methods are poor at generating objects on a green screen, producing images with little detail around the edges or producing background that are not constant colored (Fig.~\ref{fig:greenscreenComparison}),  and 3) chroma keying is an underconstrained problem, often failing to produce high-quality alpha without manual post-processing. 

We overcome these challenges with the following approach. 
%First, we utilize both prompt engineering and two diffusion models to produce objects on a colored backgrounds. Given a large collection of description of objects, we \emph{briefly describe two methods used, including choosing appropriate colors.}  
First, for a given prompt, we generate a sample object to determine its primary colors, then select the color least present in the object for the background color. 
We then use two generation models to produce a suitable image for chroma keying: the text-to-image generation method DeepFloyd~\cite{deepfloyd} which generates solid colored backgrounds but foregrounds with poor alpha detail, then the image-to-image generation method proposed in \cite{sdedit} with SDXL~\cite{podell2023sdxl} that can generate a good foreground with alpha detail given the output of DeepFloyd.
Finally, we use a combination of chroma-based and deep-learning based keyers and segmenters to extract the object from the image accurately. This process is shown in Fig.~\ref{fig:pipelineOverview}.

Given this approach, we produced a dataset of 150,000 images (to be released upon publication), a sample of which is shown in Fig.~\ref{fig:datasetExpose}.  To show the utility of \methodname, we train a model on one of the many applicable tasks, alpha-to-image generation. We fine-tune ControlNet~\cite{zhang2023adding} with our dataset to generate RGB images given an input alpha and show improved performance over using the pretrained ControlNet (\cref{fig:priorwork}), showing the utility of our dataset.





\begin{figure*}
    \centering
    \includegraphics[width=1\textwidth]{full_512.jpg}
    \vspace{-18pt}
    \caption{100 image and alpha samples from our dataset. Please zoom in to see all the details. Please see the appendix for more examples.}
    \label{fig:datasetExpose}
\end{figure*}