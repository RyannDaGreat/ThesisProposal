\section{Motion Control in Video Diffusion Models}
\label{gwtf_sec:related_work}

\textbf{Image and Video Diffusion Models:}
With the theoretical establishments of diffusion models~\cite{song2021score,ho2020denoising,song2021denoising,karras2022elucidating} and their practical advancements~\cite{nichol2021glide,ho2022classifier}, and when sophisticated text encoders~\cite{radford2021learning} and language models~\cite{raffel2020exploring} meet diffusion models, great breakthroughs in text-to-image generation~\cite{rombach2022high,podell2023sdxl,stabilityai2023deepfloyd} have revolutionized how we digitize and create visual worlds. Building upon these, image-to-image diffusion models~\cite{brooks2023instructpix2pix,zhang2023adding,ke2024repurposing} enable image editing applications like stylization~\cite{meng2022sdedit}, relighting~\cite{he2024diffrelight}, and super-resolution~\cite{yue2024resshift,stabilityai2023deepfloyd}, expanding creativity in recreating or enhancing visual worlds.

A natural extension of image generation use cases is to cover the temporal dimension for video generation. The most cost-efficient way is to reuse the well-trained image diffusion model weights. Directly querying the above image diffusion models using random noise to generate videos frame-by-frame often struggles with temporal inconsistency, flickering, or semantic drifting. Noise warping, HIWYN~\cite{chang2024warped}, as a method for creating a sequence of temporally-correlated latent noise from optical flow while claiming spatial Gaussianity preservation, yields temporally consistent motion patterns after querying image diffusion models without further fine-tuning. To overcome its defective spatial Gaussianity preservation and undesired time complexity, we propose a novel warped noise sampling algorithm that guarantees spatial Gaussianity and runs fast enough in real time. We validate its efficacy by applying it to the training-free image diffusion models like DifFRelight~\cite{he2024diffrelight} for video relighting and DeepFloyd IF~\cite{stabilityai2023deepfloyd} for video super-resolution.

Video diffusion model training is a more costly yet more effective way for video generation~\cite{brooks2024video,chen2023videocrafter1,blattmann2023align,guo2024animatediff,blattmann2023stable,qing2024hierarchical,xing2024dynamicrafter,yang2024cogvideox}. AnimateDiff~\cite{guo2024animatediff} upgrades pre-trained image diffusion models by fine-tuning temporal attention layers on large-scale video datasets. CogVideoX~\cite{yang2024cogvideox}, a state-of-the-art open-source video diffusion model, combines spatial and temporal dimensions by encoding/decoding videos via 3D causal VAE~\cite{yu2024language} and diffusing/denoising spatiotemporal tokens via diffusion transformers~\cite{peebles2023scalable}. We use CogVideoX~\cite{yang2024cogvideox} as a base model and incorporate our warped noise sampling for motion-controllable fine-tuning. We also fine-tune on AnimateDiff~\cite{guo2024animatediff} to show our method is model-agnostic.

\vspace{0.5em}

\textbf{Motion Controllable Video Generation:}
Beyond text~\cite{guo2024animatediff,yang2024cogvideox} and image controls~\cite{guo2024sparsectrl,xing2024dynamicrafter,zhou2024storydiffusion} for video diffusion models, motion control makes video generation more interactive, dynamically targeted, and spatiotemporally fine-grained. Current approaches to motion control follow three main paradigms:

Firstly, \textit{local object motion control} is represented by object bounding boxes or masks with motion trajectories~\cite{jain2024peekaboo,yang2024direct,wang2024motionctrl,shi2024motion,wu2024draganything,namekata2024sg,qiu2024freetraj,geng2024motion}. DragAnything~\cite{wu2024draganything} allows precise object motion manipulation in images without retraining, while SG-I2V~\cite{namekata2024sg} generates realistic, continuous video from single images using self-guided motion trajectories. These serve as recent baselines for local object motion control. Our method is plug-and-play, treating diffusion models as a black box while using synthetic flows to mimic and densify object trajectories at the pixel level.

Secondly, \textit{global camera movement control} is parameterized by camera poses and trajectories~\cite{yang2024direct,he2024cameractrl,kuang2024collaborative,wang2024motionctrl,xu2024camco,wu2024cat4d} or categorized by common directional patterns like panning and tilting~\cite{guo2024animatediff,yang2024direct}. These methods introduce additional modules that accept camera parameters, trained in a supervised manner. Other approaches~\cite{yu2024viewcrafter,hou2024training} leverage rendering priors as input for camera control. Approaches like ReCapture~\cite{zhang2024recapture} enable reconfiguration of camera trajectories in given videos. Our method bypasses the need for extensive camera parameter collection, and directly generalizes new camera movements from reference videos at inference.

Lastly, \textit{motion transfer} happens from reference videos to target contexts~\cite{wang2024videocomposer,yatim2024space,geyer2023tokenflow,yin2024scalable,ku2024anyv2v,ling2024motionclone,mou2024revideo,aira2024motioncraft}. DiffusionMotionTransfer~\cite{yatim2024space} introduces a loss that maintains scene layout and motion fidelity in target videos, while MotionClone~\cite{ling2024motionclone} uses temporal attention as motion representation, streamlining motion transfer. Using them as motion transfer baselines, we demonstrate our model's flexibility in combining reference geometries with target text guidance.
