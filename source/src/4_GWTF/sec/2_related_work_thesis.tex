\section{Motion Control in Video Diffusion Models}
\label{gwtf_sec:related_work}

\textbf{Image and Video Diffusion Models:} Theoretical advances in diffusion models~\cite{song2021score,ho2020denoising,song2021denoising,karras2022elucidating} paired with large-scale text encoders~\cite{radford2021learning,raffel2020exploring} have driven breakthroughs in text-to-image generation~\cite{rombach2022high,podell2023sdxl,stabilityai2023deepfloyd} and image-to-image editing~\cite{brooks2023instructpix2pix,zhang2023adding,meng2022sdedit,he2024diffrelight}. Extending to video, frame-by-frame generation with independent noise produces temporal inconsistencies. Noise warping~\cite{chang2024warped} addresses this by creating temporally-correlated latent noise from optical flow, yielding consistent motion without fine-tuning. However, its spatial Gaussianity preservation is defective. We propose a novel warped noise sampling algorithm that guarantees spatial Gaussianity and runs in real time. Video diffusion models trained end-to-end~\cite{brooks2024video,guo2024animatediff,blattmann2023stable,yang2024cogvideox} offer higher quality; we incorporate our warped noise into CogVideoX~\cite{yang2024cogvideox} and AnimateDiff~\cite{guo2024animatediff} to demonstrate model-agnostic motion control.

\textbf{Motion Controllable Video Generation:} Current motion control approaches follow three paradigms. \textit{Local object motion control} uses bounding boxes or trajectories~\cite{jain2024peekaboo,yang2024direct,wang2024motionctrl,wu2024draganything,namekata2024sg,qiu2024freetraj,geng2024motion}; our method treats diffusion models as a black box while using synthetic flows to densify object trajectories at the pixel level. \textit{Global camera control} is parameterized by camera poses~\cite{he2024cameractrl,kuang2024collaborative,xu2024camco} or directional patterns~\cite{guo2024animatediff,yang2024direct}; our method bypasses camera parameter collection and generalizes from reference videos at inference. \textit{Motion transfer}~\cite{wang2024videocomposer,yatim2024space,geyer2023tokenflow,ling2024motionclone,mou2024revideo} propagates motion between videos; we demonstrate flexibility in combining reference geometries with target text guidance.
