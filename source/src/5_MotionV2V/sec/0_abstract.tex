\begin{abstract}
While generative video models have achieved remarkable fidelity and consistency, applying these capabilities to video editing remains a complex challenge. Recent research has extensively explored motion controllability as a means to enhance text-to-video generation or image animation; however, we identify precise motion control as a promising, yet under-explored, paradigm for editing \textbf{existing videos}. In this work, we propose modifying video motion by directly editing sparse trajectories extracted from the input. We term the deviation between input and output trajectories a `motion edit' and demonstrate that this representation, when coupled with a generative backbone, enables many powerful video editing capabilities. To achieve this, we introduce a novel pipeline for generating `motion counterfactuals' — video pairs that share identical content but distinct motion — and fine-tune a motion-conditioned video diffusion architecture on this dataset. Our approach allows for edits that start at any timestamp and propagate naturally. In a 4-way head-to-head user study, our model achieves over 65\% preference against prior work. Please see our project page: \href{https://ryanndagreat.github.io/MotionV2V}{ryanndagreat.github.io/MotionV2V}
\end{abstract}


% Motion controlability for text-to-video models remains a challenging and open problem. Recent work has explored the potential of these models to animate existing images, and even add some motion controlability. One unsolved frontier is to control motion in an \textbf{existing video}, generating a new video that has user-specified motion. In this work, we propose directly editing sparse trajectories extracted from the input video to change the motion of the output video. We term the change between the input and output trajectories the motion edit and show that motion edits, coupled with a powerful generative video model, can address several challenging video motion editing tasks. Our work allows for precise edits to motion patterns, starting at any point in the video, and naturally propagating through it. It exhibits strong generalization properties.
% To achieve this, we propose a new method to generate counterfactual video pairs sharing identical content with different motion. We then propose a motion-conditioned video diffusion architecture that can be finetuned on this dataset. Our final model achieves over \textbf{65\%} preference in a 4-way head-to-head user study with prior work.


% Motion controlability for text-to-video models remains a challenging and open problem. Recent work has explored the potential of these models to animate existing images, and even add some motion controlability. One unsolved frontier is to control motion in an \textbf{existing video}, generating a new video that has user-specified motion. In our work we present a novel method for generalized motion editing that achieves visually pleasing state-of-the-art results for this task. This problem is particularly hard due to the requirement of maintaining consistency of object appearance and scene context, while simultaneously following the user-provided motion and generating unseen visual characteristics of subjects, objects and background as they are revealed. Our work allows for precise edits to motion patterns, starting at any point in the video, and naturally propagating through it. It exhibits strong generalization properties.
% To achieve this, we propose a new method to generate counterfactual video pairs sharing identical content with different motion. We then propose a motion-conditioned video diffusion architecture that can be finetuned on this dataset. Our final model achieves over \textbf{65\%} preference in a 4-way head-to-head user study with prior work.


% Ryan Abstract
% \begin{abstract}
% Video motion controllability remains an important aspect of video generation tasks. While prior work primarily addresses generating entirely new videos, the ability to edit existing videos requires maintaining consistency in aspects such as object appearance and scene context, altering only the intended regions. Previous methods predominantly focus on controlling motion during video generation, often resulting in unintended changes to backgrounds and other details, thus failing to constitute genuine video edits. Another shortcoming of these approaches is that they typically rely on sharing an initial frame between the original and edited videos.
% In contrast, our approach introduces generalized motion editing, enabling precise edits to motion patterns in any frame throughout the video. We emphasize appearance preservation: maintaining background integrity and object properties unaffected by motion. We achieve this by curating a dataset of "counterfactual videos," comprising video pairs sharing identical content but differing solely in motion. Leveraging this dataset, we train a model that synthesizes edited videos by incorporating a target motion pattern into an existing video. Our experimental results demonstrate superior motion control and enhanced preservation of original video details compared to baseline methods.
% \end{abstract}
