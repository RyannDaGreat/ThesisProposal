% User study data - single source of truth (DRY!)
% Study parameters
\newcommand{\NumParticipants}{41}

% Win rates
\newcommand{\OursQOne}{70}
\newcommand{\ATIQOne}{24}
\newcommand{\ReVideoQOne}{1}
\newcommand{\GWTFQOne}{5}

\newcommand{\OursQTwo}{71}
\newcommand{\ATIQTwo}{24}
\newcommand{\ReVideoQTwo}{2}
\newcommand{\GWTFQTwo}{3}

\newcommand{\OursQThree}{69}
\newcommand{\ATIQThree}{25}
\newcommand{\ReVideoQThree}{1}
\newcommand{\GWTFQThree}{5}

% Helper to convert percentage to decimal for plots
\newcommand{\pct}[1]{\fpeval{#1/100}}

\section{Results}
\label{mv2v_sec:experiments}

\subsection{Implementation Details}

We use CogVideoX-5B~\cite{cogvideox2024} as our base text-to-video model for both the finetuned counterfactual video generation model and the V2V editing model.
Training was conducted on 8 H100 GPUs for one week using standard latent diffusion training with L2 loss.
We set $\Fframes = 49$,
with input resolution of $480 \times 720$ pixels,
corresponding to latent dimensions of $60 \times 90$.
We use $\Nblobs$ varying between 1 and 64 during training and set the control branch depth appropriately.
We use a learning rate of $10^{-4}$ and a dataset size of $100,000$ videos,
for $15,000$ iterations with an effective batch size of $32$.
We use an internal video dataset with 500,000 samples.

% Additional macros for experiments
\newcommand{\Vtest}{V_{\text{test}}}     % test video
\newcommand{\Vzero}{V_0}                 % first half of test video
\newcommand{\Vone}{V_1}                  % second half of test video
\newcommand{\Voneprime}{V_1'}            % reversed second half
\newcommand{\Ntest}{N_{\text{test}}}     % number of test videos
\newcommand{\Npoints}{N_{\text{points}}} % number of tracking points for evaluation

We evaluate our motion editing approach through user studies and quantitative metrics, comparing against state-of-the-art motion control methods.

\subsection{User Study}

We conducted a user study comparing our method against three baselines: ATI~\cite{ati},
a trajectory-guided image-to-video method based on WAN 2.1~\cite{wan};
ReVideo~\cite{revideo2024};
and Go-with-the-Flow (GWTF)~\cite{gowiththeflow2025}.
We manually created 20 test videos spanning diverse scenarios including object motion editing,
camera motion changes,
and complex scenes with multiple moving elements. \NumParticipants{} participants compared all four methods using the interface shown in the Supplementals,
selecting the best video for each of three questions per test case:
\begin{itemize}
    \item \textbf{Q1:} ``Which video better preserves the input video's content?''
    \item \textbf{Q2:} ``Which video better reflects the desired motion?''
    \item \textbf{Q3:} ``Which video is overall a better edit of the input video?''
\end{itemize}


\begin{table}[h]
\centering
\small
\begin{tabular}{l|cccc}
\hline
Question & Ours & ATI & ReVideo & GWTF \\
\hline
Q1: Content ($\uparrow$) & \textbf{\OursQOne\%} & \ATIQOne\% & \ReVideoQOne\% & \GWTFQOne\% \\
Q2: Motion ($\uparrow$) & \textbf{\OursQTwo\%} & \ATIQTwo\% & \ReVideoQTwo\% & \GWTFQTwo\% \\
Q3: Overall ($\uparrow$) & \textbf{\OursQThree\%} & \ATIQThree\% & \ReVideoQThree\% & \GWTFQThree\% \\
\hline
\end{tabular}
\caption[User study win rates for MotionV2V]{\textbf{User study win rates across all methods.}
Participants selected the best video for each question.
Our method consistently wins across all evaluation criteria.}
\label{mv2v_tab:user_study}
\end{table}

Table~\ref{mv2v_tab:user_study} show that users consistently ranked our method highest across all questions,
with win rates around 70\% compared to 25\% for ATI and less than 5\% for ReVideo and GWTF,
demonstrating superior content preservation and motion control.

\subsection{Quantitative Evaluation}

We developed a quantitative evaluation protocol using photometric reconstruction error to assess motion editing quality.

\subsubsection{Dataset Construction}

We curated a dataset of $\Ntest = 100$ test videos using the following protocol.
Given a source video $\Vtest$ of length $\Ffull$ frames,
we split it temporally at the midpoint to obtain $\Vzero = \Vtest\left[1:\Ffull/2\right]$ and $\Vone = \Vtest\left[\Ffull/2:\Ffull\right]$.
We then create the counterfactual input by temporally reversing $\Vone$ to get $\Voneprime$,
ensuring temporal continuity between $\Vzero$ and $\Voneprime$ (i.e., the last frame of $\Vzero$ matches the first frame of $\Voneprime$).

We selected random internet videos not seen during training where significant content appears in middle frames but is not visible in the first frame.
To quantify this,
we initialize $\Npoints = 25$ tracking points at the temporal midpoint of each video and track them bidirectionally using TAPNext~\cite{tapnext}.
We retain only videos where a substantial number of points become occluded when tracked to both the first and last frames.

\begin{figure}[htb]
    \centering
    \includegraphics[width=\linewidth]{src/5_MotionV2V/successive_edit_horz.pdf}
    \caption[Iterative motion editing]{\textbf{Iterative editing.}
Outputs can become inputs for subsequent edits,
enabling complex sequential motion changes. Yellow dots used for first edit, green/cyan for second. Arrows added from old to new position for ease of visualization.}
    \label{mv2v_fig:iterative_editing}
\end{figure}


\subsubsection{Evaluation Protocol}

For each test case,
we use $\Vzero$ as input and $\Vone$ as the target video.
We provide both our method and ATI with identical motion trajectories extracted from $\Vone$ and measure reconstruction quality using frame-wise L2 loss:

\begin{equation}
L_2 = \frac{1}{\Fframes} \sum_{i=1}^{\Fframes} \|I_i^{\text{pred}} - I_i^{\text{target}}\|_2^2
\end{equation}

where $\Fframes$ is the number of frames,
$I_i^{\text{pred}}$ is the $i$-th predicted frame,
and $I_i^{\text{target}}$ is the corresponding target frame.

\begin{table}[h]
\centering
\begin{tabular}{l|c|c|c}
\hline
\textbf{Method} & $\mathbf{L_2}\,(\downarrow)$ & \textbf{SSIM}\,($\uparrow$) & \textbf{LPIPS}\,($\downarrow$) \\
\hline
Ours               & \textbf{0.024} & \textbf{0.098} & \textbf{0.031} \\
ATI                & 0.038 & 0.094 & 0.072 \\
Go-with-the-Flow   & 0.067 & 0.089 & 0.088 \\
ReVideo            & 0.096 & 0.080 & 0.106 \\
\hline
\end{tabular}
\caption[Quantitative evaluation of MotionV2V]{Evaluation of photometric reconstruction error for our method and baselines. Our method achieves significantly lower L2 reconstruction error.}
\label{mv2v_tab:quantitative}
\end{table}

Our method achieves substantially lower reconstruction error (Table~\ref{mv2v_tab:quantitative}),
confirming that our full-video approach better preserves content compared to first-frame generation methods,
particularly in scenarios involving content not visible in initial frames.

\subsection{Qualitative Comparisons}

\noindent \textbf{Iterative Edits.}
One of the strengths of our technique is that it can be applied iteratively --- taking the output of one run and using it as input for a successive video edit. This allows users to chain multiple simple, intuitive edits together in order to achieve a very complicated edit. This iterative editing also provides more immediate feedback to the user making the process more transparent and easier to control. In~\cref{mv2v_fig:iterative_editing}, we show that a complex edit (an object motion and a camera change) can be decomposed into its core parts and applied one by one. While this example demonstrates some degree of subject drift, this can be attributed in part to the quality of the base video model. We believe that future versions of our method will be able to be applied infinitely.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{src/5_MotionV2V/megacomparisonfigurev2.jpg}
    \caption[Comparison of MotionV2V against baselines]{Comparison of our method vs.\ baselines across eight challenging motion editing scenarios.
Each row shows a different editing task with input video,
our result,
and ATI's result (with additional baselines shown for subfigure 4).
\textbf{Icon key:} Human Pose (modifying human motion),
Move Object (repositioning objects),
Move Camera (changing camera motion),
Time Control (retiming events),
Changed All Frames (no shared frames between input/output---impossible for image-to-video methods).
Colored dots track correspondence points throughout the video;
dot presence/absence indicates object visibility.
Red circles highlight key differences where baselines fail.}
    \label{mv2v_fig:mega_comparison}
\end{figure}

\noindent\textbf{Baseline comparisons.} Figure~\ref{mv2v_fig:mega_comparison} compares our method against several baselines in multiple video editing scenarios, each of which demonstrate the capabilities of our motion edits. We primarily compare against ATI~\cite{ati},
a trajectory-guided image-to-video method based on WAN 2.1~\cite{wan},
our strongest baseline despite using a more powerful base model than our CogVideoX base.
Subfigure 4 additionally shows ReVideo~\cite{revideo2024} and Go-with-the-Flow~\cite{gowiththeflow2025},
which rated poorly in user evaluation---ReVideo lacks text conditioning and Go-with-the-Flow was not designed for point control.


\noindent\textbf{Edit \#1: Complex Edits on the Boat Scene.} This edit moves the boat left and shifts the camera so that mountains from the original's last frame appear in the edit's first. This requires specifying a substantial temporal trajectory change and holistic knowledge of the scene content. Ours is the only method that realistically moves the boat while correctly adjusting the camera to reveal the mountains at the beginning of the video.

\noindent\textbf{Edit \#2: Reposing a Cheerleader.} This edit raises the cheerleader's arms. The challenge involves preserving the red pom-pom, which is absent from the first frame. Ours successfully modifies the motion while retaining this content. In contrast, ATI and ReVideo rely solely on the first frame, leading to unnatural movements and a failure to preserve the pom-pom.

\noindent\textbf{Edit \#3: Move The Bicyclist.} This edit controls a cyclist visible only in the final frame of the original video. Ours correctly propagates the cyclist and tracking dots (cyan, magenta, white) throughout. ATI, lacking full temporal context, misplaces the cyclist and synthesizes wrong buildings (red circles).

\noindent\textbf{Edit \#4: Dog Race.}
Differential timing breaks single-frame-based methods. We decelerate the Corgi (green dot) to reverse the race outcome while keeping the Bichon steady. This requires independent temporal control; ATI fails to decouple the motions, incorrectly copying the Bichon and transforming a light pole into a tree.

\noindent\textbf{Edit \#5: Moving Static Balloons.}
We add upward motion to stationary balloons. The white balloon (white dot), which appears mid-video, challenges partial information methods. While ATI moves visible balloons, it renders the initially hidden white balloon orange due to missing appearance data. Our method uses full video context to maintain correct colors.

\noindent\textbf{Edit \#6: Zooming out on the Swan}
In this DAVIS~\cite{davis2016} example, we transform a panning shot into a static, zoomed-out view. The output field of view differs entirely from the input, yet the swan must remain anchored to specific vegetation. Lacking full spatial context, ATI synthesizes a second swan and produces inconsistent motion.

\noindent\textbf{Edit \#7: Retiming a taxi.}
We do a complex isolated retiming of taxi and truck movement. This requires complete temporal understanding; ATI's single-frame generation cannot achieve this reversal. Figure~\ref{mv2v_fig:mega_comparison} compares our method against ATI (WAN 2.1-based), as well as ReVideo and Go-with-the-Flow (in Subfigure 4), both of which were rated poorly due to their design limitations.

\noindent \textbf{Edit \#8: Moving an Offscreen Car.}
As the camera follows a red car, a motorcyclist enters late. We reposition this initially invisible rider behind the car while maintaining consistent background architecture. Lacking future frames to reference the rider and buildings (red circles), ATI synthesizes incorrect content.


\noindent\textbf{Discussion.}
These scenarios highlight I2V limitations: conditioning only on the first frame prevents leveraging information from the full input. Our V2V formulation enables bidirectional flow, allowing outputs to pull content from \emph{any} input frame. This handles offscreen content, camera changes, and reordering---challenges where I2V methods like ReVideo~\cite{revideo2024}, Go-with-the-Flow~\cite{gowiththeflow2025}, and MotionPrompting~\cite{motionprompt2024} fail.
