\section{Related Works}
% Diffusion models have revolutionized media generation, starting with foundational works on denoising diffusion probabilistic models in images generation~\cite{ddpm2020, stablediffusion2022} and rapidly extending to video~\cite{vdm2022, imagenvideo2022, makeavideo2022, bar2024lumiere}, and various other domains. Recent text-conditioned video models~\cite{cogvideox2024, wan, opensora2024} utilize transformers~\cite{dit2023} as the denoising architecture.

Diffusion models have fundamentally reshaped media generation, evolving from foundational image synthesis frameworks~\cite{ddpm2020, stablediffusion2022} to complex video dynamics~\cite{vdm2022, imagenvideo2022, makeavideo2022, bar2024lumiere}. Recent text-conditioned video models~\cite{cogvideox2024, wan, opensora2024} have further advanced the field by adopting transformer-based architectures~\cite{dit2023} for scalable denoising.


\subsection{Conditional Video Generation}
% Conditional video diffusion models extend the base text-to-video architectures with additional control signals beyond text and image. Inspired by the ControlNet~\cite{controlnet2023} architectural pattern, works have adapted the approach to video domains with various conditioning mechanisms~\cite{das2025, videocontrolnet2023, motioni2v2024} enabling video conditioning through depth maps, motion vectors, camera parameters, and other modalities.
% Other methods introduce V2V editing mechanisms by propagating edits across frames and preserving some features of an original video when generating an edited one~\cite{tokenflow2024, fatezero2023, codef2024, pix2video2023, tuneavideo2023, text2videozero2023, cove2024}. A series of works use the DDIM inversion-based approaches for appearance modifications~\cite{i2vedit2024, magicedit2023, stablevideo2023}. However, these methods are fundamentally designed for local appearance changes and cannot handle nonlocal motion edits where the structural correspondence between frames is broken. When motion patterns change, the temporal alignment assumptions underlying these approaches are violated.

Conditional video diffusion extends base text-to-video architectures by incorporating auxiliary control signals. Inspired by the spatial conditioning of ControlNet~\cite{controlnet2023}, recent works have adapted similar mechanisms to the temporal domain~\cite{das2025, videocontrolnet2023, motioni2v2024}, enabling guidance through depth maps, motion vectors, and camera parameters. 
Concurrently, video-to-video (V2V) editing methods focus on propagating edits across frames while preserving the features of the source video~\cite{tokenflow2024, fatezero2023, codef2024, pix2video2023, tuneavideo2023, text2videozero2023, cove2024}. Many such approaches leverage DDIM inversion to facilitate appearance modifications~\cite{i2vedit2024, magicedit2023, stablevideo2023}. However, these methods are fundamentally designed for local appearance changes; they struggle with non-local motion edits where the structural correspondence between frames is disrupted. When motion patterns are altered, the temporal alignment assumptions underlying these inversion-based approaches are violated.


\subsection{Motion-Guided Video Generation}
% Motion-controlled video generation has emerged as a major research direction, with approaches divided into trajectory-based and optical-flow-based methods. Trajectory-based approaches condition on point trajectories for motion control \cite{tora2024, dragnuwa2023, draganything2024, dragavideo2023, imageconductor2024, boximator2024, i2vcontrol2024, 3dtrajmaster2024, flextraj2024, freetraj2024, trailblazer2024} enabling control over object motion, camera movement, and complex interactions. Optical flow-based methods~\cite{onlyflow2024, animateanything2024} utilize dense correspondences from a source video optical flow estimators and points trackers \cite{raft2020, tapir2023, bootstap2024, cotracker3_2024} for a fine-grained motion conditioning and transfer.

% Despite impressive capabilities in motion-controlled generation, these methods operate fundamentally as \textit{generation rather than editing} approaches: instead of editing an input video they take some of its attributes (e.g. optical flow) and use it as conditioning in a new video generation. Recent trajectory-based methods \cite{motionprompt2024, gowiththeflow2025, ati} condition on single images plus motion trajectories to generate new videos. While powerful for content creation, they \textit{cannot preserve the revealed visual content} of existing videos when modifying motion. First-frame preserving methods like ReVideo \cite{revideo2024} attempt to address this through inpainting, but fundamentally break down when camera motion reveals content not present in the initial frame.

% Our method addresses these limitations and enables true video-to-video motion editing. Specifically, we can edit videos by (very flexibly) modifying object and camera trajectories while preserving the rest of the video content. The method generalizes to arbitrary objects, camera motion, and complex multi-element scenes.

Motion control has emerged as a critical research direction, broadly categorized into trajectory-based and optical-flow-based methods. Trajectory-based approaches condition generation on point trajectories~\cite{tora2024, dragnuwa2023, draganything2024, dragavideo2023, imageconductor2024, boximator2024, i2vcontrol2024, 3dtrajmaster2024, flextraj2024, freetraj2024, trailblazer2024}, granting precise control over object paths, camera movement, and complex interactions. Conversely, optical flow-based methods~\cite{onlyflow2024, animateanything2024} utilize dense correspondence priors derived from optical flow estimators and point trackers~\cite{raft2020, tapir2023, bootstap2024, cotracker3_2024} to achieve fine-grained motion transfer.

Despite their impressive capabilities, these methods operate primarily as \textit{generators} rather than editors. Instead of modifying an input video directly, they extract attributes (e.g., optical flow) to condition the synthesis of an entirely new video. Recent trajectory-based methods~\cite{motionprompt2024, gowiththeflow2025, ati} attempt to bridge this by conditioning on single images and motion trajectories. However, while powerful for content creation, they fail to preserve the unrevealed visual context of existing videos when motion is modified. First-frame preserving methods like ReVideo~\cite{revideo2024} attempt to address this via inpainting but degrade when camera motion reveals content absent from the initial frame.

Our method addresses these limitations to enable true video-to-video motion editing. Specifically, we allow for flexible modification of object and camera trajectories while rigorously preserving the remaining video content. This approach generalizes effectively to arbitrary objects, diverse camera motions, and complex multi-element scenes.
